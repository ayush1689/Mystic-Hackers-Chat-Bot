{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 1 articles cleaned.\r",
      "['anova anova anova ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-10-112bd975655b>\", line 76, in <lambda>\n",
      "    command= lambda: send(EntryBox,ChatLog,value) )\n",
      "  File \"<ipython-input-10-112bd975655b>\", line 111, in send\n",
      "    res = chatbot_response(msg,value)\n",
      "  File \"<ipython-input-10-112bd975655b>\", line 123, in chatbot_response\n",
      "    output = tokenize_test3(msg)\n",
      "  File \"<ipython-input-10-112bd975655b>\", line 36, in tokenize_test3\n",
      "    clntxt=cleanData(text)\n",
      "  File \"<ipython-input-10-112bd975655b>\", line 10, in cleanData\n",
      "    temp_string = cleanString(string1)\n",
      "  File \"<ipython-input-10-112bd975655b>\", line 20, in cleanString\n",
      "    sentence_token = tokenize.word_tokenize(review)\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 143, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in sent_tokenize\n",
      "    return tokenizer.tokenize(text)\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1269, in tokenize\n",
      "    return list(self.sentences_from_text(text, realign_boundaries))\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1323, in sentences_from_text\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1323, in <listcomp>\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1313, in span_tokenize\n",
      "    for sl in slices:\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1354, in _realign_boundaries\n",
      "    for sl1, sl2 in _pair_iter(slices):\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 317, in _pair_iter\n",
      "    prev = next(it)\n",
      "  File \"C:\\Users\\kunalk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1327, in _slices_from_text\n",
      "    for match in self._lang_vars.period_context_re().finditer(text):\n",
      "TypeError: expected string or bytes-like object\n"
     ]
    }
   ],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))\n",
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)\n",
    "\n",
    "def cleanString(review):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        #single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in sentence_token]\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString\n",
    "\n",
    "def tokenize_test3(text):\n",
    "    print(text)\n",
    "    clntxt=cleanData(text)\n",
    "    word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    print(y_pred_class)\n",
    "    print(labelEncoder.classes_[y_pred_class])\n",
    "    return(y_pred_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clicked (value):\n",
    "    clearscreen()   ## to clear the screen\n",
    "    label1 = Label(root, text=value, font=(\"Verdana\", 12) )\n",
    "    label1.pack()\n",
    "\n",
    "    #Create Chat window\n",
    "    ChatLog = Text(root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\")\n",
    "    \n",
    "    ChatLog.insert(END, \"BOT:  Hello!!, Please type your question \" '\\n\\n')\n",
    "    ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ),borderwidth = 5)\n",
    "\n",
    "    ChatLog.config(state=DISABLED)\n",
    "\n",
    "    #Bind scrollbar to Chat window\n",
    "    scrollbar = Scrollbar(root, command=ChatLog.yview, cursor=\"heart\")\n",
    "    ChatLog['yscrollcommand'] = scrollbar.set\n",
    "    \n",
    "    #Create the box to enter message\n",
    "    EntryBox = Text(root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\",borderwidth = 5)\n",
    "    #EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "    #Create Button to send message\n",
    "    SendButton = Button(root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"11\", height=4,\n",
    "                        bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',borderwidth = 5,\n",
    "                        command= lambda: send(EntryBox,ChatLog,value) )\n",
    "\n",
    "    #Place all components on the screen\n",
    "    scrollbar.place(x=376,y=6, height=386)\n",
    "    ChatLog.place(x=6,y=25, height=375, width=370)\n",
    "    EntryBox.place(x=150, y=401, height=90, width=265)\n",
    "    SendButton.place(x=6, y=425, height=40)\n",
    "    #root.title(\"Transparency\")\n",
    "\n",
    "    #set width and height\n",
    "    #canvas=Canvas(root,width=375,height=390)\n",
    "    #give this image path. image should be in png format.\n",
    "    #image=ImageTk.PhotoImage(Image.open(\"Chat_Bot_Image.png\"))\n",
    "    #canvas.create_image(0,0,anchor=NW,image=image)\n",
    "    #canvas.place(height=400, x=0, y=0)\n",
    "\n",
    "def clearscreen():\n",
    "    list = root.pack_slaves()\n",
    "    for l in list:\n",
    "        l.destroy()    \n",
    "        \n",
    "def send(EntryBox,ChatLog,value):\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    \n",
    "    if msg == '':\n",
    "        messagebox.showinfo(\"Allert\", \"Please enter your question and then click send\")\n",
    "        \n",
    "    elif msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "\n",
    "        res = chatbot_response(msg,value)\n",
    "        ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "        \n",
    "\n",
    "def chatbot_response(msg,value):\n",
    "    \n",
    "    msg =cleanData(msg)  ##output is articals\n",
    "\n",
    "    if value == \"Python\":\n",
    "        output = tokenize_test3(msg)\n",
    "        value = output\n",
    "        return(output) \n",
    "    elif value == \"Statistics\":\n",
    "        value = value + \" output\"\n",
    "        return(value) \n",
    "    elif value == \"Supervised Learning\":\n",
    "        value = value + \" output\"\n",
    "        return(value)\n",
    "    elif value == \"Ensemble Techniques\":\n",
    "        value = value + \" output\"\n",
    "        return(value)\n",
    "    elif value == \"Unsupervised learning\":\n",
    "        value = value + \" output\"\n",
    "        return(value)\n",
    "    else:\n",
    "        value = value + \" output\"\n",
    "        return(value)\n",
    "\n",
    "#Description: This is a chat bot GUI\n",
    "\n",
    "\n",
    "#Import the library\n",
    "\n",
    "import time\n",
    "from PIL import ImageTk,Image\n",
    "import tkinter\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "root = Tk()\n",
    "#Name of the chat Bot\n",
    "root.title(\"AASKK\") \n",
    "#Geometry of the chat Bot\n",
    "root.geometry(\"400x480\")\n",
    "root.resizable(width=FALSE, height=FALSE)\n",
    "main_menu = Menu(root)\n",
    "# Create the submenu \n",
    "file_menu_1 = Menu(root)\n",
    "file_menu_2 = Menu(root)\n",
    "# Add commands to submenu\n",
    "file_menu_1.add_command(label=\"New..\")\n",
    "file_menu_1.add_command(label=\"Save As..\")\n",
    "file_menu_1.add_command(label=\"Exit\")\n",
    "main_menu.add_cascade(label=\"File\", menu=file_menu_1)\n",
    "main_menu.add_command(label=\"Edit\")\n",
    "main_menu.add_command(label=\"Restart\")\n",
    "root.config(menu=main_menu)\n",
    "\n",
    "label2 = Label(root, text=\"Welcome\", font=(\"Arial\", 18) )\n",
    "label2.pack(padx= 20, pady =10)\n",
    "\n",
    "label2 = Label(root, text=\"Please select the topic from which you want to learn\", font=(\"Arial\", 12) )\n",
    "label2.pack(padx= 20, pady =20)\n",
    "\n",
    "\n",
    "TOPICS = [(\"Python\",\"Python\"),\n",
    "          (\"Statistics\",\"Statistics\"),\n",
    "          (\"Supervised Learning\",\"Supervised Learning\"),\n",
    "          (\"Ensemble Techniques\",\"Ensemble Techniques\"),\n",
    "          (\"Unsupervised learning\",\"Unsupervised learning\"),\n",
    "          (\"Not Sure about the topic\",\"All Topic\")\n",
    "         ]\n",
    "option = StringVar()  \n",
    "option.set(\"Python\")\n",
    "  \n",
    "for text,topics in TOPICS:\n",
    "    r =Radiobutton(root, text = text, variable=option , value=topics, font=(\"Arial\", 12))\n",
    "    r.pack(anchor = W)\n",
    "    \n",
    "Button2 = Button(root, text=\"Submit\", activebackground=\"#00bfff\", font=(\"Arial\", 12), command = lambda : clicked(option.get()))\n",
    "\n",
    "Button2.pack(padx= 20, pady =20)\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageTk,Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kunalk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kunalk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kunalk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "##from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import hstack\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
