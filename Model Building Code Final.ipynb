{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic library\n",
    "\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the library for model building\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#library for NLP\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "import nltk\n",
    "import nltk.stem\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Data\n",
    "\n",
    "1. cleanString() function clean the data both incoming and our train data.\n",
    "2. stopwords used here to remove the redundant word from our sentences.\n",
    "3. lematizer used here to remove the verb,adjective form inorder to provide the required word from the sentence.\n",
    "4. also all the sentences are converted to one similar format(here converted to lower). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review):\n",
    "    # Setup the stopwords from the English dictionary\n",
    "    #This can be used to clean Entire Dataframe of Documents.\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    #Create an object for word lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    #Tokenize the words from string to a list\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        #For each sentence after tokenization, we take in the word for processing\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        #We apply the word lemmetizer to lemmatize the word where we remove verb , adjactive inorder to provide the required word in the sentence.\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in single_sentence]\n",
    "        # Filter stop words , make the all words in lower character and enumerate the sentence to remove special characters.\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        #Filtered string is taken for further processing\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString\n",
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "    #special case of Clean String function specifically implemented for cleaning the Incoming text(onely one text)\n",
    "    # In this case, we clean only one question which is entered by the user in the GUI\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)\n",
    "def tokenize_test(model,train,validation):\n",
    " #APPLY CHARACTER VECTORIZER TRANSFORM ON VALIDATION DATA, 20000 dimensions of characters are created\n",
    "    X_test1 = char_vectorizer.transform(validation)\n",
    "    X_test2 = word_vectorizer.transform(validation)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    #APPLY WORD VECTORIZER TRANSFORM ON VALIDATION DATA, 3000 dimensions of words are created\n",
    "     \n",
    "    #train_text = vect.fit_transform(train[\"Text\"])\n",
    "    #print ('Features: ', train_text.shape[1])\n",
    "    #test_text = vect.transform(validation[\"Text\"])\n",
    "      #Normalize the incoming test stack and train data\n",
    "       \n",
    "    train_features2 = Normalizer(copy=False).fit_transform(train_features1)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    \n",
    "    \n",
    "    model.fit(train_features2, y_train)\n",
    "    filename = 'model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    #word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    #char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    #X_test1 = word_model.transform(clntxt)\n",
    "    #X_test2 = char_model.transform(clntxt)\n",
    "    #test_stack = hstack([X_test1,X_test2])\n",
    "    #LOWER THE DIMENSION USING SVD IN PRINCIPAL COMPONENT ANALYSIS TO VERY FEW 500 DIMENSIONS\n",
    "      \n",
    "    svd_model=pickle.load( open( \"Pyhton_SVD_preprocessing.sav\", \"rb\" ) )\n",
    "    test_stack=svd_model.transform(test_stack)\n",
    "    \n",
    "      #FIT THE MODEL and SAVE THE FITTED MODEL\n",
    "            # APPLY UNSUPERVISED METHOD PAIRWAISE COSINE DISTANCE BETWEEN THE INCOMING TEXT and DOCUMENT and FIND THE CLOSEST MATCH \n",
    "                #Unsupervised method ensures the words are matched with context of the document and closest match is choosen for prediction\n",
    "                 #Predict the model with the incoming test data\n",
    "   \n",
    "    best_thread = pairwise_distances_argmin(\n",
    "            X=test_stack,\n",
    "            Y=train_features2,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    print(best_thread)\n",
    "    # There are two predictions done here. best_thread is an output from pairwise cosine distance and that is used for prediction \n",
    "    # Original test word is also used for prediction\n",
    "    y_pred_class1 = model.predict(train_features2[best_thread])\n",
    "    y_pred_class2 = model.predict(test_stack)\n",
    "    print(y_pred_class1,y_pred_class2)\n",
    "    #return(y_pred_class1,y_pred_class2)\n",
    "    \n",
    "    \n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    # Print the training accuracy \n",
    "    print(\"Training Accuracy\")\n",
    "    print(model.score(train_features2,y_train))\n",
    "    #Print the test accuracy with original incoming text\n",
    "    print(\"Testing Accuracy1\")\n",
    "    print(model.score(test_stack,y_test1))\n",
    "    #Print the test accuracy with pairwise distance (cosine) from the original text and similar words\n",
    "    print(\"Testing Accuracy2\")\n",
    "    print(model.score(train_features1[best_thread],y_test1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE the built model using modular functions\n",
    "def tokenize_test2(model,train,text):\n",
    "    from sklearn.decomposition import NMF\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    print(text)\n",
    "    # APPLY CHARACTER vectorizer\n",
    "    X_test1 = char_vectorizer.transform(text)\n",
    "     # APPLY Word vectorizer\n",
    "    X_test2 = word_vectorizer.transform(text)\n",
    "    # Combine character and word vectorizer\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    # Normalize the features before doing fit\n",
    "    train_features = Normalizer(copy=False).fit_transform(train_features1)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    # FIT THE MODEL \n",
    "    model.fit(train_features, y_train)\n",
    "    # SAVE THE MODEL\n",
    "    filename = 'model.sav'\n",
    "    # DUMP THE PICKLE FILE FOR GUI USAGE\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    # PREDICT the result from the test data\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    # return the predicted label\n",
    "    return(y_pred_class)\n",
    "# Again modular function to be integrated with the GUI Code\n",
    "def tokenize_test3(text):\n",
    "    print(text)\n",
    "    # text is the incoming text data\n",
    "    clntxt=cleanData(text)\n",
    "    #clean the incoming text data using cleanString\n",
    "    # APPLY word model\n",
    "    \n",
    "    word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    #APPLY character model\n",
    "    char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    #Stack the word model and character model together\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    \n",
    "    #Save the Model in a pickle file\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    \n",
    "    #Predict the model using model.predict\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    print(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. TruncatedSVD used here to reduce the dimension inorder to get best performance of the model.(n_components=200,200 dimensions)\n",
    "2. Character vectorizer and Word vectorizer both are used to get the vector form the tokenized word.We can use any vectorizer but here we used both for better performance of our model.\n",
    "3. 'pickle' is used to load our model.\n",
    "4. pairwise distance is used here on the basis of cosine similarity in order to find out the match between word.It also help model to find our the context which will help Model to find the proper response for a required question.\n",
    "5. Normalizer is also used here to keep our data under one scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "\n",
    "1) TruncatedSVD used here to reduce the dimension inorder to get best performance of the model.(n_components=200,200 dimensions)\n",
    "\n",
    "2) Dimensions of the character vectorizer is set at 20K so that all possible combinations of characters are captured by the character model. \n",
    "\n",
    "3) Dimension of the word vectorizer is set at 3K so that all possible combinations of words are captured by the word model.\n",
    "\n",
    "4) We then create a context definition by applying TFIDF on the stememed words.\n",
    "\n",
    "5) We apply stemming twice to reduce the dimensions of the models and our model fits in 25MB if we apply Stemming twice once hrough the TFIDF and another through a script.\n",
    "\n",
    "6) We took sufficient care to have minimal dimensions by reprojecting the higher dimension to 500X500 dimension using Principal component analysis.(SVD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Our Main Corpous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Sub_Topic_Code</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Python Introduction</td>\n",
       "      <td>I want to know about Python</td>\n",
       "      <td>https://beginnersbook.com/2018/01/introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Python Introduction</td>\n",
       "      <td>What is Python</td>\n",
       "      <td>https://beginnersbook.com/2018/01/introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Python Introduction</td>\n",
       "      <td>I Want to understand Python</td>\n",
       "      <td>https://beginnersbook.com/2018/01/introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Python Introduction</td>\n",
       "      <td>what is python ecosystem</td>\n",
       "      <td>https://beginnersbook.com/2018/01/introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Python Introduction</td>\n",
       "      <td>whether python ecosystem is well supported</td>\n",
       "      <td>https://beginnersbook.com/2018/01/introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>AASKK010</td>\n",
       "      <td>Ploting</td>\n",
       "      <td>What are different types of plots in python</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>AASKK010</td>\n",
       "      <td>Ploting</td>\n",
       "      <td>What is Stacked barplot with matplotlib</td>\n",
       "      <td>https://www.geeksforgeeks.org/bar-plot-in-matp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>AASKK010</td>\n",
       "      <td>Ploting</td>\n",
       "      <td>Which plot is used for correlation</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Scatter_plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>AASKK010</td>\n",
       "      <td>Ploting</td>\n",
       "      <td>Histogram used for?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Histogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ploting</td>\n",
       "      <td>Matplotlib Python Plotting Ways?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Matplotlib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>669 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Code       Sub_Topic_Code  \\\n",
       "0      AASKK01  Python Introduction   \n",
       "1      AASKK01  Python Introduction   \n",
       "2      AASKK01  Python Introduction   \n",
       "3      AASKK01  Python Introduction   \n",
       "4      AASKK01  Python Introduction   \n",
       "..         ...                  ...   \n",
       "664   AASKK010              Ploting   \n",
       "665   AASKK010              Ploting   \n",
       "666   AASKK010              Ploting   \n",
       "667   AASKK010              Ploting   \n",
       "668        NaN              Ploting   \n",
       "\n",
       "                                        Question  \\\n",
       "0                    I want to know about Python   \n",
       "1                                 What is Python   \n",
       "2                    I Want to understand Python   \n",
       "3                       what is python ecosystem   \n",
       "4     whether python ecosystem is well supported   \n",
       "..                                           ...   \n",
       "664  What are different types of plots in python   \n",
       "665      What is Stacked barplot with matplotlib   \n",
       "666           Which plot is used for correlation   \n",
       "667                          Histogram used for?   \n",
       "668             Matplotlib Python Plotting Ways?   \n",
       "\n",
       "                                              Response  \n",
       "0    https://beginnersbook.com/2018/01/introduction...  \n",
       "1    https://beginnersbook.com/2018/01/introduction...  \n",
       "2    https://beginnersbook.com/2018/01/introduction...  \n",
       "3    https://beginnersbook.com/2018/01/introduction...  \n",
       "4    https://beginnersbook.com/2018/01/introduction...  \n",
       "..                                                 ...  \n",
       "664           https://en.wikipedia.org/wiki/Matplotlib  \n",
       "665  https://www.geeksforgeeks.org/bar-plot-in-matp...  \n",
       "666         https://en.wikipedia.org/wiki/Scatter_plot  \n",
       "667            https://en.wikipedia.org/wiki/Histogram  \n",
       "668           https://en.wikipedia.org/wiki/Matplotlib  \n",
       "\n",
       "[669 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats=pd.read_csv('ALL_merged.csv',encoding =  \"ISO-8859-1\")\n",
    "stats.head()\n",
    "stats=stats.drop([\"Unnamed: 4\"],axis=1)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for Model Building\n",
    "\n",
    "1. Feature encoding is carried out to change the sub_topic_code into respective Label.\n",
    "2. labelEncoder.pickle is used to load it into model.\n",
    "3. Sampling of data also done in order to get all the sub_topic in equal proportion.\n",
    "4. Dataset is splited into train set and test set.\n",
    "5. StemmedTfidfVectorizer is used here to convert word or character to its respective vector.It is also an advanced version of TfidfVectorizer.\n",
    "6. We are used both word_vectorizer and Character_vectorizer for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_feature_name:8924\n",
      "word_feature_name:2475\n",
      "Explained Variance:1.0000002\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE ALL MERGED CSV FILE \n",
    "stats=pd.read_csv('ALL_merged.csv',encoding =  \"ISO-8859-1\")\n",
    "\n",
    "# LABEL ENCODE THE CATEGORIES \n",
    "labelEncoder = LabelEncoder()\n",
    "\n",
    "# STORE THE RESPONSE AND CATEGORY ASSOCIATION IN A DICTIONARY. FINALLY WHEN WE give the results, we lookup the dictionary \n",
    "#to give the responses\n",
    "df = dict(zip(stats.Sub_Topic_Code, stats.Response))\n",
    "\n",
    "stats[\"Topic_Name\"] = labelEncoder.fit_transform(stats[\"Sub_Topic_Code\"])\n",
    "#df = dict(zip(stats.Topic_Name, stats.Response))\n",
    "#stats[\"Topic_Name\"] = labelEncoder.transform(stats[\"Response\"])\n",
    "\n",
    "import pickle\n",
    "\n",
    "#SAVE THE LABEL ENCODED OUTPUTS. This is helpful to lookup wikipedia or other online resources for links once we have \n",
    "# label encoded categories as pickle file.\n",
    "with open('labelEncoder.pickle', 'wb') as file:\n",
    "    pickle.dump(labelEncoder, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Label dictionary is useful to give hard coded links which are associated to label encoded outputs    \n",
    "with open('LabelDictionary.pickle', 'wb') as file:\n",
    "    pickle.dump(df, file, pickle.HIGHEST_PROTOCOL)\n",
    "stats = stats.rename(columns={'Topic_Name': 'Category', 'Question': 'Text'})\n",
    "\n",
    "# shuffle the data so that same categories dont stay next to each other\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"Text\"]=data_df.Text.apply(cleanString)\n",
    "y=data_df[\"Category\"]\n",
    "X=data_df[\"Text\"]\n",
    "\n",
    "with open('Data_df.pickle', 'wb') as file:\n",
    "    pickle.dump(data_df, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    \n",
    "## test train split the data with 1% validation data and rest is used to build the model.    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01, random_state=42)\n",
    "\n",
    "\n",
    "# special tokenizer function integarted with TfiDfvectorizer. THis along with stemmer is incorporated as a function\n",
    "def build_tokenizer(doc):\n",
    "    token_pattern=r\"(?u)\\b\\w+\"\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    return token_pattern.findall(doc)\n",
    "\n",
    "\n",
    "# special Stemmer based TFIDF which is unique to our code. We have incorporated Stemmming along with TFIDF to reduce the dimensions.\n",
    "# So each incoming word is stemmed twice to reduce the dimension.\n",
    "# Thsi is an overlay metho which works along with the built in TFIDF vectorizer method\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))\n",
    "    \n",
    "posts_root1=[]\n",
    "\n",
    "for post in X:\n",
    "    a=\" \".join([english_stemmer.stem(word) for word in build_tokenizer(post)])\n",
    "    posts_root1.append(a)\n",
    "\n",
    "posts_root1=[]\n",
    "\n",
    "for post in X_train:\n",
    "      posts_root1.append(post)\n",
    "\n",
    "        \n",
    "# Apply word vecotirzer. \n",
    "# Stop words are used as arguments\n",
    "# Word analyzer with token pattern is used.\n",
    "# we have used Ngram of ( 1,5 ) upto 5 word groupings can be found in our database.\n",
    "# We return l2 norm outputs so that output stays sparse after TFIDF.\n",
    "#Maximum features considered are 3K only\n",
    "# This retursn 3K dimensions after TFIDF transformation\n",
    "word_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words = 'english',\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1,5),\n",
    "    norm='l2',\n",
    "    dtype=np.float32,\n",
    "    max_features=3000,\n",
    "    \n",
    ")\n",
    "# Character Stemmer\n",
    "# Apply Character vecotirzer. \n",
    "# Character analyzer with token pattern is used.\n",
    "# we have used Ngram of ( 1,5 ) upto 5 character groupings can be found in our database.\n",
    "# We return l2 norm outputs so that output stays sparse after TFIDF.\n",
    "#Maximum features considered are 20K  only.\n",
    "# This returns 20K dimensions after TFIDF transformation\n",
    "char_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    dtype=np.float32,\n",
    "    norm='l2',\n",
    "    max_features=20000,\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "# FIT and TRANSFROM THE TFIDF word and character vectorizer.\n",
    "word_vectorizer.fit(posts_root1)\n",
    "char_vectorizer.fit(posts_root1)\n",
    "\n",
    "\n",
    "train_word_features = word_vectorizer.transform(posts_root1)\n",
    "train_char_features = char_vectorizer.transform(posts_root1)\n",
    "\n",
    "\n",
    "# Stack the TFIDF word and character vectorizer\n",
    "train_features = hstack([\n",
    "    train_char_features,\n",
    "    train_word_features])\n",
    "\n",
    "## APPLY TRUNCATED SVD\n",
    "tsvd=TruncatedSVD(700)\n",
    "\n",
    "train_features1 = tsvd.fit_transform(train_features)\n",
    "\n",
    "## SAVE the MODELS in a FIle\n",
    "#explained_variances = np.var(train_features1) / np.var(train_features).sum()\n",
    "filename = 'train_features1_preprocessing.sav'\n",
    "pickle.dump(train_features1, open(filename, 'wb'))\n",
    "filename = 'Python_word_preprocessing.sav'\n",
    "pickle.dump(word_vectorizer, open(filename, 'wb'))\n",
    "filename = 'Pyhton_char_preprocessing.sav'\n",
    "pickle.dump(char_vectorizer, open(filename, 'wb'))\n",
    "filename = 'Pyhton_SVD_preprocessing.sav'\n",
    "pickle.dump(tsvd, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "print(\"char_feature_name:%s\" % len(char_vectorizer.get_feature_names()))\n",
    "print(\"word_feature_name:%s\" % len(word_vectorizer.get_feature_names()))\n",
    "print(\"Explained Variance:%s\" % tsvd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance :\n",
    "\n",
    "### a) with NeuralNetwork Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 78 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35] [73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 78 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "0.9909365558912386\n",
      "Testing Accuracy1\n",
      "0.9950248756218906\n",
      "Testing Accuracy2\n",
      "0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "#stats1=pd.read_excel('Ensemble.xlsx',encoding =  \"ISO-8859-1\")\n",
    "stats1=pd.read_csv('ALL_merged.csv',encoding =  \"ISO-8859-1\")\n",
    "#stats.columns=[\"Topic_Code\",\"Sub_Topic_Code\",\"Topic_Name\",\"Question\"]\n",
    "stats1[\"Sub_Topic_Code\"] = labelEncoder.transform(stats1[\"Sub_Topic_Code\"])\n",
    "#stats1[\"Sub_Topic_Code\"] = labelEncoder.transform(stats1[\"Sub_Topic_Code\"])\n",
    "stats1 = stats1.rename(columns={'Sub_Topic_Code': 'Category1', 'Question': 'Text'})\n",
    "data_df=stats1.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"Text\"]=data_df.Text.apply(cleanString)\n",
    "y=data_df[\"Category1\"]\n",
    "X=data_df[\"Text\"]\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y,test_size=0.3, random_state=23)\n",
    "mlp=MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.75,\n",
    "              beta_2=0.8, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(300, 300), learning_rate='constant',\n",
    "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
    "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "              power_t=0.5, random_state=200, shuffle=True, solver='adam',\n",
    "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "              warm_start=False)\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "tokenize_test(mlp,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87    28\n",
       "52    51\n",
       "46    51\n",
       "95    20\n",
       "88    15\n",
       "Name: Category1, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the predicted class\n",
    "y_test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      46\n",
       "1      75\n",
       "2      32\n",
       "3      64\n",
       "4      22\n",
       "       ..\n",
       "664    43\n",
       "665    64\n",
       "666    51\n",
       "667    55\n",
       "668    73\n",
       "Name: Category, Length: 669, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) With regularised Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 47 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 63 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35] [73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 47 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 63 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "0.9697885196374623\n",
      "Testing Accuracy1\n",
      "0.9850746268656716\n",
      "Testing Accuracy2\n",
      "0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=50, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=0.01, min_samples_split=0.02,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=None, oob_score=False, random_state=200,\n",
    "                       verbose=0, warm_start=False)\n",
    "tokenize_test(rf,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Stochastic gradient descent (SGD) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[86 86 85 46 86 64 34 86 71 86 86 86 54 86 33 86 48 46 28 85 56 48 86 86\n",
      " 86 46 86 34 85 60 85 55 32 86 86 86 86 80 32 86 86 80 86 48 54 86 86 86\n",
      " 86 28 80 86 55 39 64 60  6 86 64 80 46 85 86 86 86 39  6 86 48 86 86 48\n",
      " 86 28 86 86 86  6 86 39 34 33 86 86 86 86 46 86 86 86 48 60 82 85 86 80\n",
      " 86 86 86 86 86 86 86 86 54 48 86 39 86 86 86 32 86 86 86 48 86 86 86 86\n",
      " 48 86 86 48 80 34 60 32 48 86 86 86 86 48 86 86 86 48 86  6 80 86 49  0\n",
      " 46 46 86 86 86 86 80 48 86 33 28 53 86 86 54 64 48 64 43 86 86 71  6 86\n",
      " 80 86 80 48 32 86 85 86 85 33 46 86 60 54 86 86 86 86 86 86 86 86 28 86\n",
      " 86 86 86 55 28 85 86 64 86] [86 86 85 46 86 64 34 86 71 86 86 86 54 86 33 86 48 46 28 85 56 48 86 86\n",
      " 86 46 86 34 85 60 85 55 32 86 86 86 86 80 32 86 86 80 86 48 54 86 86 86\n",
      " 86 28 80 86 55 39 64 60  6 86 64 80 46 85 86 86 86 39  6 86 48 86 86 48\n",
      " 86 28 86 86 86  6 86 39 34 33 86 86 86 86 46 86 86 86 48 60 82 85 86 80\n",
      " 86 86 86 86 86 86 86 86 54 48 86 39 86 86 86 32 86 86 86 48 86 86 86 86\n",
      " 48 86 86 48 80 34 60 32 48 86 86 86 86 48 86 86 86 48 86  6 80 86 49  0\n",
      " 46 46 86 86 86 86 80 48 86 33 28 53 86 86 54 64 48 64 43 86 86 71  6 86\n",
      " 80 86 80 48 32 86 85 86 85 33 46 86 60 54 86 86 86 86 86 86 86 86 28 86\n",
      " 86 86 86 55 28 85 86 64 86]\n",
      "Training Accuracy\n",
      "0.32326283987915405\n",
      "Testing Accuracy1\n",
      "0.32338308457711445\n",
      "Testing Accuracy2\n",
      "0.32338308457711445\n"
     ]
    }
   ],
   "source": [
    "sgd=SGDClassifier(alpha=0.01, average=False, class_weight=None,\n",
    "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "              l1_ratio=0.2, learning_rate='optimal', loss='hinge', max_iter=500,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
    "              power_t=0.5, random_state=200, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "tokenize_test(sgd,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) With Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 47 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 63 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35] [73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 47 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 63 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "0.9758308157099698\n",
      "Testing Accuracy1\n",
      "0.9850746268656716\n",
      "Testing Accuracy2\n",
      "0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "EnsembleClassifier = VotingClassifier(estimators = [ ('rf',rf),('mlp',mlp),('sgd1',sgd)], voting = 'hard', weights = [1,1,1])\n",
    "\n",
    "tokenize_test(EnsembleClassifier,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) With Mltinomial Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 84 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 78 55 39 64 60  7 57 64 45 28 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 78 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 72 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 84 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35] [73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 84 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 78 55 39 64 60  7 57 64 45 28 85 32 30 82 39  6 39 35 56 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 78 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 72 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 56 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 84 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "0.9501510574018127\n",
      "Testing Accuracy1\n",
      "0.9701492537313433\n",
      "Testing Accuracy2\n",
      "0.9701492537313433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "tokenize_test(GaussianNB(),X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) With Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 51 64 34 53 71 30  3 60 54 61 33 60 69 46 28 85 56 66 70 63\n",
      " 60 43 60 56 45 60 85 55 32 61 60 47 51 45 63 47 60 80 63 48 54 55 60 55\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 47 39  6 39 35 47 70 32\n",
      " 47 28 32 84 42  6 60 39 52 33  0 47 68 47 46 60  8 60 48 60 47 85  3 45\n",
      " 46 63 42 81 61 47 19 42 46 26 47 39 21 42 60 32 73 84 83 35 61  4 60 86\n",
      " 66 55 47 66 45 34 60 32 48 52 42 56 71 48 61 72 60 35 47  7 80 60 49  0\n",
      " 46 46 35 63 47 47 46 47 52 33 28 53 42 71 54 64 55 64 43 63 32 71  6 55\n",
      " 45 56 45 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 47 51 47 19 28 84\n",
      " 52 60 20 55 28 85 35 64 35] [73 47  3 46 51 64 34 53 71 30  3 60 54 61 33 60 69 46 28 85 56 66 70 63\n",
      " 60 43 60 56 45 60 85 55 32 61 60 47 51 45 63 47 60 80 63 48 54 55 60 55\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 47 39  6 39 35 47 70 32\n",
      " 47 28 32 84 42  6 60 39 52 33  0 47 68 47 46 60  8 60 48 60 47 85  3 45\n",
      " 46 63 42 81 61 47 19 42 46 26 47 39 21 42 60 32 73 84 83 35 61  4 60 86\n",
      " 66 55 47 66 45 34 60 32 48 52 42 56 71 48 61 72 60 35 47  7 80 60 49  0\n",
      " 46 46 35 63 47 47 46 47 52 33 28 53 42 71 54 64 55 64 43 63 32 71  6 55\n",
      " 45 56 45 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 47 51 47 19 28 84\n",
      " 52 60 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "0.7885196374622356\n",
      "Testing Accuracy1\n",
      "0.8258706467661692\n",
      "Testing Accuracy2\n",
      "0.8258706467661692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "tokenize_test(LogisticRegression(),X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) With Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 47 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 78 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 47 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35] [73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 12 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 47 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 78 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 47 47 46 27 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "1.0\n",
      "Testing Accuracy1\n",
      "0.9950248756218906\n",
      "Testing Accuracy2\n",
      "0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tokenize_test(DecisionTreeClassifier(),X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 221 254 377 225  21 484 262  77 613 423 491 536 334 565 386 621 305\n",
      "  46 209 661 556 359 618 413 362 338 336 206 383 546 418 117 586 496  95\n",
      " 292 402  44 593 331 110 311 121 519 414 540 202 589 611 304 312 439  23\n",
      "  78 464 369 308 568 276 528 236 164 394 411 459 623 208 594 440  82 150\n",
      " 646 608 226 632 373 537 381 310 476 368 133 319 550 462 442 136 173 630\n",
      " 327 134  61 514 158  90 215 160 520 495   6 320 518 174 473  38 288 635\n",
      " 605 260 389 541 627 255  25 316 642 302 242  37 412 390 374 182  20 187\n",
      "  91  51 167 463  74 558 547 243 300 144  79 293 416 449 585 193 469 244\n",
      "   2  26 275 194 440 376 211 357 600 109 631   3 584 660 649 648 258 458\n",
      "  85 647 387 572  35 111 370  39  12 527 471 553 295 308  97 371 400 251\n",
      " 134 145  89 410 283 560 108 505 301 382 175 653 549 278 517  18  54 282\n",
      " 580  71 269]\n",
      "[73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 61 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 47 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 47 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 47 47 46 47 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35] [73 47  3 46 23 64 34 53 71 30  3 60 54 61 33 31 69 46 28 85 56 66 70 63\n",
      "  5 43 60 56  3 60 85 55 49 61 60 47 51 45 63 47 41 80 63 48 54 49 61 84\n",
      " 84 28 45 86 55 39 64 60  7 57 64 45 46 85 32 30 82 39  6 39 35 47 70 32\n",
      " 47 28 32 84 42  6 60 39 24 33  0 47 68 47 21 72 17 58 48 60 47 85  3 45\n",
      "  1 63 42 81 61 47 19 42 77 26 47 58 21 42 72 32 73 84 83 35 61  4  5 67\n",
      " 66 81 78 66 50 34 60 32 48 52 42 56 71 48 61 72 50 35 23  7 80 37 49  0\n",
      " 46 46 35 63 47 47 46 47 52 33 28 53 42 71 54 65 55 64 43 63 32 71  6 55\n",
      " 45 56 50 47 32 81  4 57 85 33 43 63 60 54 18 55 55 86 44 51 82 19 28 84\n",
      " 52 86 20 55 28 85 35 64 35]\n",
      "Training Accuracy\n",
      "0.9697885196374623\n",
      "Testing Accuracy1\n",
      "0.9800995024875622\n",
      "Testing Accuracy2\n",
      "0.9800995024875622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf',gamma=0.23, C=3.20)\n",
    "tokenize_test(model,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_loss(y_true, y_predicted):\n",
    "    \"\"\"Turn MCC into a loss function to be minimized.\n",
    "    Args:\n",
    "        y_true: Ground truth (correct) target values.\n",
    "        y_predicted: Estimated targets as returned by a classifier.\n",
    "\n",
    "    Returns:\n",
    "        MCC loss (1-MCC).\n",
    "    \"\"\"\n",
    "    return 1 - matthews_corrcoef(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "from sklearn.metrics import SCORERS, make_scorer, matthews_corrcoef\n",
    "\n",
    "default_hyperparams_grid = { 'MLPClassifier1':\n",
    "            {'hidden_layer_sizes': [(200, 200), (200, 150), (200, 100)],\n",
    "             'activation': ['tanh', 'relu'],\n",
    "             'solver': ['adam'],\n",
    "             'alpha': [.0001, .001,],\n",
    "             'beta_1': [.75,  ],\n",
    "             'beta_2': [.8, .9, ],\n",
    "             'max_iter': [200],\n",
    "             'early_stopping': [False],\n",
    "             'random_state': [200]\n",
    "             }\n",
    "}\n",
    "hyperparams = default_hyperparams_grid[\"MLPClassifier1\"]\n",
    "scoring = {'MCC': 'matthews_corrcoef'}\n",
    "scoring_methods = {\n",
    "        'matthews_corrcoef': matthews_corrcoef\n",
    "    }\n",
    "scoring = {name: SCORERS[scorer] if scorer in SCORERS.keys() else make_scorer(scoring_methods[scorer]) for name, scorer in scoring.items()}\n",
    "scorer_names = list(scoring.keys())\n",
    "X_test1 = char_vectorizer.transform(X_test)\n",
    "X_test2 = word_vectorizer.transform(X_test)\n",
    "test_stack = hstack([X_test1,X_test2])\n",
    "X_train1 = char_vectorizer.transform(X_train)\n",
    "X_train2 = word_vectorizer.transform(X_train)\n",
    "train_stack = hstack([X_train1,X_train2])\n",
    "        \n",
    "search = GridSearchCV(model, param_grid=hyperparams, scoring=scoring, n_jobs=16, cv=10, error_score=0,\n",
    "                              refit=scorer_names[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "from sklearn.metrics import SCORERS, make_scorer, matthews_corrcoef\n",
    "\n",
    "default_hyperparams_grid = {'RandomForestClassifier': {'n_estimators': [50, 100],\n",
    "                                   'max_depth': [None],\n",
    "                                   'max_features': ['auto'],\n",
    "                                   'min_samples_split': [ .02, .2],\n",
    "                                   'min_samples_leaf': [ .01, .2],\n",
    "                                   'bootstrap': [True],\n",
    "                                   'criterion': ['entropy'],\n",
    "                                   'max_leaf_nodes': [50],\n",
    "                                   'random_state': [200]\n",
    "                                   },\n",
    "}\n",
    "hyperparams = default_hyperparams_grid[\"RandomForestClassifier\"]\n",
    "scoring = {'MCC': 'matthews_corrcoef'}\n",
    "scoring_methods = {\n",
    "        'matthews_corrcoef': matthews_corrcoef\n",
    "    }\n",
    "scoring = {name: SCORERS[scorer] if scorer in SCORERS.keys() else make_scorer(scoring_methods[scorer]) for name, scorer in scoring.items()}\n",
    "scorer_names = list(scoring.keys())\n",
    "X_test1 = char_vectorizer.transform(X_test)\n",
    "X_test2 = word_vectorizer.transform(X_test)\n",
    "test_stack = hstack([X_test1,X_test2])\n",
    "X_train1 = char_vectorizer.transform(X_train)\n",
    "X_train2 = word_vectorizer.transform(X_train)\n",
    "train_stack = hstack([X_train1,X_train2])\n",
    "        \n",
    "search = GridSearchCV(model, param_grid=hyperparams, scoring=scoring, n_jobs=16, cv=10, error_score=0,\n",
    "                              refit=scorer_names[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "search.fit(train_stack, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=50, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=0.01, min_samples_split=0.02,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=200,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
