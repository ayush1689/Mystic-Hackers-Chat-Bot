{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import hstack\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in single_sentence]\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString\n",
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)\n",
    "def tokenize_test(model,train,validation):\n",
    "\n",
    "    X_test1 = char_vectorizer.transform(validation)\n",
    "    X_test2 = word_vectorizer.transform(validation)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    #train_text = vect.fit_transform(train[\"Text\"])\n",
    "    #print ('Features: ', train_text.shape[1])\n",
    "    #test_text = vect.transform(validation[\"Text\"])\n",
    "    train_features2 = Normalizer(copy=False).fit_transform(train_features1)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    \n",
    "    \n",
    "    model.fit(train_features2, y_train)\n",
    "    filename = 'model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    #word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    #char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    #X_test1 = word_model.transform(clntxt)\n",
    "    #X_test2 = char_model.transform(clntxt)\n",
    "    #test_stack = hstack([X_test1,X_test2])\n",
    "    svd_model=pickle.load( open( \"Pyhton_SVD_preprocessing.sav\", \"rb\" ) )\n",
    "    test_stack=svd_model.transform(test_stack)\n",
    "    \n",
    "    \n",
    "    best_thread = pairwise_distances_argmin(\n",
    "            X=test_stack,\n",
    "            Y=train_features2,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    print(best_thread)\n",
    "    \n",
    "    y_pred_class1 = model.predict(train_features2[best_thread])\n",
    "    y_pred_class2 = model.predict(test_stack)\n",
    "    print(y_pred_class1,y_pred_class2)\n",
    "    #return(y_pred_class1,y_pred_class2)\n",
    "    \n",
    "    \n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    \n",
    "    print(\"Training Accuracy\")\n",
    "    print(model.score(train_features2,y_train))\n",
    "    print(\"Testing Accuracy1\")\n",
    "    print(model.score(test_stack,y_test1))\n",
    "    print(\"Testing Accuracy2\")\n",
    "    print(model.score(train_features1[best_thread],y_test1))\n",
    "\n",
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)\n",
    "def tokenize_test2(model,train,text):\n",
    "    from sklearn.decomposition import NMF\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    print(text)\n",
    "    X_test1 = char_vectorizer.transform(text)\n",
    "    X_test2 = word_vectorizer.transform(text)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    train_features = Normalizer(copy=False).fit_transform(train_features1)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    model.fit(train_features, y_train)\n",
    "    filename = 'model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    return(y_pred_class)\n",
    "def tokenize_test3(text):\n",
    "    print(text)\n",
    "    clntxt=cleanData(text)\n",
    "    word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_feature_name:18105\n",
      "word_feature_name:2151\n",
      "Explained Variance:0.99988794\n"
     ]
    }
   ],
   "source": [
    "#stats=pd.read_excel('Ensemble.xlsx',encoding =  \"ISO-8859-1\")\n",
    "stats=pd.read_csv('ALL_merged.csv',encoding =  \"ISO-8859-1\")\n",
    "#stats.columns=[\"Topic_Code\",\"Topic_Categories\",\"Question\",\"Response\"]\n",
    "labelEncoder = LabelEncoder()\n",
    "df = dict(zip(stats.Sub_Topic_Code, stats.Response))\n",
    "#df=stats[[\"Topic_Name\",\"Response\"]].to_dict()\n",
    "#labelEncoder.fit(df)\n",
    "\n",
    "\n",
    "stats[\"Topic_Name\"] = labelEncoder.fit_transform(stats[\"Sub_Topic_Code\"])\n",
    "#df = dict(zip(stats.Topic_Name, stats.Response))\n",
    "#stats[\"Topic_Name\"] = labelEncoder.transform(stats[\"Response\"])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('labelEncoder.pickle', 'wb') as file:\n",
    "    pickle.dump(labelEncoder, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('LabelDictionary.pickle', 'wb') as file:\n",
    "    pickle.dump(df, file, pickle.HIGHEST_PROTOCOL)\n",
    "stats = stats.rename(columns={'Topic_Name': 'Category', 'Question': 'Text'})\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"Text\"]=data_df.Text.apply(cleanString)\n",
    "y=data_df[\"Category\"]\n",
    "X=data_df[\"Text\"]\n",
    "\n",
    "with open('Data_df.pickle', 'wb') as file:\n",
    "    pickle.dump(data_df, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "posts_root1=[]\n",
    "\n",
    "for post in X_train:\n",
    "      posts_root1.append(post)\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words = 'english',\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1,5),\n",
    "    norm='l2',\n",
    "    dtype=np.float32,\n",
    "    max_features=3000,\n",
    "    \n",
    ")\n",
    "# Character Stemmer\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 7),\n",
    "    dtype=np.float32,\n",
    "    norm='l2',\n",
    "    max_features=20000,\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "word_vectorizer.fit(posts_root1)\n",
    "char_vectorizer.fit(posts_root1)\n",
    "\n",
    "\n",
    "train_word_features = word_vectorizer.transform(posts_root1)\n",
    "train_char_features = char_vectorizer.transform(posts_root1)\n",
    "\n",
    "\n",
    "\n",
    "train_features = hstack([\n",
    "    train_char_features,\n",
    "    train_word_features])\n",
    "tsvd=TruncatedSVD(500)\n",
    "\n",
    "train_features1 = tsvd.fit_transform(train_features)\n",
    "#explained_variances = np.var(train_features1) / np.var(train_features).sum()\n",
    "filename = 'train_features1_preprocessing.sav'\n",
    "pickle.dump(train_features1, open(filename, 'wb'))\n",
    "filename = 'Python_word_preprocessing.sav'\n",
    "pickle.dump(word_vectorizer, open(filename, 'wb'))\n",
    "filename = 'Pyhton_char_preprocessing.sav'\n",
    "pickle.dump(char_vectorizer, open(filename, 'wb'))\n",
    "filename = 'Pyhton_SVD_preprocessing.sav'\n",
    "pickle.dump(tsvd, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "print(\"char_feature_name:%s\" % len(char_vectorizer.get_feature_names()))\n",
    "print(\"word_feature_name:%s\" % len(word_vectorizer.get_feature_names()))\n",
    "print(\"Explained Variance:%s\" % tsvd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<354x20905 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 53281 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=stats.drop([\"Unnamed: 4\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  83 401 322 214 505  18 141 487  15 409 167 460  86 113 254 443 281\n",
      " 532 454 469 146 217 287 399 294 491 231 152  90 474 241 361 446 132  46\n",
      " 249 440 140 163 242  32 163 124  62 198 254 469 263  49 135 301 371  14\n",
      " 178 134 368 207 120 202 143 170 433 195 245 478 333 171 518 319 339 320\n",
      " 143  67 100 477 287 180 101 155 410 154  56 222 357 150 423 443 513 520\n",
      " 240 489 458 323 295 314 436 183 130 500 226 355  37 233 142  71  77  20\n",
      " 101 342 299 447 229  84 258 398 111  38 321 352 508 213  11 170 185 148\n",
      " 117 409  39 409 230 495 398 136]\n",
      "[30 54  1 54 35 78 64 60 47 70 63 64 55 32 72 81 30 51  4 51 71 64 45 78\n",
      " 73 45 60 54 22 52 64 30 49 81 41 85 65 36 53 47 55 30 47 32 34 81 81 71\n",
      " 47 72 43 58 77 58 28 56 39 48 48 64 21 18 85 48 71  4 78 15 61 46 46  3\n",
      " 21 23 52 48 78 42 28 71 70 52 46 53 52 60 46 30 86 85 58 31 60 16 19 34\n",
      " 63 83 72 84 59 20 65  5 44  8 50 47 28 60 54 56 47 46 82 60 55  3 47  4\n",
      " 42  2 64 18 73 44 61 63 83 63 55 63 60 84] [42 54  1 54 35 78 64 60 47 70 63 64 55 32 72 81 30 51 71 51 71 64 45 78\n",
      " 73 45 60 54 22 52 64 30 49 81 41 85 65 36 53 47 60 30 47 32 34 81 81 56\n",
      " 47 72 43 58 77 58 28 56 39 48 48 64 21 61 85 48 71  4 78 15 61 46 46  3\n",
      " 21 23 66 48 78 42 28 71 70 52 46 53 52 60 46 42 86 85 58 31 61 16 19 34\n",
      " 63 83 72 84 59 20 65  5 44  8 50 47 28 60 54 56 47 46 82 60 55  3 47  4\n",
      " 42  2 64 18 73 44 61 63 83 63 55 63 60 84]\n",
      "Training Accuracy\n",
      "0.9869158878504672\n",
      "Testing Accuracy1\n",
      "0.9253731343283582\n",
      "Testing Accuracy2\n",
      "0.8880597014925373\n"
     ]
    }
   ],
   "source": [
    "stats.columns=[\"Topic_Code\",\"Topic_Categories\",\"Question\",\"Response\",\"Categories\"]\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"Question\"]=data_df.Question.apply(cleanString)\n",
    "\n",
    "\n",
    "y=data_df[\"Categories\"]\n",
    "X=data_df[\"Question\"]\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "mlp=MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.75,\n",
    "              beta_2=0.8, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(200, 150), learning_rate='constant',\n",
    "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
    "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "              power_t=0.5, random_state=200, shuffle=True, solver='adam',\n",
    "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "              warm_start=False)\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "tokenize_test(mlp,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Topic_Categories</th>\n",
       "      <th>Text</th>\n",
       "      <th>Response</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Dictionary</td>\n",
       "      <td>key value pair used  dictionary</td>\n",
       "      <td>https://chrisalbon.com/#python</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK03</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>linear relationship</td>\n",
       "      <td>https://towardsdatascience.com/linear-regressi...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>Measures of Dispersion, Range, IQR,  Standard ...</td>\n",
       "      <td>range</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Statistical_disp...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Misc</td>\n",
       "      <td>sample randomly   panda dataframe</td>\n",
       "      <td>https://www.kaggle.com/dark4user/python-progra...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max depth parameter  random forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>AASKK06</td>\n",
       "      <td>Distance Calculation methods</td>\n",
       "      <td>different type  distance calculation technique</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>AASKK03</td>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>binary logistic regression</td>\n",
       "      <td>https://towardsdatascience.com/logistic-regres...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>AASKK03</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>type  machine learning</td>\n",
       "      <td>https://vinodsblog.com/2018/10/08/machine-lear...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>type  voting process  available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>AASKK04</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>still   question  decision tree</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Decision_tree</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>717 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Code                                   Topic_Categories  \\\n",
       "0      AASKK01                                         Dictionary   \n",
       "1      AASKK03                                  Linear Regression   \n",
       "2      AASKK02  Measures of Dispersion, Range, IQR,  Standard ...   \n",
       "3      AASKK01                                               Misc   \n",
       "4      AASKK01                                      Random Forest   \n",
       "..         ...                                                ...   \n",
       "712    AASKK06                       Distance Calculation methods   \n",
       "713    AASKK03                                Logistic regression   \n",
       "714    AASKK03                                       Introduction   \n",
       "715    AASKK01                                      Random Forest   \n",
       "716    AASKK04                                      Decision Tree   \n",
       "\n",
       "                                                Text  \\\n",
       "0                   key value pair used  dictionary    \n",
       "1                               linear relationship    \n",
       "2                                             range    \n",
       "3                sample randomly   panda dataframe     \n",
       "4               max depth parameter  random forest     \n",
       "..                                               ...   \n",
       "712  different type  distance calculation technique    \n",
       "713                      binary logistic regression    \n",
       "714                          type  machine learning    \n",
       "715                type  voting process  available     \n",
       "716                still   question  decision tree     \n",
       "\n",
       "                                              Response  Category  \n",
       "0                       https://chrisalbon.com/#python        35  \n",
       "1    https://towardsdatascience.com/linear-regressi...        56  \n",
       "2    https://en.wikipedia.org/wiki/Statistical_disp...        59  \n",
       "3    https://www.kaggle.com/dark4user/python-progra...        62  \n",
       "4                                                  NaN        74  \n",
       "..                                                 ...       ...  \n",
       "712                                                NaN        36  \n",
       "713  https://towardsdatascience.com/logistic-regres...        58  \n",
       "714  https://vinodsblog.com/2018/10/08/machine-lear...        50  \n",
       "715                                                NaN        74  \n",
       "716        https://en.wikipedia.org/wiki/Decision_tree        34  \n",
       "\n",
       "[717 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Topic_Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>Unsupervised Learnings Basic</td>\n",
       "      <td>basics of unsupervised learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unsupervised_lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>Unsupervised Learnings Basic</td>\n",
       "      <td>learn  unsupervised leering</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unsupervised_lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>Unsupervised Learnings Basic</td>\n",
       "      <td>What is the principle of Unsupervised learnings?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unsupervised_lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>Unsupervised Learnings Basic</td>\n",
       "      <td>what is unsupervised learnings</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unsupervised_lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>Unsupervised Learnings Basic</td>\n",
       "      <td>What is use of unsupervised learning techniques</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Unsupervised_lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>PCA</td>\n",
       "      <td>How to do PCA</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Principal_compon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>SVM</td>\n",
       "      <td>What is SVM</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Support_vector_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>AASKK05</td>\n",
       "      <td>SVM</td>\n",
       "      <td>How to do SVM</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Support_vector_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Code                    Topic_Name  \\\n",
       "0     AASKK05  Unsupervised Learnings Basic   \n",
       "1     AASKK05  Unsupervised Learnings Basic   \n",
       "2     AASKK05  Unsupervised Learnings Basic   \n",
       "3     AASKK05  Unsupervised Learnings Basic   \n",
       "4     AASKK05  Unsupervised Learnings Basic   \n",
       "..        ...                           ...   \n",
       "89    AASKK05                           PCA   \n",
       "90    AASKK05                           SVM   \n",
       "91    AASKK05                           SVM   \n",
       "92        NaN                           NaN   \n",
       "93        NaN                           NaN   \n",
       "\n",
       "                                            Question  \\\n",
       "0                    basics of unsupervised learning   \n",
       "1                        learn  unsupervised leering   \n",
       "2   What is the principle of Unsupervised learnings?   \n",
       "3                    what is unsupervised learnings    \n",
       "4   What is use of unsupervised learning techniques    \n",
       "..                                               ...   \n",
       "89                                     How to do PCA   \n",
       "90                                       What is SVM   \n",
       "91                                     How to do SVM   \n",
       "92                                               NaN   \n",
       "93                                               NaN   \n",
       "\n",
       "                                             Response  \n",
       "0   https://en.wikipedia.org/wiki/Unsupervised_lea...  \n",
       "1   https://en.wikipedia.org/wiki/Unsupervised_lea...  \n",
       "2   https://en.wikipedia.org/wiki/Unsupervised_lea...  \n",
       "3   https://en.wikipedia.org/wiki/Unsupervised_lea...  \n",
       "4   https://en.wikipedia.org/wiki/Unsupervised_lea...  \n",
       "..                                                ...  \n",
       "89  https://en.wikipedia.org/wiki/Principal_compon...  \n",
       "90  https://en.wikipedia.org/wiki/Support_vector_m...  \n",
       "91  https://en.wikipedia.org/wiki/Support_vector_m...  \n",
       "92                                                NaN  \n",
       "93                                                NaN  \n",
       "\n",
       "[94 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Topic_Categories</th>\n",
       "      <th>Text</th>\n",
       "      <th>Response</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK04</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>More information on Random Forest?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Random_forest</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK04</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Can you provide me more details on Random Forest?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Random_forest</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK04</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>I need more details on Random Forest?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Random_forest</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK04</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Where can I find more information on Random Fo...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Random_forest</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK04</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Can I get more info on Random Forest?</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Random_forest</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Boosting</td>\n",
       "      <td>What is Gradient Boosting?</td>\n",
       "      <td>https://chrisalbon.com/#regex</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Boosting</td>\n",
       "      <td>What is Extreme Gradient (XG) Boosting?</td>\n",
       "      <td>https://chrisalbon.com/#regex</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>What is Bagging?</td>\n",
       "      <td>https://chrisalbon.com/#regex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>What is Bagging Classifier algorithm?</td>\n",
       "      <td>https://chrisalbon.com/#regex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>AASKK01</td>\n",
       "      <td>Bagging</td>\n",
       "      <td>What are the demerits of Bagging?</td>\n",
       "      <td>https://chrisalbon.com/#regex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Code Topic_Categories  \\\n",
       "0      AASKK04    Random Forest   \n",
       "1      AASKK04    Random Forest   \n",
       "2      AASKK04    Random Forest   \n",
       "3      AASKK04    Random Forest   \n",
       "4      AASKK04    Random Forest   \n",
       "..         ...              ...   \n",
       "438    AASKK01         Boosting   \n",
       "439    AASKK01         Boosting   \n",
       "440    AASKK01          Bagging   \n",
       "441    AASKK01          Bagging   \n",
       "442    AASKK01          Bagging   \n",
       "\n",
       "                                                  Text  \\\n",
       "0                   More information on Random Forest?   \n",
       "1    Can you provide me more details on Random Forest?   \n",
       "2                I need more details on Random Forest?   \n",
       "3    Where can I find more information on Random Fo...   \n",
       "4                Can I get more info on Random Forest?   \n",
       "..                                                 ...   \n",
       "438                         What is Gradient Boosting?   \n",
       "439            What is Extreme Gradient (XG) Boosting?   \n",
       "440                                   What is Bagging?   \n",
       "441              What is Bagging Classifier algorithm?   \n",
       "442                  What are the demerits of Bagging?   \n",
       "\n",
       "                                        Response  Category  \n",
       "0    https://en.wikipedia.org/wiki/Random_forest        69  \n",
       "1    https://en.wikipedia.org/wiki/Random_forest        69  \n",
       "2    https://en.wikipedia.org/wiki/Random_forest        69  \n",
       "3    https://en.wikipedia.org/wiki/Random_forest        69  \n",
       "4    https://en.wikipedia.org/wiki/Random_forest        69  \n",
       "..                                           ...       ...  \n",
       "438                https://chrisalbon.com/#regex         3  \n",
       "439                https://chrisalbon.com/#regex         3  \n",
       "440                https://chrisalbon.com/#regex         1  \n",
       "441                https://chrisalbon.com/#regex         1  \n",
       "442                https://chrisalbon.com/#regex         1  \n",
       "\n",
       "[443 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ANOVA', 'Bayes theorem and Example', 'Boxplot and whisker plots',\n",
       "       'Central Tendency and 3 Ms', 'Coefficient of Variation',\n",
       "       'Concepts of sampling distribution and Central Limit Theorem',\n",
       "       'Confidence Intervals and tests', 'Correlation Analysis',\n",
       "       'Covariance', 'Data Analysis', 'Data Preprocessing',\n",
       "       'Data Visualizations', 'Dictionary', 'Distributions and Example',\n",
       "       'Exploratory Data analysis', 'Feature Selection', 'File',\n",
       "       'Five Number Summary', 'GroupBy', 'Hypothesis  Tests',\n",
       "       'Hypothesis Formulation', 'Lists',\n",
       "       'Measures of Dispersion, Range, IQR,  Standard Deviation, Variance',\n",
       "       'Misc', 'Missing', 'Numpy', 'Pandas', 'PivotTable', 'Ploting',\n",
       "       'Probability - Meaning and concepts', 'Python Introduction',\n",
       "       'RegularExpression', 'Scaling and Normalization', 'Sets',\n",
       "       'Skewness ', 'Statistics', 'String', 'Test of variance',\n",
       "       'The Empirical Rule and Chebyshev Rule'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTesting-ANOVA/BS704_HypothesisTesting-Anova_print.html'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[labelEncoder.classes_[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[222 197 327 200 214 526 466  68  84  95 505 524 315 374 382 459 330 390\n",
      " 590  81 227 537 398 309 623 177 282 206 621 147 116   4  50 229 603 420\n",
      "  88 371 469 472 157 176 190 282 605 387 479 229 154 582 119 484 508 384\n",
      " 226 271 210  34 252 576  80 480 104 366 575 414 310 244 335 287 278 180\n",
      " 317  23 567 411  52 589 142 586 491 476  78  36 109 372  29 555 380 272\n",
      "  95 460 242 153 336   5 289 583 594 536 313 354 362 243 320 175 397 430\n",
      " 464 162 224 313 344 318 168 378 127 212 253 134 194  95 383 181 184 544\n",
      " 182  66 202 326 503  16 444 425 125   9 107 192 260 461 179  10 612  57\n",
      " 160 368 376 301 110 113 215 436 334]\n",
      "[70 70 60 70 64 73 61 67 56 56 56 60 56 52 43 62 77 43 49 60 65 64 78 64\n",
      " 60 65 67 60 70 78 62 52 61 43 62 63 49 62 61 65 52 52 60 67 49 60 62 43\n",
      " 56 49 65 63 52 60 60 65 77 73 60 67 65 65 62 60 49 56 65 49 67 60 49 67\n",
      " 60 60 49 62 61 65 49 67 78 49 56 43 52 61 65 62 61 43 56 49 67 43 70 63\n",
      " 65 52 62 60 43 43 64 62 77 60 65 67 67 62 77 43 43 67 67 73 56 77 65 63\n",
      " 77 56 65 56 61 70 65 65 65 60 60 52 56 63 70 73 78 62 78 63 62 61 65 60\n",
      " 49 56 43 77 67 70 65 77 52] [70 70 67 70 64 73 61 67 56 56 56 60 56 52 43 62 65 43 49 60 65 64 62 64\n",
      " 60  1 67 60 70 78 62 52 60 43 62 63 49 62 61 65 52 52 60 67 49 60 62 43\n",
      " 56 49 65 63 52 60 60 65 77 73 60  0 65 65 62 60 49 56 65 49 67 60 49 67\n",
      " 65 60 49 62 61 65 49 67 78 49 56 43 52 61 65 62 61 43 56 49 67 43 70 63\n",
      " 65 52 62 60 43 43 64 62 77 60 65 67 67 62 77 43 43 67 67 73 56 77 65 63\n",
      " 77 56 65 56 61 70 65 65 65 60 60 52 56 63 70 73 78 62 78 63 62 61 65 60\n",
      " 49 56 43 77 67 70 65 77 52]\n",
      "Training Accuracy\n",
      "0.918918918918919\n",
      "Testing Accuracy1\n",
      "0.9411764705882353\n",
      "Testing Accuracy2\n",
      "0.9869281045751634\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=50, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=0.01, min_samples_split=0.02,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=None, oob_score=False, random_state=200,\n",
    "                       verbose=0, warm_start=False)\n",
    "tokenize_test(rf,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[222 197 327 200 214 526 466  68  84  95 505 524 315 374 382 459 330 390\n",
      " 590  81 227 537 398 309 623 177 282 206 621 147 116   4  50 229 603 420\n",
      "  88 371 469 472 157 176 190 282 605 387 479 229 154 582 119 484 508 384\n",
      " 226 271 210  34 252 576  80 480 104 366 575 414 310 244 335 287 278 180\n",
      " 317  23 567 411  52 589 142 586 491 476  78  36 109 372  29 555 380 272\n",
      "  95 460 242 153 336   5 289 583 594 536 313 354 362 243 320 175 397 430\n",
      " 144 162 224 313 344 318 168 378 127 212 253 134 194  95 383 181 184 544\n",
      " 182  66 202 326 503  16 444 425 125   9 107 192 260 461 179  10 612  57\n",
      " 160 368 376 301 110 113 215 436 334]\n",
      "[70 70 69 56 65 73 61 69 56 56 56 60 56 52 43 69 69 43 49 60 65 52 43 65\n",
      " 44 69 67 60 70 78 62 52 43 43 62 52 49 62 61 65 52 52 69 67 49 69 62 43\n",
      " 56 49 65 49 52 52 44 65 36 73 69 44 65 65 69 60 49 56 65 49 69 60 49 67\n",
      " 49 52 49 56 69 52 49 67 78 49 56 43 52 61 65 69 61 43 56 49 67 43 70 49\n",
      " 65 52 62 44 43 43 52 69 69 60 65 67 69 62 36 43 43 67 69 49 56 43 65 49\n",
      " 69 56 65 56 61 70 65 65 69 52 60 52 56 49 70 73 78 52 78 52 69 61 65 60\n",
      " 49 56 43 69 69 70 65 69 56] [70 70 67 56 65 56 61 67 56 56 56 60 56 52 43 69 62 43 49 60 65 52 43 65\n",
      " 60 69 67 60 70 78 62 52 43 43 62 52 49 62 61 65 52 52 69 67 49 69 62 43\n",
      " 56 49 65 49 52 52 44 65 36 73 69 69 65 65 69 60 49 56 65 49 69 60 49 67\n",
      " 49 52 49 56 69 52 49 67 78 49 56 43 52 61 65 69 61 43 56 49 67 43 70 49\n",
      " 65 52 62 44 43 43 52 69 69 60 65 67 67 62 36 43 43 67 69 49 56 43 65 49\n",
      " 69 56 65 56 61 70 65 65 67 52 60 52 56 49 70 73 78 52 78 52 69 61 65 60\n",
      " 67 56 43 69 69 70 65 69 56]\n",
      "Training Accuracy\n",
      "0.38950715421303655\n",
      "Testing Accuracy1\n",
      "0.6601307189542484\n",
      "Testing Accuracy2\n",
      "0.6535947712418301\n"
     ]
    }
   ],
   "source": [
    "sgd=SGDClassifier(alpha=0.01, average=False, class_weight=None,\n",
    "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "              l1_ratio=0.2, learning_rate='optimal', loss='hinge', max_iter=500,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
    "              power_t=0.5, random_state=200, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "tokenize_test(sgd,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 34 152 168 151  84 148  62 140  67 188  56 161 129 125 134  54  29 181\n",
      "  70  66 104   2  34 137 181 206 176  43  91 156 157 227  41 201 207  10\n",
      "  43 213 187  69 139  19  79 110   4 226  30  84  81 213 171 164 220 116\n",
      " 158 100 101 170 180  24 118 149 164  55 148  12  82 198  37 142 105  87\n",
      " 221  64  68 176 202 165  71  99 158 200  52 223 165 161  44  45 175  26\n",
      "  65 147  38 184 107  73 180 103 199  23 197  11 196 188  69 200 195 159\n",
      " 218 212   9  28  78 102  61  38 130 123 214 116 113 216 181 217 136 135\n",
      " 106 126 178 165  79 130 225 166 114  72 127 148 165 173  20  75   6  43\n",
      "  90  98  14   3  27   1  77  72  35]\n",
      "[12 21 23 30 23 21 24 30 26 25 25 21 28 12  2 28 12 23 35 12 23 24 12 23\n",
      " 23 23 23 12 26 16 30 36 23 25  3 10 12 36 10 14 30 24 27 24 28 21 35 23\n",
      " 31 36 35 30 23 18 33 21 31 27 21 18 25 30 30 16 21 23 25 18 23 25 25 36\n",
      " 12 31 12 23 28 18 35 24 33 26 21 21 18 21 33 23 21 23 25 31 16 16 31 25\n",
      " 21 24 30 36 16 28 21 25 14 26 21 35 16 25 18 30 35 27 28 16 28 28 30 18\n",
      " 31 28 23 18 30 28 28 24 25 18 27 28 23 28 26 12 35 21 18 18 31 16 25 12\n",
      " 30 10 35 28 16 31 16 12 16] [12 21 23 30 23 21 24 30 26 25 25 21 28 12 22 28 12 23 35 12 23 24 12 23\n",
      " 23 23 23 12 26 16 30 36 23 25  3 28 12 36 18 30 30 24 27 24 28 21 35 23\n",
      " 31 36 35 30 23 18 33 21 31 27 21 18 25 30 30 16 21 23 25 18 23 25 25 36\n",
      " 12 31 12 23 28 18 35 24 33 16 21 21 18 21 33 23 21 23 25 31 16 16 31 25\n",
      " 21 24 30 36 16 28 21 25 14 26 21 35 16 25 18 30 30 27 28 16 28 28 30 18\n",
      " 31 28 23 18 30 28 28 24 25 18 27 28 30 28 26 12 35 21 18 18 31 16 25 12\n",
      " 30 18 35 28 16 31 16 21 16]\n",
      "Training Accuracy\n",
      "1.0\n",
      "Testing Accuracy1\n",
      "0.934640522875817\n",
      "Testing Accuracy2\n",
      "0.9215686274509803\n"
     ]
    }
   ],
   "source": [
    "EnsembleClassifier = VotingClassifier(estimators = [ ('rf',rf),('mlp',mlp),('sgd1',sgd)], voting = 'hard', weights = [1,1,1])\n",
    "\n",
    "tokenize_test(EnsembleClassifier,X_train1,X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
