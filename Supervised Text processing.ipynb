{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\CP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review,stopWords):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nonan = re.compile(r'[^a-zA-Z ]')\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = nonan.sub('', sentence_token[j])\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in single_sentence]\n",
    "        single_sentence = [w for w in single_sentence if not w.isdigit() and not w in stopWords and not w in string.punctuation]\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(dataframe, column_name, training_split = 0.6, validation_split = 0.2, test_split = 0.2):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into trainingset, validationset and testset in specified ratio.\n",
    "    All sets are balanced, which means they have the same ratio for each categorie as the full set.\n",
    "    Input:   dataframe        - Pandas Dataframe, should include a column for data and one for categories\n",
    "             column_name      - Name of dataframe column which contains the categorical output values\n",
    "             training_split   - from ]0,1[, default = 0.6\n",
    "             validation_split - from ]0,1[, default = 0.2        \n",
    "             test_split       - from ]0,1[, default = 0.2\n",
    "                                Sum of all splits need to be 1\n",
    "    Output:  train            - Pandas DataFrame of trainset\n",
    "             validation       - Pandas DataFrame of validationset\n",
    "             test             - Pandas DataFrame of testset\n",
    "    \"\"\"\n",
    "    if training_split + validation_split + test_split != 1.0:\n",
    "        raise ValueError('Split paramter sum should be 1.0')\n",
    "        \n",
    "    total = len(dataframe.index)\n",
    " \n",
    "    train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
    "    .reset_index(drop=True).set_index('index')\n",
    "    train = train.sample(frac=1)\n",
    "    temp_df = dataframe.drop(train.index)\n",
    "    validation = temp_df.reset_index().groupby(column_name)\\\n",
    "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
    "           .reset_index(drop=True).set_index('index')\n",
    "    validation = validation.sample(frac=1)\n",
    "    test = temp_df.drop(validation.index)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    print('Total: ', len(dataframe))\n",
    "    print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
    "    print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
    "    print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spvl=pd.read_excel('Supervised Learning-Chatbot.xlsx',encoding =  \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>QUESTIONS</th>\n",
       "      <th>TAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AASKK1</td>\n",
       "      <td>What is Supervised learning?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AASKK2</td>\n",
       "      <td>What is regression ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AASKK3</td>\n",
       "      <td>What is slope ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CODE                     QUESTIONS  TAG\n",
       "0  AASKK1  What is Supervised learning?    0\n",
       "1  AASKK2          What is regression ?    0\n",
       "2  AASKK3               What is slope ?    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\cp\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cp\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in c:\\users\\cp\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\cp\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\cp\\anaconda3\\lib\\site-packages (from nltk) (0.15.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000004A7C828>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')': /simple/wordcloud/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000004A7C7B8>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')': /simple/wordcloud/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000004A7C4E0>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')': /simple/wordcloud/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000004A7C438>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')': /simple/wordcloud/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000004A7C208>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')': /simple/wordcloud/\n",
      "ERROR: Could not find a version that satisfies the requirement wordcloud (from versions: none)\n",
      "ERROR: No matching distribution found for wordcloud\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# NLP packages\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "# Modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pylab import rcParams\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rcParams['figure.figsize'] = 14, 6\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and performing basic analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spvl=pd.read_excel('Supervised Learning-Chatbot.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>Text</th>\n",
       "      <th>TAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AASKK1</td>\n",
       "      <td>What is Supervised learning</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AASKK2</td>\n",
       "      <td>What is regression</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AASKK3</td>\n",
       "      <td>What is slope</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CODE                         Text  TAG\n",
       "0  AASKK1  What is Supervised learning    0\n",
       "1  AASKK2           What is regression    0\n",
       "2  AASKK3                What is slope    0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAF2CAYAAAC7ytMRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRUBZ7+/6dSZRJCTEiqIBgWZW2oVgIYGoyAIAXDqK3oQVxGHA4CIioqiixf2/QM4InGdJAmCGMDLTi22wyMWysdbJYOLkBCg0SFtCJqAiE7SEJMqn5/8LPOTSeBAlJ1w837dY7HulvVEz6njjzeJTafz+cTAAAAAECSFGZ2AAAAAABoTShJAAAAAGBASQIAAAAAA0oSAAAAABhQkgAAAADAgJIEAAAAAAaUJAAAAAAwcJgdIJgKCwvNjiBJcrlcKikpMTsGWhhztS5ma03M1bqYrTUxV2tqbXNNTExscj1nkgAAAADAgJIEAAAAAAaUJAAAAAAwoCQBAAAAgAElCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMCAkgQAAAAABo5QfEhJSYmysrJUUVEhm80mj8ejG264QSdOnFBmZqaOHTumjh076rHHHlN0dHSj4/fs2aO1a9fK6/VqzJgxmjBhQihiAwAAAGiDQlKS7Ha7Jk+erJ49e6q6ulrz58/XgAEDtGXLFl111VWaMGGCNm7cqI0bN+qee+5pcKzX69Xq1av11FNPyel0asGCBUpOTlbXrl1DER0AAABAGxOSy+3i4uLUs2dPSVK7du3UpUsXlZWVaefOnbruuuskSdddd5127tzZ6NiCggJ17txZCQkJcjgcSklJaXI/AAAAAGgJITmTZFRcXKxvvvlGvXv3VmVlpeLi4iSdLlJVVVWN9i8rK5PT6fQvO51OHTx4sMn3zs7OVnZ2tiQpLS1NLpcrCD/BuXM4HK0mC87f0VtTGi4bXids2BHaMAgqvrPWxFyti9laE3O1potlriEtSTU1NcrIyNCUKVMUFRUV0DE+n6/ROpvN1uS+Ho9HHo/Hv1xSUnJ+QVuYy+VqNVkQHMzXWvjOWhNztS5ma03M1Zpa21wTExObXB+yp9vV1dUpIyNDI0aM0NChQyVJsbGxKi8vlySVl5crJiam0XFOp1OlpaX+5dLSUv/ZJwAAAABoaSEpST6fTytXrlSXLl100003+dcnJydr69atkqStW7dqyJAhjY7t1auXioqKVFxcrLq6Ou3YsUPJycmhiA0AAACgDQrJ5XZfffWVtm3bpu7du2vu3LmSpLvuuksTJkxQZmamPvroI7lcLs2ZM0fS6fuQVq1apQULFshut2vq1KlasmSJvF6vRo8erW7duoUiNgAAAIA2yOZr6qYfiygsLDQ7gqTWd+0lzk/99Jub3WZ/6e0QJkGw8Z21JuZqXczWmpirNbW2uZp+TxIAAAAAXAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAA0oSAAAAABhQkgAAAADAgJIEAAAAAAaUJAAAAAAwoCQBAAAAgAElCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGjlB8yIoVK5Sbm6vY2FhlZGRIkjIzM1VYWChJOnnypKKiopSent7o2AcffFCRkZEKCwuT3W5XWlpaKCIDAAAAaKNCUpJGjRql8ePHKysry7/uscce879et26doqKimj0+NTVVMTExQc0IAAAAAFKILrdzu92Kjo5ucpvP59PHH3+sa6+9NhRRAAAAAOCMQnIm6Uy++OILxcbG6rLLLmt2nyVLlkiSxo4dK4/H0+x+2dnZys7OliSlpaXJ5XK1bNjz5HA4Wk0WnL+jZ9jGfK2F76w1MVfrYrbWxFyt6WKZq+klKScn54xnkRYtWqT4+HhVVlZq8eLFSkxMlNvtbnJfj8fToESVlJS0eN7z4XK5Wk0WBAfztRa+s9bEXK2L2VoTc7Wm1jbXxMTEJteb+nS7+vp6ffbZZ0pJSWl2n/j4eElSbGyshgwZooKCglDFAwAAANAGmVqS9u3bp8TERDmdzia319TUqLq62v9679696t69eygjAgAAAGhjQnK53dKlS5Wfn6/jx49r5syZmjRpkq6//vomL7UrKyvTqlWrtGDBAlVWVur555+XdPqs0/DhwzVw4MBQRAYAAADQRtl8Pp/P7BDB8vPvYTJba7v2EuenfvrNzW6zv/R2CJMg2PjOWhNztS5ma03M1Zpa21xb5T1JAAAAANDaUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAA0oSAAAAABhQkgAAAADAgJIEAAAAAAaUJAAAAAAwoCQBAAAAgAElCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAICBIxQfsmLFCuXm5io2NlYZGRmSpDfeeEObN29WTEyMJOmuu+7S4MGDGx27Z88erV27Vl6vV2PGjNGECRNCERkAAABAGxWSkjRq1CiNHz9eWVlZDdbfeOONuvnmm5s9zuv1avXq1XrqqafkdDq1YMECJScnq2vXrsGODAAAAKCNCsnldm63W9HR0ed8XEFBgTp37qyEhAQ5HA6lpKRo586dQUgIAAAAAKeF5ExScz788ENt27ZNPXv21L333tuoSJWVlcnpdPqXnU6nDh48GOqYAAAAANoQ00rSuHHjNHHiREnS66+/rnXr1mnWrFkN9vH5fI2Os9lszb5ndna2srOzJUlpaWlyuVwtmPj8ORyOVpMF5+/oGbYxX2vhO2tNzNW6mK01MVdruljmalpJ6tChg//1mDFj9Oyzzzbax+l0qrS01L9cWlqquLi4Zt/T4/HI4/H4l0tKSloo7YVxuVytJguCg/laC99Za2Ku1sVsrYm5WlNrm2tiYmKT6017BHh5ebn/9WeffaZu3bo12qdXr14qKipScXGx6urqtGPHDiUnJ4cyJgAAAIA2JiRnkpYuXar8/HwdP35cM2fO1KRJk7R//34dOnRINptNHTt21IwZMySdvg9p1apVWrBggex2u6ZOnaolS5bI6/Vq9OjRTZYpAAAAAGgpNl9TN/5YRGFhodkRJLW+04o4P/XTm39cvf2lt0OYBMHGd9aamKt1MVtrYq7W1Nrm2uoutwMAAACA1oiSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAA0oSAAAAABhQkgAAAADAgJIEAAAAAAaUJAAAAAAwoCQBAAAAgAElCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGDgCMWHrFixQrm5uYqNjVVGRoYkaf369dq9e7ccDocSEhI0a9YstW/fvtGxDz74oCIjIxUWFia73a60tLRQRAYAAADQRoWkJI0aNUrjx49XVlaWf92AAQN09913y26365VXXtGGDRt0zz33NHl8amqqYmJiQhEVAAAAQBsXksvt3G63oqOjG6xLSkqS3W6XJPXt21dlZWWhiAIAAAAAZxSSM0ln89FHHyklJaXZ7UuWLJEkjR07Vh6PJ1SxAAAAALRBppek//3f/5XdbteIESOa3L5o0SLFx8ersrJSixcvVmJiotxud5P7ZmdnKzs7W5KUlpYml8sVtNznwuFwtJosOH9Hz7CN+VoL31lrYq7WxWytibla08UyV1NL0pYtW7R79249/fTTstlsTe4THx8vSYqNjdWQIUNUUFDQbEnyeDwNzjSVlJS0fOjz4HK5Wk0WBAfztRa+s9bEXK2L2VoTc7Wm1jbXxMTEJteb9gjwPXv26P/+7/80b948RURENLlPTU2Nqqur/a/37t2r7t27hzImAAAAgDYm4DNJu3bt0qBBg/wPWzgXS5cuVX5+vo4fP66ZM2dq0qRJ2rBhg+rq6rRo0SJJUp8+fTRjxgyVlZVp1apVWrBggSorK/X8889Lkurr6zV8+HANHDjwnD8fAAAAAAJl8/l8vkB2nDt3rsrKypSSkqKRI0eqT58+wc52wQoLC82OIKn1nVbE+amffnOz2+wvvR3CJAg2vrPWxFyti9laE3O1ptY21+Yutwv4TFJ6eroOHTqk7du3KyMjQxERERo5cqRGjBihTp06tVhQAAAAADDTOT244YorrtAVV1yhe+65R/v27dP69ev1xhtvqF+/fvJ4PLr22msVFmbabU4AAAAAcMHO+el2R44c0fbt27V9+3bZbDbdcccdcrlc+uCDD/Tpp5/qiSeeCEZOAAAAAAiJgEvSBx98oO3bt+vIkSO65ppr9NBDD6lv377+7UOHDtW0adOCEhIAAAAAQiXgkrRnzx7ddNNNGjJkiByOxodFRERwFgkAAADARS/gkjRnzhyFhYU1KEh1dXXy+Xy65JJLJElJSUktnxAAAAAAQijgpywsWbJEX3/9dYN1X3/9tZYsWdLioQAAAADALAGXpG+//bbR70bq3bu3vv322xYPBQAAAABmCbgktW/fXpWVlQ3WVVZWKiIiosVDAQAAAIBZAi5JQ4cO1QsvvKDDhw/r1KlTOnz4sJYvX65rrrkmmPkAAAAAIKQCfnDDnXfeqXXr1mnhwoX66aefFB4erlGjRumuu+4KZj4AAAAACKmAS1J4eLimTZum++67T8ePH9ell14qm80WzGwAAAAAEHIBlyRJOnnypAoLC1VTU9Ng/ZVXXtmioQAAAADALAGXpC1btmj16tWKjIxUeHi4f73NZtPy5cuDEg4AAAAAQi3gkvSnP/1Jc+bM0aBBg4KZBwAAAABMFfDT7bxer5KSkoKZBQAAAABMF3BJuuWWW/Q///M/8nq9wcwDAAAAAKYK+HK79957TxUVFXr77bcVHR3dYNuLL77Y4sEAAAAAwAwBl6SHH344mDkAAAAAoFUIuCS53e5g5gAAAACAViHgkvTTTz/prbfeUk5Ojo4fP66XX35Zf//731VUVKTx48cHMyMAAAAAhEzAD254+eWX9d1332n27Nmy2WySpG7dumnTpk1BCwcAAAAAoRbwmaTPPvtMy5YtU2RkpL8kxcfHq6ysLGjhAAAAACDUAj6T5HA4Gj3+u6qqSpdeemmLhwIAAAAAswRckoYNG6bly5eruLhYklReXq7Vq1crJSUlaOEAAAAAINQCLkl33323OnXqpMcff1wnT57U7NmzFRcXp9tvvz2Y+QAAAAAgpAK+J8nhcGjKlCmaMmWK/zK7n+9NAgAAAACrCLgkHT16tMFydXW1/3VCQkLLJQIAAAAAEwVckmbPnt3sttdff71FwgAAAACA2QIuSf9chCoqKvTmm2+qf//+LR4KAAAAAMwScEn6Zx06dNCUKVP0yCOPaPjw4Wfcd8WKFcrNzVVsbKwyMjIkSSdOnFBmZqaOHTumjh076rHHHlN0dHSjY/fs2aO1a9fK6/VqzJgxmjBhwvlGBgAAAICzCvjpdk0pLCzUqVOnzrrfqFGjtHDhwgbrNm7cqKuuukrLli3TVVddpY0bNzY6zuv1avXq1Vq4cKEyMzOVk5Oj77///kIiAwAAAMAZBXwm6emnn27wNLtTp07pu+++08SJE896rNvt9v9+pZ/t3LlTv/3tbyVJ1113nX7729/qnnvuabBPQUGBOnfu7H8wREpKinbu3KmuXbsGGhsAAAAAzknAJen6669vsBwZGanLL79cl1122Xl9cGVlpeLi4iRJcXFxqqqqarRPWVmZnE6nf9npdOrgwYPNvmd2drays7MlSWlpaXK5XOeVraU5HI5WkwXn7+gZtjFfa+E7a03M1bqYrTUxV2u6WOYacEkaNWpUEGM0zefzNVp3pt/N5PF45PF4/MslJSVByXWuXC5Xq8mC4GC+1sJ31pqYq3UxW2tirtbU2uaamJjY5Przfrpdc+64446A9ouNjVV5ebni4uJUXl6umJiYRvs4nU6Vlpb6l0tLS/1nnwAAAAAgGAJ+cENRUZE2btyozz//XEeOHNHnn3+ujRs3qqioSKWlpf5/ApWcnKytW7dKkrZu3aohQ4Y02qdXr14qKipScXGx6urqtGPHDiUnJwf8GQAAAABwrs7pEeCPPPKIhg0b5l/+9NNP9fHHH2vWrFlnPG7p0qXKz8/X8ePHNXPmTE2aNEkTJkxQZmamPvroI7lcLs2ZM0fS6fuQVq1apQULFshut2vq1KlasmSJvF6vRo8erW7dup3HjwkAAAAAgQm4JOXl5Wn27NkN1g0ZMkQrVqw467GPPvpok+uffvrpRuvi4+O1YMEC//LgwYM1ePDgQGMCAAAAwAUJ+HK7zp0764MPPmiw7sMPP1Tnzp1bPBQAAAAAmCXgM0kzZ87U888/r7ffflvx8fEqKyuT3W7X448/Hsx8AAAAABBSAZekHj166IUXXtDBgwdVXl6uDh06qG/fvnI4zum2JgAAAABo1QK+3O6fud1u1dXVqaampiXzAAAAAICpAj4NdPjwYT377LO65JJLVFpaqpSUFOXn52vr1q167LHHgpkRAAAAAEIm4DNJL730ku644w4tXbrUf4md2+3Wl19+GbRwAAAAABBqAZek77//XiNGjGiwLjIyUrW1tS0eCgAAAADMEnBJ6tixo77++usG6woKCngEOAAAAABLCfiepDvuuENpaWkaO3as6urqtGHDBv3lL3/R/fffH8x8AAAAABBSAZ9Juvrqq7VgwQJVVVXJ7Xbr2LFjeuKJJ5SUlBTMfAAAAAAQUgGdSfJ6vXrkkUf0u9/9TtOmTQt2Jss5emtKs9vsL70dwiQAAAAAziagM0lhYWEKCwvTTz/9FOw8AAAAAGCqgO9JuuGGG5SZmalbb71V8fHxstls/m0JCQlBCQcAAAAAoXbWklRRUaEOHTpozZo1kqS9e/c22uf1119v+WQAAAAAYIKzlqRHHnlEL7/8sr8Ipaena+7cuUEPBgAAAABmOOs9ST6fr8Fyfn5+0MIAAAAAgNnOWpKM9x4BAAAAgNWd9XK7+vp6ff755/5lr9fbYFmSrrzyypZPBgAAAAAmOGtJio2N1Ysvvuhfjo6ObrBss9m0fPny4KQDAAAAgBA7a0nKysoKRQ4AAAAAaBUC+mWyAAAAANBWUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADM76CHAAaAvqp9/sf320ie32l94OXRgExdFbU5rdxnwBAEacSQIAAAAAA0oSAAAAABhQkgAAAADAwNR7kgoLC5WZmelfLi4u1qRJk3TjjTf61+3fv1/PPfecOnXqJEkaOnSoJk6cGPKsAAAAANoGU0tSYmKi0tPTJUler1f333+/fvWrXzXar3///po/f36o4wEAAABog1rN5Xb79u1T586d1bFjR7OjAAAAAGjDWs0jwHNycnTttdc2ue3AgQOaO3eu4uLiNHnyZHXr1q3J/bKzs5WdnS1JSktLk8vlClrec9HU44R/1loy4uyYo7Wdab4SM7YCvsPW5XA4mKEFMVdruljm2ipKUl1dnXbv3q2777670bYePXpoxYoVioyMVG5urtLT07Vs2bIm38fj8cjj8fiXS0pKgpa5pVwMGXF2zNH6mLG1Md+Lm8vlYoYWxFytqbXNNTExscn1reJyu7y8PPXo0UMdOnRotC0qKkqRkZGSpMGDB6u+vl5VVVWhjggAAACgjWgVJelMl9pVVFTI5/NJkgoKCuT1enXppZeGMh4AAACANsT0y+1OnTqlvXv3asaMGf51mzZtkiSNGzdOn3zyiTZt2iS73a7w8HA9+uijstlsZsUFAAAAYHGml6SIiAitWbOmwbpx48b5X48fP17jx48PdSwAAAAAbVSruNwOAAAAAFoLShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAA0oSAAAAABg4zA4AAAAQqPrpNzdad/T//7f9pbdDGwaAZXEmCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAAOH2QEefPBBRUZGKiwsTHa7XWlpaQ22+3w+rV27Vnl5eYqIiNCsWbPUs2dPk9ICAAAAsDrTS5IkpaamKiYmpslteXl5OnLkiJYtW6aDBw/qD3/4g5555pkQJwQAAADQVrT6y+127dqlkSNHymazqW/fvvrxxx9VXl5udiwAAAAAFtUqziQtWbJEkjR27Fh5PJ4G28rKyuRyufzLTqdTZWVliouLa/Q+2dnZys7OliSlpaU1OM5MR8+wrbVkxNkxR2s703wlZmwFfIetgTm2HQ6Hg5la0MUyV9NL0qJFixQfH6/KykotXrxYiYmJcrvd/u0+n6/RMTabrcn38ng8DUpWSUlJywduYRdDRpwdc7Q+ZmxtzNcamKO1uFwuZmpBrW2uiYmJTa43/XK7+Ph4SVJsbKyGDBmigoKCBtudTmeDP8jS0tImzyIBAAAAQEswtSTV1NSourra/3rv3r3q3r17g32Sk5O1bds2+Xw+HThwQFFRUZQkAAAAAEFj6uV2lZWVev755yVJ9fX1Gj58uAYOHKhNmzZJksaNG6dBgwYpNzdXs2fPVnh4uGbNmmVmZAAAAAAWZ2pJSkhIUHp6eqP148aN87+22WyaNm1aKGMBAAAAaMNMvycJAAAAAFoTShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAA0oSAAAAABhQkgAAAADAgJIEAAAAAAaUJAAAAAAwoCQBAAAAgAElCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMCAkgQAAAAABpQkAAAAADCgJAEAAACAgcPMDy8pKVFWVpYqKipks9nk8Xh0ww03NNhn//79eu6559SpUydJ0tChQzVx4kQz4gIAAABoA0wtSXa7XZMnT1bPnj1VXV2t+fPna8CAAeratWuD/fr376/58+eblBIAAABAW2Lq5XZxcXHq2bOnJKldu3bq0qWLysrKzIwEAAAAoI0z9UySUXFxsb755hv17t270bYDBw5o7ty5iouL0+TJk9WtW7cm3yM7O1vZ2dmSpLS0NLlcrqBmDtTRM2xrLRlxdszR2s40X4kZWwHfYWtgjm2Hw+FgphZ0scy1VZSkmpoaZWRkaMqUKYqKimqwrUePHlqxYoUiIyOVm5ur9PR0LVu2rMn38Xg88ng8/uWSkpKg5m4JF0NGnB1ztD5mbG3M1xqYo7W4XC5makGtba6JiYlNrjf96XZ1dXXKyMjQiBEjNHTo0Ebbo6KiFBkZKUkaPHiw6uvrVVVVFeqYAAAAANoIU0uSz+fTypUr1aVLF910001N7lNRUSGfzydJKigokNfr1aWXXhrKmAAAAADaEFMvt/vqq6+0bds2de/eXXPnzpUk3XXXXf5TcOPGjdMnn3yiTZs2yW63Kzw8XI8++qhsNpuZsQEAAABYmKklqV+/fnrjjTfOuM/48eM1fvz4ECUCAAAA0NaZfk8SAAAAALQmlCQAAAAAMGgVjwAHAAAAYA31029ufuOGHaELcgE4kwQAAAAABpQkAAAAADCgJAEAAACAASUJAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAA4fZAQAAAICf1U+/WZJ0tIlt9pfeDm0YtFmcSQIAAAAAA0oSAAAAABhQkgAAAADAgJIEAAAAAAaUJAAAAAAwoCQBAAAAgAElCQAAAAAMKEkAAAAAYEBJAgAAAAADShIAAAAAGFCSAAAAAMDAYXaAPXv2aO3atfJ6vRozZowmTJjQYLvP59PatWuVl5eniIgIzZo1Sz179jQpLQAAAACrM/VMktfr1erVq7Vw4UJlZmYqJydH33//fYN98vLydOTIES1btkwzZszQH/7wB5PSAgAAAGgLTC1JBQUF6ty5sxISEuRwOJSSkqKdO3c22GfXrl0aOXKkbDab+vbtqx9//FHl5eUmJQYAAABgdaaWpLKyMjmdTv+y0+lUWVlZo31cLtcZ9wEAAACAlmLqPUk+n6/ROpvNds77/Cw7O1vZ2dmSpLS0NCUmJrZAyhbw3i6zE6AlMEdrY77Wx4ytgTlaHzO++J1lhq3m7+hnYOqZJKfTqdLSUv9yaWmp4uLiGu1TUlJyxn1+5vF4lJaWprS0tOAEPk/z5883OwKCgLlaF7O1JuZqXczWmpirNV0sczW1JPXq1UtFRUUqLi5WXV2dduzYoeTk5Ab7JCcna9u2bfL5fDpw4ICioqKaLUkAAAAAcKFMvdzObrdr6tSpWrJkibxer0aPHq1u3bpp06ZNkqRx48Zp0KBBys3N1ezZsxUeHq5Zs2aZGRkAAACAxZn+e5IGDx6swYMHN1g3btw4/2ubzaZp06aFOlaL8ng8Zgl8VroAAA7WSURBVEdAEDBX62K21sRcrYvZWhNztaaLZa42X1NPRgAAAACANsrUe5IAAAAAoLUx/XI7K6utrVVqaqrq6upUX1+vYcOGadKkSWbHQgvxer2aP3++4uPjL5onteDMHnzwQUVGRiosLEx2u73VPSkT5+/HH3/UypUr9d1338lms+mBBx5Q3759zY6FC1BYWKjMzEz/cnFxsSZNmqQbb7zRxFRoCe+++64++ugj2Ww2devWTbNmzVJ4eLjZsdAC3n//fW3evFk+n09jxoxp1d9XSlIQXXLJJUpNTVVkZKTq6ur09NNPa+DAgfyH2SLef/99denSRdXV1WZHQQtKTU1VTEyM2THQwtauXauBAwfq8ccfV11dnU6dOmV2JFygxMREpaenSzr9P63uv/9+/epXvzI5FS5UWVmZ/vznPyszM1Ph4eH63e9+px07dmjUqFFmR8MFOnz4sDZv3qxnnnlGDodDzzzzjAYPHqzLLrvM7GhN4nK7ILLZbIqMjJQk1dfXq76+vtlfhIuLS2lpqXJzczVmzBizowA4i5MnT+qLL77Q9ddfL0lyOBxq3769yanQkvbt26fOnTurY8eOZkdBC/B6vaqtrVV9fb1qa2v51S8W8cMPP6hPnz6KiIiQ3W5X//799dlnn5kdq1mcSQoyr9erefPm6ciRI/qXf/kX9enTx+xIaAF//OMfdc8993AWyYKWLFkiSRo7duxF8wQenFlxcbFiYmK0YsUKffvtt+rZs6emTJni/59YuPjl5OTo2muvNTsGWkB8fLx+/etf64EHHlB4eLiSkpKUlJRkdiy0gG7duum1117T8ePHFR4erry8PPXq1cvsWM3iTFKQhYWFKT09XStXrtQ//vEPHT582OxIuEC7d+9WbGysevbsaXYUtLBFixbp2Wef1cKFC/Xhhx8qPz/f7EhoAfX19frmm280btw4Pffcc4qIiNDGjRvNjoUWUldXp927d2vYsGFmR0ELOHHihHbu3KmsrCytWrVKNTU12rZtm9mx0AK6du2qW265RYsXL9Yzzzyjyy+/XGFhrbeKcCYpRNq3by+32609e/aoe/fuZsfBBfjqq6+0a9cu5eXlqba2VtXV1Vq2bJlmz55tdjRcoPj4eElSbGyshgwZooKCArndbpNT4UI5nU45nU7/mfxhw4ZRkiwkLy9PPXr0UIcOHcyOghawb98+derUyX9v6NChQ3XgwAGNHDnS5GRoCddff73/0udXX31VTqfT5ETNa731zQKqqqr0448/Sjr9pLt9+/apS5cuJqfChbr77ru1cuVKZWVl6dFHH9WVV15JQbKAmpoa/+WTNTU12rt3L/9DwyI6dOggp9OpwsJCSaf/Eta1a1eTU6GlcKmdtbhcLh08eFCnTp2Sz+fj704WU1lZKUkqKSnRZ5991qq/u5xJCqLy8nJlZWXJ6/XK5/Ppmmuu0dVXX212LABNqKys1PPPPy/p9OVZw4cP18CBA01OhZYydepULVu2THV1derUqZNmzZpldiS0gFOnTmnv3r2aMWOG2VHQQvr06aNhw4Zp3rx5stvtuuKKK7g/1EIyMjJ0/PhxORwO3XfffYqOjjY7UrNsPp/PZ3YIAAAAAGgtuNwOAAAAAAwoSQAAAABgQEkCAAAAAANKEgAAAAAYUJIAAAAAwICSBAA4Z//1X/+lt956y+wYQbdlyxb95je/OadjXn31Vb333ntBStS04uJiTZo0SfX19Wfcb9euXVq6dGmIUgHAxYuSBAAW9uCDD+rf/u3fNHnyZE2fPl1ZWVmqqam54PedMWOGJk6c2AIJmxboX/pb22dWVVVp69atGjt2bAsmaznJycn67rvv9O2335odBQBaNUoSAFjcvHnztH79eqWnp+vQoUPasGGD2ZEsa8uWLRo0aJDCw8OD9hkXWhyvvfZaZWdnt1AaALAmh9kBAACh0aFDByUlJenQoUP+dQcOHNC6dev0/fffq2PHjpoyZYp++ctfKicnR++8847S0tL8+7777rvav3+/5s2bp6ysLDmdTt15552SpN27d+u1117TsWPH1LVrV02fPl2XX365/vrXv+rTTz/V/PnzJUkPP/ywevTooTlz5kiSHnjgAc2bN09XXHFFwD/HyZMn9fLLLysvL082m02jR4/WpEmTFBYWpi1btmjz5s3q06eP/vrXvyoqKkrTpk3ToEGDJJ0+W5SVlaVvvvlGffr00WWXXaaTJ09q9uzZSk1NlSRNmTJFkhpcZrdu3bom3++f5eXlafTo0f7l1NRU/eu//quGDRumL7/8Uk8//bTmz5+vwYMHa+/evf7y6vV6tWHDBm3evFm1tbUaOHCgpk6dqqioKBUXF+uhhx7SzJkz9eabb6pTp05KTU3VK6+8oq1bt6pdu3a66aabGuTYsmWL3nrrLVVVVenSSy/VnXfeqREjRkiS3G63fv/73+u+++4L+M8cANoaziQBQBtRWlqqvLw8de7cWZJUVlamtLQ03XbbbVqzZo0mT56sjIwMVVVVKTk5WYWFhSoqKvIfn5OTo+HDhzd636+//lovvviiZsyYoTVr1sjj8ei5557TTz/9JLfbrS+//FJer1fl5eWqr6/XV199JUk6evSoampq1L1793P6OZYvXy673a5ly5bpueee09///ndt3rzZv72goECJiYlavXq1brnlFq1cuVI+n0+S9MILL6hXr15as2aNbr/9dm3fvt1/3H/8x39Ikv74xz9q/fr16tu371nf758dPnxYiYmJ/mW32638/HxJUn5+vhISEvzLX3zxhdxut6TTpWbLli1KTU3V8uXLVVNTo9WrVzd47/z8fGVmZur//b//p+zsbOXm5urZZ59VWlqaPv30U/9+NTU1Wrt2rRYuXKh169Zp8eLFDUpo165ddezYMZ08efKc/twBoC2hJAGAxaWnp+vee+/VAw88oNjYWE2aNEmStG3bNg0aNEiDBw9WWFiYBgwYoF69eik3N1cRERFKTk5WTk6OJKmoqEg//PCDkpOTG73/5s2b5fF41KdPH4WFhWnUqFFyOBw6ePCgEhIS1K5dOx06dEj5+flKSkpSfHy8fvjhB+Xn56tfv34KCwv8P0UVFRXas2ePpkyZosjISMXGxurGG2/Ujh07/Pu4XC55PB6FhYXpuuuuU3l5uSorK1VSUqJ//OMfuuOOO+RwONSvXz9dffXVZ/3M5t6vKSdPnlS7du38y8aS9MUXX2jChAn64osvJJ0uPT+XpL/97W+66aablJCQoMjISN19993asWNHg0vrbr/9dkVGRio8PFwff/yxbrjhBrlcLkVHR2vChAkNcthsNh0+fFi1tbWKi4tTt27d/NsiIyP9WQEATeNyOwCwuLlz52rAgAHKz8/XCy+8oOPHj6t9+/YqKSnRJ598ot27d/v3ra+v1y9/+UtJ0vDhw7V+/XpNnDhRf/vb3zRkyBBFREQ0ev+SkhJt3bpVH3zwgX9dXV2dysrKJEn9+/dXfn6+jhw5Irfbrfbt2ys/P18HDhzwl4RAlZSUqL6+XjNmzPCv8/l8cjqd/uUOHTr4X/+ct6amRlVVVYqOjm7wM7hcLpWUlJzxM5t7v6a0b99e1dXV/uW+ffuqqKhIFRUVOnTokObNm6c33nhDVVVVKigoUP/+/SVJ5eXl6tixY4Nc9fX1DcqY8WcsLy+Xy+XyLxuPjYyM1KOPPqp33nlHK1eu1C9+8Qvde++96tKlS4PsUVFRZ/y5AaAtoyQBQBvhdrs1atQorVu3Tk8++aScTqdGjBihmTNnNrl/UlKSsrKydOjQIeXk5Ojf//3fm9zP6XTqtttu02233dbs5+7evVvFxcW69dZb1b59e23fvl0HDhzQ+PHjz+lncDqdcjgcWr16tex2+zkdGxcXpxMnTujUqVP+smMsSDab7ZzerymXX365ioqK1Lt3b0mnS1XPnj31/vvvq3v37nI4HPrFL36hd999V507d1ZMTIw/27Fjx/zvU1JSIrvdrtjYWJWWljbKFxcX1yD7Pxe9gQMHauDAgaqtrdVrr72mVatW6T//8z8lyX//GSUJAJrH5XYA0IbceOON2rdvnw4dOqQRI0Zo9+7d2rNnj7xer2pra7V//37/X8rtdruGDRum9evX68SJExowYECT7zlmzBj95S9/0cGDB+Xz+VRTU6Pc3Fz/GRW32639+/ertrZWTqdT/fr10549e3TixAn16NHjjHl/+ukn1dbW+v+JjY1VUlKS1q1bp5MnT8rr9erIkSP+S9rOpGPHjurVq5fefPNN1dXV6cCBAw3OosXExMhms+no0aOB/nE2MmjQoEZZ+vfvrw8//NB/1sjtdjdYlk4/ce69995TcXGxampq9Kc//UnXXHNNs0Xwmmuu0Z///GeVlpbqxIkT2rhxo39bRUWFdu3apZqaGjkcDkVGRja4pDE/P7/ZB08AAE7jTBIAtCExMTEaOXKk3nrrLT3xxBN68skn9corr+iFF15QWFiYevfurenTp/v3Hz58uFJTUzVu3Lhm/8Leq1cv3X///VqzZo2KiooUHh6ufv36+UtAYmKiIiMj/ctRUVFKSEhQTEzMWe9HuvfeexssP/XUU3rooYf03//935ozZ46qq6uVkJCgW265JaCf/+GHH9aKFSs0depU9e7dWykpKfJ6vZJOn/W57bbb9Jvf/Eb19fVauHBhQO9pNHLkSD355JOqra31Pwbc7XZr48aN/ksL3W63qqurG1xqOHr0aJWXlys1NVW1tbVKSkrS1KlTm/2cMWPGqLCwUHPnzlW7du3061//Wp9//rmk05cfvvPOO/r9738vm82mK664QtOmTfMfm5OTo4cffvicfzYAaEtsvuYe0QMAgMVlZmaqS5cu/odZtIRXX33V/0CJ1mbXrl3atm2b/xHsAICmUZIAAG1GQUGBoqOj1alTJ+3du1fp6elavHjxWS/7AwC0LVxuBwBoMyoqKpSRkaHjx4/L6XRq2rRpFCQAQCOcSQIAAAAAA55uBwAAAAAGlCQAAAAAMKAkAQAAAIABJQkAAAAADChJAAAAAGBASQIAAAAAg/8PHiAKMpNbRlkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Getting the number of words by splitting them by a space\n",
    "words_per_review = spvl.Text.apply(lambda x: len(x.split(\" \")))\n",
    "words_per_review.hist(bins = 100)\n",
    "plt.xlabel('Review Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words: 4.87037037037037\n",
      "Skewness: 0.8809530055736419\n"
     ]
    }
   ],
   "source": [
    "print('Average words:', words_per_review.mean())\n",
    "print('Skewness:', words_per_review.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the distribution of ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    48.148148\n",
       "1    38.888889\n",
       "2    12.962963\n",
       "Name: TAG, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_val = 100 * spvl['TAG'].value_counts()/len(spvl)\n",
    "percent_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "percent_val.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "As discussed previously, text preprocessing and normalization is crucial before building a proper NLP model. Some of the important steps are:\n",
    "\n",
    "1. Converting words to lower/upper case \n",
    "2. Removing special characters\n",
    "3. Removing stopwords and high/low-frequency words\n",
    "4. Stemming/lemmatization\n",
    "### 1. Converting words to lower/upper case\n",
    "Let's start by converting all of the words into a consistent case format, say lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spvl['reviews_text_new'] = spvl['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell we analyze the number of unique words that are found in reviews before and after applying this step. This process of extracting individual words is called word tokenization, and can be run using the word_tokenize function in the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens then:  118\n",
      "Number of unique tokens now:  111\n"
     ]
    }
   ],
   "source": [
    "#For reviews not converted to lowe case\n",
    "token_lists = [word_tokenize(each) for each in spvl['Text']]\n",
    "tokens = [item for sublist in token_lists for item in sublist]\n",
    "print(\"Number of unique tokens then: \",len(set(tokens)))\n",
    "\n",
    "# For reviews converted to lowe case\n",
    "token_lists_lower = [word_tokenize(each) for each in spvl['reviews_text_new']]\n",
    "tokens_lower = [item for sublist in token_lists_lower for item in sublist]\n",
    "print(\"Number of unique tokens now: \",len(set(tokens_lower)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords and high/low frequency words\n",
    "As discussed before, stopwords naturally occur very frequently in the English language without adding any context specific insights. It makes sense to remove them. Let's first review what languages are available in the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available languages for NLTK v.3.4.5: \n",
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print('Available languages for NLTK v.3.4.5: ')\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now review the list of English stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_words = []\n",
    "eng_stop_words = stopwords.words('english')\n",
    "eng_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Original Sentence --\n",
      " what is supervised learning\n",
      "\n",
      "-- Stopwords in the sentence --\n",
      " ['what', 'is']\n",
      "\n",
      "-- Non-stopwords in the sentence --\n",
      " ['supervised', 'learning']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(eng_stop_words)\n",
    "without_stop_words = []\n",
    "stopword = []\n",
    "sentence = spvl['reviews_text_new'][0]\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in words:\n",
    "    if word in stop_words:\n",
    "        stopword.append(word)\n",
    "    else:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print('-- Original Sentence --\\n', sentence)\n",
    "print('\\n-- Stopwords in the sentence --\\n', stopword)\n",
    "print('\\n-- Non-stopwords in the sentence --\\n', without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we omit these stopwords from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_text_new</th>\n",
       "      <th>reviews_text_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is supervised learning</td>\n",
       "      <td>[supervised, learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is regression</td>\n",
       "      <td>[regression]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is slope</td>\n",
       "      <td>[slope]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is intercept</td>\n",
       "      <td>[intercept]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is semi supervised learning</td>\n",
       "      <td>[semi, supervised, learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what is data split</td>\n",
       "      <td>[data, split]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what do you mean by the training of data</td>\n",
       "      <td>[mean, training, data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what do you mean by the testing of data</td>\n",
       "      <td>[mean, testing, data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what is the validation of model</td>\n",
       "      <td>[validation, model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is feature selection</td>\n",
       "      <td>[feature, selection]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>what is feature scaling</td>\n",
       "      <td>[feature, scaling]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>what is model evalution</td>\n",
       "      <td>[model, evalution]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what is curse of dimensionality</td>\n",
       "      <td>[curse, dimensionality]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>how specific output links to supervised learning</td>\n",
       "      <td>[specific, output, links, supervised, learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what is confusion matrix</td>\n",
       "      <td>[confusion, matrix]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what is classification</td>\n",
       "      <td>[classification]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>what is linear regression</td>\n",
       "      <td>[linear, regression]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>what is logistic regression</td>\n",
       "      <td>[logistic, regression]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what is nave bayes algorithm</td>\n",
       "      <td>[nave, bayes, algorithm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>what is euclidean distance</td>\n",
       "      <td>[euclidean, distance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what is manhattan distance</td>\n",
       "      <td>[manhattan, distance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>what is knn algorithm</td>\n",
       "      <td>[knn, algorithm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>what is mean squared error or mse</td>\n",
       "      <td>[mean, squared, error, mse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>what is accuracy</td>\n",
       "      <td>[accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>what is roc or auc curve</td>\n",
       "      <td>[roc, auc, curve]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>what is ols methods</td>\n",
       "      <td>[ols, methods]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>what is gradient descent</td>\n",
       "      <td>[gradient, descent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>what is cost function</td>\n",
       "      <td>[cost, function]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>what is f1 score</td>\n",
       "      <td>[f1, score]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>what is precision</td>\n",
       "      <td>[precision]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>what is recall</td>\n",
       "      <td>[recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>what is true positive rate or tpr</td>\n",
       "      <td>[true, positive, rate, tpr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>what is true negative rate or tnr</td>\n",
       "      <td>[true, negative, rate, tnr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>what is dependent or target variable</td>\n",
       "      <td>[dependent, target, variable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>what is independent or predictor variable</td>\n",
       "      <td>[independent, predictor, variable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>what is train test split</td>\n",
       "      <td>[train, test, split]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>how multicolinearity affect the model performance</td>\n",
       "      <td>[multicolinearity, affect, model, performance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>what is pearson's correalation coefficient</td>\n",
       "      <td>[pearson, 's, correalation, coefficient]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>what is assumption of linear regression</td>\n",
       "      <td>[assumption, linear, regression]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>what is sum of squared error or sse</td>\n",
       "      <td>[sum, squared, error, sse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>what is regression  error or ssr</td>\n",
       "      <td>[regression, error, ssr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>what is toatal error or sst</td>\n",
       "      <td>[toatal, error, sst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>what is coeeficient of determinant</td>\n",
       "      <td>[coeeficient, determinant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>what is adjusted r-square</td>\n",
       "      <td>[adjusted, r-square]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>what is decision boundary</td>\n",
       "      <td>[decision, boundary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>what is log loss</td>\n",
       "      <td>[log, loss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>what is predict proba</td>\n",
       "      <td>[predict, proba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>what is imbalance class problem</td>\n",
       "      <td>[imbalance, class, problem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>what  is polynomial regression</td>\n",
       "      <td>[polynomial, regression]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>what is bias variance trade off</td>\n",
       "      <td>[bias, variance, trade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>what is standadization</td>\n",
       "      <td>[standadization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>what is normalization</td>\n",
       "      <td>[normalization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>what is posterior probability</td>\n",
       "      <td>[posterior, probability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>what is predictiving modelling</td>\n",
       "      <td>[predictiving, modelling]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     reviews_text_new  \\\n",
       "0                         what is supervised learning   \n",
       "1                                  what is regression   \n",
       "2                                       what is slope   \n",
       "3                                   what is intercept   \n",
       "4                   what is semi supervised learning    \n",
       "5                                 what is data split    \n",
       "6            what do you mean by the training of data   \n",
       "7             what do you mean by the testing of data   \n",
       "8                     what is the validation of model   \n",
       "9                           what is feature selection   \n",
       "10                            what is feature scaling   \n",
       "11                            what is model evalution   \n",
       "12                    what is curse of dimensionality   \n",
       "13   how specific output links to supervised learning   \n",
       "14                           what is confusion matrix   \n",
       "15                             what is classification   \n",
       "16                          what is linear regression   \n",
       "17                        what is logistic regression   \n",
       "18                      what is nave bayes algorithm   \n",
       "19                         what is euclidean distance   \n",
       "20                         what is manhattan distance   \n",
       "21                              what is knn algorithm   \n",
       "22                  what is mean squared error or mse   \n",
       "23                                   what is accuracy   \n",
       "24                           what is roc or auc curve   \n",
       "25                                what is ols methods   \n",
       "26                           what is gradient descent   \n",
       "27                              what is cost function   \n",
       "28                                   what is f1 score   \n",
       "29                                  what is precision   \n",
       "30                                     what is recall   \n",
       "31                  what is true positive rate or tpr   \n",
       "32                  what is true negative rate or tnr   \n",
       "33              what is dependent or target variable    \n",
       "34          what is independent or predictor variable   \n",
       "35                           what is train test split   \n",
       "36  how multicolinearity affect the model performance   \n",
       "37         what is pearson's correalation coefficient   \n",
       "38            what is assumption of linear regression   \n",
       "39                what is sum of squared error or sse   \n",
       "40                   what is regression  error or ssr   \n",
       "41                        what is toatal error or sst   \n",
       "42                 what is coeeficient of determinant   \n",
       "43                          what is adjusted r-square   \n",
       "44                          what is decision boundary   \n",
       "45                                   what is log loss   \n",
       "46                              what is predict proba   \n",
       "47                    what is imbalance class problem   \n",
       "48                     what  is polynomial regression   \n",
       "49                    what is bias variance trade off   \n",
       "50                             what is standadization   \n",
       "51                              what is normalization   \n",
       "52                      what is posterior probability   \n",
       "53                     what is predictiving modelling   \n",
       "\n",
       "                               reviews_text_nonstop  \n",
       "0                            [supervised, learning]  \n",
       "1                                      [regression]  \n",
       "2                                           [slope]  \n",
       "3                                       [intercept]  \n",
       "4                      [semi, supervised, learning]  \n",
       "5                                     [data, split]  \n",
       "6                            [mean, training, data]  \n",
       "7                             [mean, testing, data]  \n",
       "8                               [validation, model]  \n",
       "9                              [feature, selection]  \n",
       "10                               [feature, scaling]  \n",
       "11                               [model, evalution]  \n",
       "12                          [curse, dimensionality]  \n",
       "13  [specific, output, links, supervised, learning]  \n",
       "14                              [confusion, matrix]  \n",
       "15                                 [classification]  \n",
       "16                             [linear, regression]  \n",
       "17                           [logistic, regression]  \n",
       "18                        [nave, bayes, algorithm]  \n",
       "19                            [euclidean, distance]  \n",
       "20                            [manhattan, distance]  \n",
       "21                                 [knn, algorithm]  \n",
       "22                      [mean, squared, error, mse]  \n",
       "23                                       [accuracy]  \n",
       "24                                [roc, auc, curve]  \n",
       "25                                   [ols, methods]  \n",
       "26                              [gradient, descent]  \n",
       "27                                 [cost, function]  \n",
       "28                                      [f1, score]  \n",
       "29                                      [precision]  \n",
       "30                                         [recall]  \n",
       "31                      [true, positive, rate, tpr]  \n",
       "32                      [true, negative, rate, tnr]  \n",
       "33                    [dependent, target, variable]  \n",
       "34               [independent, predictor, variable]  \n",
       "35                             [train, test, split]  \n",
       "36   [multicolinearity, affect, model, performance]  \n",
       "37         [pearson, 's, correalation, coefficient]  \n",
       "38                 [assumption, linear, regression]  \n",
       "39                       [sum, squared, error, sse]  \n",
       "40                         [regression, error, ssr]  \n",
       "41                             [toatal, error, sst]  \n",
       "42                       [coeeficient, determinant]  \n",
       "43                             [adjusted, r-square]  \n",
       "44                             [decision, boundary]  \n",
       "45                                      [log, loss]  \n",
       "46                                 [predict, proba]  \n",
       "47                      [imbalance, class, problem]  \n",
       "48                         [polynomial, regression]  \n",
       "49                          [bias, variance, trade]  \n",
       "50                                 [standadization]  \n",
       "51                                  [normalization]  \n",
       "52                         [posterior, probability]  \n",
       "53                        [predictiving, modelling]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stopwords_removal(stop_words, sentence):\n",
    "    return [word for word in nltk.word_tokenize(sentence) if word not in stop_words]\n",
    "\n",
    "spvl['reviews_text_nonstop'] = spvl['reviews_text_new'].apply(lambda row: stopwords_removal(stop_words, row))\n",
    "spvl[['reviews_text_new','reviews_text_nonstop']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming works by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found.\n",
    "\n",
    "Lemmatization takes into consideration the morphological analysis of the words. So lemmatization considers the grammar of the word and tries to find the root word instead of just getting to the root word by brute force methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer # Common stemmers\n",
    "from nltk.stem import WordNetLemmatizer # Common Lematizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "WordNet Lemmatizer\n",
      "trouble\n",
      "trouble\n",
      "trouble\n"
     ]
    }
   ],
   "source": [
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))\n",
    "\n",
    "# Provide a word to be lemmatized\n",
    "print(\"WordNet Lemmatizer\")\n",
    "print(lemmatizer.lemmatize(\"trouble\", wordnet.NOUN))\n",
    "print(lemmatizer.lemmatize(\"troubling\", wordnet.VERB))\n",
    "print(lemmatizer.lemmatize(\"troubled\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It can be seen that we get a meaning root word from Lemmatizer while Stemmer just cuts out and extracts the first important part of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "The bag-of-words procedure falls under a broader category of techniques known as count-based representations. These are techniques to analyze documents by indicating how frequently certain types of structures occur throughout.\n",
    "\n",
    "Let's start with 1-grams (words). The simplest type of information would be whether a particular word occurs in particular documents. This leads to word-document co-occurrence matrices, where the $(W, X)$ entry of the word-document matrix is set to 1 if word $W$ occurs in document $X$, and 0 otherwise.\n",
    "\n",
    "Let's create a word-document co-occurrence matrix for our set of reviews using the CountVectorizer class that automatically accounts for certain preprocessing steps like removing stopwords, stemming, creating n-grams, and word tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>adjusted</th>\n",
       "      <th>affect</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>assumption</th>\n",
       "      <th>auc</th>\n",
       "      <th>bayes</th>\n",
       "      <th>bias</th>\n",
       "      <th>boundary</th>\n",
       "      <th>by</th>\n",
       "      <th>...</th>\n",
       "      <th>tpr</th>\n",
       "      <th>trade</th>\n",
       "      <th>train</th>\n",
       "      <th>training</th>\n",
       "      <th>true</th>\n",
       "      <th>validation</th>\n",
       "      <th>variable</th>\n",
       "      <th>variance</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  adjusted  affect  algorithm  assumption  auc  bayes  bias  \\\n",
       "0         0         0       0          0           0    0      0     0   \n",
       "1         0         0       0          0           0    0      0     0   \n",
       "2         0         0       0          0           0    0      0     0   \n",
       "3         0         0       0          0           0    0      0     0   \n",
       "4         0         0       0          0           0    0      0     0   \n",
       "\n",
       "   boundary  by  ...  tpr  trade  train  training  true  validation  variable  \\\n",
       "0         0   0  ...    0      0      0         0     0           0         0   \n",
       "1         0   0  ...    0      0      0         0     0           0         0   \n",
       "2         0   0  ...    0      0      0         0     0           0         0   \n",
       "3         0   0  ...    0      0      0         0     0           0         0   \n",
       "4         0   0  ...    0      0      0         0     0           0         0   \n",
       "\n",
       "   variance  what  you  \n",
       "0         0     1    0  \n",
       "1         0     1    0  \n",
       "2         0     1    0  \n",
       "3         0     1    0  \n",
       "4         0     1    0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code creates a word-document matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(spvl['reviews_text_new'])\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to create a bag of words from the reviews, excluding the noise words we identified earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a python object of the class CountVectorizer\n",
    "\n",
    "bow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
    "                             stop_words=noise_words, # List of stopwords\n",
    "                             ngram_range=(1,1)) # number of n-grams\n",
    "\n",
    "bow_data = bow_counts.fit_transform(spvl['reviews_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<54x111 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a sparse matrix of 259 observations (number of rows of the reviews_text_new series) has been built, and 17679 columns corresponding to the features extracted by means of the representation of the 1-gram count of the user reviews.\n",
    "\n",
    "Once the bag of words is prepared, the dataset should be divided into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n",
    "                                                                    spvl['TAG'], # Target variable\n",
    "                                                                    test_size = 0.2, # 20% test size\n",
    "                                                                    random_state = 0) # random state for replication purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.454545\n",
       "0    0.454545\n",
       "2    0.090909\n",
       "Name: TAG, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_bow.value_counts()/y_test_bow.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train_bow)\n",
    "y_pred_class = nb.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454545454545453\n",
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test_bow, y_pred_class))\n",
    "print(nb.score(X_test_bow,y_test_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF model\n",
    "Of course, bag-of-words are not the only way to featurize text. Another method is the Term Frequency-Inverse Document Frequency (TF-IDF) method. This evaluates how important a word is to a document within a large collection of documents (i.e. corpus). The importance increases proportionally based on the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "The TF-IDF weight is the product of two terms. The first computes the normalized Term Frequency (TF); i.e. the number of times a word appears in a document divided by the total number of words in that document. The second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears:\n",
    "\n",
    "\n",
    "\n",
    "Let's re-featurize our original set of reviews based on TF-IDF and split the resulting features into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### Creating a python object of the class CountVectorizer\n",
    "tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
    "                               stop_words=noise_words, # List of stopwords\n",
    "                               ngram_range=(1,1)) # number of n-grams\n",
    "\n",
    "tfidf_data = tfidf_counts.fit_transform(spvl['reviews_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<54x111 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,\n",
    "                                                                            spvl['TAG'],\n",
    "                                                                            test_size = 0.2,\n",
    "                                                                            random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5454545454545454\n",
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "### Setting up the model class\n",
    "nb_tf_idf = MultinomialNB()\n",
    "\n",
    "## Training the model \n",
    "nb_tf_idf.fit(X_train_tfidf,y_train_tfidf)\n",
    "\n",
    "## Prediciting the results\n",
    "test_pred_nb_all = nb_tf_idf.predict(X_test_tfidf)\n",
    "\n",
    "## Evaluating the model\n",
    "#print(\"F1 score: \",f1_score(y_test_bow, test_pred_lr_all))\n",
    "\n",
    "# calculate accuracy\n",
    "print (metrics.accuracy_score(y_test_tfidf, test_pred_nb_all))\n",
    "print(nb.score(X_test_tfidf,y_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
