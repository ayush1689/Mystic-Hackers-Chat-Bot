{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e08383e100d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def cleaning(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    #s = str.split(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\",\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    s = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.MULTILINE)\n",
    "    s = re.sub(r'\\<a href', ' ', s)\n",
    "    s = re.sub(r'&amp;', '', s) \n",
    "    s = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', s)\n",
    "    s = re.sub(r'[^\\x00-\\x7f]',r'',s) #removes arabic\n",
    "    s = re.sub(r'<br />', ' ', s)\n",
    "    s = re.sub(r'\\'', ' ', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "train['Text'] = [cleaning(s) for s in train['Text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>AASKK211</td>\n",
       "      <td>10</td>\n",
       "      <td>understand boxplot outlier</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK237</td>\n",
       "      <td>16</td>\n",
       "      <td>calculate mean median mode function python</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>AASKK34</td>\n",
       "      <td>0</td>\n",
       "      <td>key dictionary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>AASKK78</td>\n",
       "      <td>3</td>\n",
       "      <td>different arithmatic operator available python</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>AASKK319</td>\n",
       "      <td>8</td>\n",
       "      <td>utilize panda dataframe</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>AASKK06</td>\n",
       "      <td>11</td>\n",
       "      <td>python general purpose</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>AASKK17</td>\n",
       "      <td>18</td>\n",
       "      <td>tuples</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>AASKK180</td>\n",
       "      <td>2</td>\n",
       "      <td>aggregation panda groupby</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>AASKK393</td>\n",
       "      <td>5</td>\n",
       "      <td>purpose operator</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>AASKK347</td>\n",
       "      <td>2</td>\n",
       "      <td>select list column groupby panda</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic_Code Category                                              Text  \\\n",
       "index                                                                         \n",
       "155     AASKK211       10                       understand boxplot outlier    \n",
       "3       AASKK237       16       calculate mean median mode function python    \n",
       "178      AASKK34        0                                   key dictionary    \n",
       "29       AASKK78        3   different arithmatic operator available python    \n",
       "98      AASKK319        8                          utilize panda dataframe    \n",
       "...          ...      ...                                               ...   \n",
       "141      AASKK06       11                           python general purpose    \n",
       "69       AASKK17       18                                           tuples    \n",
       "38      AASKK180        2                        aggregation panda groupby    \n",
       "76      AASKK393        5                                 purpose operator    \n",
       "78      AASKK347        2                 select list column groupby panda    \n",
       "\n",
       "       Code  \n",
       "index        \n",
       "155      10  \n",
       "3        16  \n",
       "178       0  \n",
       "29        3  \n",
       "98        8  \n",
       "...     ...  \n",
       "141      11  \n",
       "69       18  \n",
       "38        2  \n",
       "76        5  \n",
       "78        2  \n",
       "\n",
       "[131 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review,stopWords):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nonan = re.compile(r'[^a-zA-Z ]')\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = nonan.sub('', sentence_token[j])\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in single_sentence]\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString, idx_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(dataframe, column_name, training_split = 0.6, validation_split = 0.2, test_split = 0.2):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into trainingset, validationset and testset in specified ratio.\n",
    "    All sets are balanced, which means they have the same ratio for each categorie as the full set.\n",
    "    Input:   dataframe        - Pandas Dataframe, should include a column for data and one for categories\n",
    "             column_name      - Name of dataframe column which contains the categorical output values\n",
    "             training_split   - from ]0,1[, default = 0.6\n",
    "             validation_split - from ]0,1[, default = 0.2        \n",
    "             test_split       - from ]0,1[, default = 0.2\n",
    "                                Sum of all splits need to be 1\n",
    "    Output:  train            - Pandas DataFrame of trainset\n",
    "             validation       - Pandas DataFrame of validationset\n",
    "             test             - Pandas DataFrame of testset\n",
    "    \"\"\"\n",
    "    if training_split + validation_split + test_split != 1.0:\n",
    "        raise ValueError('Split paramter sum should be 1.0')\n",
    "        \n",
    "    total = len(dataframe.index)\n",
    " \n",
    "    train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
    "    .reset_index(drop=True).set_index('index')\n",
    "    train = train.sample(frac=1)\n",
    "    temp_df = dataframe.drop(train.index)\n",
    "    validation = temp_df.reset_index().groupby(column_name)\\\n",
    "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
    "           .reset_index(drop=True).set_index('index')\n",
    "    validation = validation.sample(frac=1)\n",
    "    test = temp_df.drop(validation.index)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    print('Total: ', len(dataframe))\n",
    "    print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
    "    print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
    "    print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_pipe(vectorizer_tfidf,num_comp=0, reducer='svd'):  \n",
    " \n",
    "    # Vectorizer\n",
    "    vec_pipe = [\n",
    "       #('col_extr', JsonFields(0, ['title', 'body', 'url'])),\n",
    "       #('squash', Squash()),\n",
    "       ('vec', vectorizer_tfidf)\n",
    "    ]\n",
    "\n",
    "    # Reduce dimensions of tfidf\n",
    "    if num_comp > 0:\n",
    "        if reducer == 'svd':\n",
    "            vec_pipe.append(('dim_red', TruncatedSVD(num_comp)))\n",
    "\n",
    "        vec_pipe.append(('norm', Normalizer()))\n",
    "\n",
    "    return Pipeline(vec_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))\n",
    "    \n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def __init__(self, stemmer):\n",
    "        super(StemmedCountVectorizer, self).__init__()\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(model,train,validation,num_comp,class1):\n",
    "    from sklearn.decomposition import NMF\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    lsa = TruncatedSVD(n_components=200)\n",
    "    X_test1 = char_vectorizer.transform(validation)\n",
    "    X_test2 = word_vectorizer.transform(validation)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    #train_text = vect.fit_transform(train[\"Text\"])\n",
    "    #print ('Features: ', train_text.shape[1])\n",
    "    #test_text = vect.transform(validation[\"Text\"])\n",
    "   \n",
    "    tsvd=TruncatedSVD(num_comp)\n",
    "    train_features1 = tsvd.fit_transform(train_features)\n",
    "    test_stack = tsvd.transform(test_stack)\n",
    "\n",
    "    train_features1 = Normalizer(copy=False).fit_transform(train_features1)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    model.fit(train_features1, y_train)\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    \n",
    "    print(\"Training Accuracy\")\n",
    "    print(model.score(train_features1,y_train))\n",
    "    print(\"Testing Accuracy\")\n",
    "    print(model.score(test_stack,y_test))\n",
    "    print(classification_report(y_pred_class,y_test))\n",
    "    cm = ConfusionMatrix(model, classes=class1)\n",
    "\n",
    "\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "    cm.score(test_stack, validation[\"Code\"])\n",
    "\n",
    "#How did we do?\n",
    "    cm.poof()\n",
    "    #print(confusion_matrix(y_pred_class,validation[\"Code\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################ACTUAL CODE STARTS FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=pd.read_excel('Supervised Learning-Chatbot.xlsx',encoding =  \"ISO-8859-1\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "stats.columns=[\"a\",\"Question\",\"Sub_Topic_Code\"]\n",
    "labelEncoder = LabelEncoder()\n",
    "stats[\"Sub_Topic_Code\"] = labelEncoder.fit_transform(stats[\"Sub_Topic_Code\"])\n",
    "stats = stats.rename(columns={'Sub_Topic_Code': 'Category', 'Question': 'Text'})\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for i in range(n):\n",
    "        temp_string,idx_string = cleanString(string1,stopWords)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK11</td>\n",
       "      <td>What is Feature scaling ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK27</td>\n",
       "      <td>What is Gradient descent ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK22</td>\n",
       "      <td>What is KNN algorithm ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK19</td>\n",
       "      <td>What is Naïve bayes algorithm ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK29</td>\n",
       "      <td>What is F1 score ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>AASKK28</td>\n",
       "      <td>What is Cost Function ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>AASKK18</td>\n",
       "      <td>What is Logistic Regression ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>AASKK51</td>\n",
       "      <td>What is Standadization ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>AASKK10</td>\n",
       "      <td>What is Feature Selection ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>AASKK42</td>\n",
       "      <td>What is Toatal error or SST ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>AASKK1</td>\n",
       "      <td>What is Supervised learning?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>AASKK16</td>\n",
       "      <td>What is Classification ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>AASKK15</td>\n",
       "      <td>What is confusion matrix ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>AASKK24</td>\n",
       "      <td>What is Accuracy ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>AASKK33</td>\n",
       "      <td>What is True Negative Rate or TNR?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>AASKK39</td>\n",
       "      <td>What is assumption of Linear Regression ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>AASKK48</td>\n",
       "      <td>What is imbalance class problem ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>AASKK17</td>\n",
       "      <td>What is Linear Regression ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>AASKK37</td>\n",
       "      <td>How multicolinearity affect the model performa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>AASKK13</td>\n",
       "      <td>What is Curse of Dimensionality ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>AASKK45</td>\n",
       "      <td>What is decision Boundary ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>AASKK14</td>\n",
       "      <td>How specific output links to Supervised learning?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>AASKK36</td>\n",
       "      <td>What is train test split ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>AASKK41</td>\n",
       "      <td>What is Regression  error or SSR?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>AASKK34</td>\n",
       "      <td>What is dependent or Target Variable ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>AASKK44</td>\n",
       "      <td>What is Adjusted R-square ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>AASKK32</td>\n",
       "      <td>What is True Positive Rate or TPR?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>AASKK26</td>\n",
       "      <td>What is OLS methods ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>AASKK31</td>\n",
       "      <td>What is Recall ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>AASKK20</td>\n",
       "      <td>What is euclidean distance ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>AASKK40</td>\n",
       "      <td>What is Sum Of Squared Error or SSE ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>AASKK3</td>\n",
       "      <td>What is slope ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>AASKK35</td>\n",
       "      <td>What is independent or Predictor Variable ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>AASKK4</td>\n",
       "      <td>What is intercept ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>AASKK49</td>\n",
       "      <td>What  is Polynomial regression ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>AASKK30</td>\n",
       "      <td>What is Precision ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>AASKK52</td>\n",
       "      <td>What is Normalization ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>AASKK5</td>\n",
       "      <td>What is semi supervised learning ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>AASKK9</td>\n",
       "      <td>What is the Validation of model ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>AASKK23</td>\n",
       "      <td>What is Mean Squared Error or MSE ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>AASKK38</td>\n",
       "      <td>What is Pearson's correalation coefficient ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>AASKK43</td>\n",
       "      <td>What is Coeeficient Of determinant ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>AASKK8</td>\n",
       "      <td>What do you mean by the testing of data ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>AASKK2</td>\n",
       "      <td>What is regression ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>AASKK7</td>\n",
       "      <td>What do you mean by the training of data ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>AASKK54</td>\n",
       "      <td>What is Predictiving modelling ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>AASKK6</td>\n",
       "      <td>What is Data split ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>AASKK53</td>\n",
       "      <td>What is Posterior probability ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>AASKK21</td>\n",
       "      <td>What is Manhattan distance ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>AASKK47</td>\n",
       "      <td>What is predict proba ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>AASKK50</td>\n",
       "      <td>What is Bias Variance trade off ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>AASKK25</td>\n",
       "      <td>What is ROC or AUC curve ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>AASKK46</td>\n",
       "      <td>What is Log loss ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>AASKK12</td>\n",
       "      <td>What is Model evalution ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a                                               Text  Category\n",
       "0   AASKK11                          What is Feature scaling ?         0\n",
       "1   AASKK27                         What is Gradient descent ?         1\n",
       "2   AASKK22                            What is KNN algorithm ?         1\n",
       "3   AASKK19                    What is Naïve bayes algorithm ?         0\n",
       "4   AASKK29                                 What is F1 score ?         1\n",
       "5   AASKK28                            What is Cost Function ?         0\n",
       "6   AASKK18                      What is Logistic Regression ?         0\n",
       "7   AASKK51                           What is Standadization ?         2\n",
       "8   AASKK10                        What is Feature Selection ?         0\n",
       "9   AASKK42                      What is Toatal error or SST ?         1\n",
       "10   AASKK1                       What is Supervised learning?         0\n",
       "11  AASKK16                           What is Classification ?         0\n",
       "12  AASKK15                         What is confusion matrix ?         0\n",
       "13  AASKK24                                 What is Accuracy ?         1\n",
       "14  AASKK33                 What is True Negative Rate or TNR?         1\n",
       "15  AASKK39          What is assumption of Linear Regression ?         1\n",
       "16  AASKK48                  What is imbalance class problem ?         1\n",
       "17  AASKK17                        What is Linear Regression ?         0\n",
       "18  AASKK37  How multicolinearity affect the model performa...         0\n",
       "19  AASKK13                  What is Curse of Dimensionality ?         0\n",
       "20  AASKK45                        What is decision Boundary ?         2\n",
       "21  AASKK14  How specific output links to Supervised learning?         0\n",
       "22  AASKK36                         What is train test split ?         0\n",
       "23  AASKK41                  What is Regression  error or SSR?         1\n",
       "24  AASKK34             What is dependent or Target Variable ?         0\n",
       "25  AASKK44                        What is Adjusted R-square ?         1\n",
       "26  AASKK32                 What is True Positive Rate or TPR?         1\n",
       "27  AASKK26                              What is OLS methods ?         1\n",
       "28  AASKK31                                   What is Recall ?         1\n",
       "29  AASKK20                       What is euclidean distance ?         1\n",
       "30  AASKK40              What is Sum Of Squared Error or SSE ?         1\n",
       "31   AASKK3                                    What is slope ?         0\n",
       "32  AASKK35        What is independent or Predictor Variable ?         0\n",
       "33   AASKK4                                What is intercept ?         0\n",
       "34  AASKK49                   What  is Polynomial regression ?         1\n",
       "35  AASKK30                                What is Precision ?         1\n",
       "36  AASKK52                            What is Normalization ?         2\n",
       "37   AASKK5                 What is semi supervised learning ?         0\n",
       "38   AASKK9                  What is the Validation of model ?         0\n",
       "39  AASKK23                What is Mean Squared Error or MSE ?         1\n",
       "40  AASKK38       What is Pearson's correalation coefficient ?         0\n",
       "41  AASKK43               What is Coeeficient Of determinant ?         1\n",
       "42   AASKK8          What do you mean by the testing of data ?         0\n",
       "43   AASKK2                               What is regression ?         0\n",
       "44   AASKK7         What do you mean by the training of data ?         0\n",
       "45  AASKK54                   What is Predictiving modelling ?         0\n",
       "46   AASKK6                               What is Data split ?         0\n",
       "47  AASKK53                    What is Posterior probability ?         2\n",
       "48  AASKK21                       What is Manhattan distance ?         1\n",
       "49  AASKK47                            What is predict proba ?         2\n",
       "50  AASKK50                  What is Bias Variance trade off ?         2\n",
       "51  AASKK25                         What is ROC or AUC curve ?         1\n",
       "52  AASKK46                                 What is Log loss ?         2\n",
       "53  AASKK12                          What is Model evalution ?         0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 54 articles cleaned.\r",
      "2 of 54 articles cleaned.\r",
      "3 of 54 articles cleaned.\r",
      "4 of 54 articles cleaned.\r",
      "5 of 54 articles cleaned.\r",
      "6 of 54 articles cleaned.\r",
      "7 of 54 articles cleaned.\r",
      "8 of 54 articles cleaned.\r",
      "9 of 54 articles cleaned.\r",
      "10 of 54 articles cleaned.\r",
      "11 of 54 articles cleaned.\r",
      "12 of 54 articles cleaned.\r",
      "13 of 54 articles cleaned.\r",
      "14 of 54 articles cleaned.\r",
      "15 of 54 articles cleaned.\r",
      "16 of 54 articles cleaned.\r",
      "17 of 54 articles cleaned.\r",
      "18 of 54 articles cleaned.\r",
      "19 of 54 articles cleaned.\r",
      "20 of 54 articles cleaned.\r",
      "21 of 54 articles cleaned.\r",
      "22 of 54 articles cleaned.\r",
      "23 of 54 articles cleaned.\r",
      "24 of 54 articles cleaned.\r",
      "25 of 54 articles cleaned.\r",
      "26 of 54 articles cleaned.\r",
      "27 of 54 articles cleaned.\r",
      "28 of 54 articles cleaned.\r",
      "29 of 54 articles cleaned.\r",
      "30 of 54 articles cleaned.\r",
      "31 of 54 articles cleaned.\r",
      "32 of 54 articles cleaned.\r",
      "33 of 54 articles cleaned.\r",
      "34 of 54 articles cleaned.\r",
      "35 of 54 articles cleaned.\r",
      "36 of 54 articles cleaned.\r",
      "37 of 54 articles cleaned.\r",
      "38 of 54 articles cleaned.\r",
      "39 of 54 articles cleaned.\r",
      "40 of 54 articles cleaned.\r",
      "41 of 54 articles cleaned.\r",
      "42 of 54 articles cleaned.\r",
      "43 of 54 articles cleaned.\r",
      "44 of 54 articles cleaned.\r",
      "45 of 54 articles cleaned.\r",
      "46 of 54 articles cleaned.\r",
      "47 of 54 articles cleaned.\r",
      "48 of 54 articles cleaned.\r",
      "49 of 54 articles cleaned.\r",
      "50 of 54 articles cleaned.\r",
      "51 of 54 articles cleaned.\r",
      "52 of 54 articles cleaned.\r",
      "53 of 54 articles cleaned.\r",
      "54 of 54 articles cleaned.\r"
     ]
    }
   ],
   "source": [
    "def clean_data(data_df):\n",
    "    articles = []\n",
    "    n = data_df['Text'].shape[0]\n",
    "    col_number = data_df.columns.get_loc('Text')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    data_cleaned = data_df.copy()\n",
    "    for i in range(n):\n",
    "        temp_string,idx_string = cleanString(data_df.iloc[i,col_number],stopWords)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    data_cleaned.loc[:,'Text'] = pd.Series(articles,index=data_df.index)\n",
    "    data_cleaned.loc[:,'Category'] = pd.Categorical(data_cleaned.Category)\n",
    "    data_cleaned['Code'] = data_cleaned.Category.cat.codes\n",
    "    categoryToCode = dict( enumerate(data_cleaned['Category'].cat.categories))\n",
    "    return(data_cleaned)\n",
    "data_cleaned=clean_data(data_df)\n",
    "from sklearn.model_selection import train_test_split\n",
    "y=data_cleaned[\"Code\"]\n",
    "X=data_cleaned[\"Text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=23, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for i in range(n):\n",
    "        temp_string,idx_string = cleanString(string1,stopWords)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  163\n",
      "Training:  130 , Percentage:  0.7975460122699386\n",
      "Validation:  33 , Percentage:  0.20245398773006135\n",
      "Test: 0 , Percentage:  0.0\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = split_df(data_cleaned, 'Code',0.8,0.20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name:1527\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline \n",
    "def build_tokenizer(doc):\n",
    "    token_pattern=r\"(?u)\\b\\w+\"\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    return token_pattern.findall(doc)\n",
    "posts_root1=[]\n",
    "for post in X_train:\n",
    "    #print build_tokenizer(post)\n",
    "    #print \" \".join([english_stemmer.stem(word) for word in build_tokenizer(post)])\n",
    "    a=\" \".join([english_stemmer.stem(word) for word in build_tokenizer(post)])\n",
    "    posts_root1.append(a)\n",
    "\n",
    "word_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words = 'english',\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1,3),\n",
    "    dtype=np.float32,\n",
    "    max_features=8000,\n",
    "    \n",
    ")\n",
    "# Character Stemmer\n",
    "char_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    dtype=np.float32,\n",
    "    max_features=8000,\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "word_vectorizer.fit(posts_root1)\n",
    "char_vectorizer.fit(posts_root1)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(posts_root1)\n",
    "train_char_features = char_vectorizer.transform(posts_root1)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "train_features = hstack([\n",
    "    train_char_features,\n",
    "    train_word_features])\n",
    "\n",
    "import pickle\n",
    "filename = 'char_preprocessing.sav'\n",
    "pickle.dump(word_vectorizer, open(filename, 'wb'))\n",
    "filename = 'word_preprocessing.sav'\n",
    "pickle.dump(char_vectorizer, open(filename, 'wb'))\n",
    "print(\"feature_name:%s\" % len(char_vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test2(model,train,text,class1):\n",
    "    from sklearn.decomposition import NMF\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    print(text)\n",
    "    X_test1 = char_vectorizer.transform(text)\n",
    "    X_test2 = word_vectorizer.transform(text)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    #\\train_text = lsa.fit_transform(X_train_dtm)\n",
    "    #test_text = lsa.fit_transform(X_test_dtm)\n",
    "    train_features1 = Normalizer(copy=False).fit_transform(train_features)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    model.fit(train_features.todense(), train[\"Code\"])\n",
    "    filename = 'model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test3(text):\n",
    "    print(text)\n",
    "    clntxt=cleanData(\"how to close a file automatically when you use\")\n",
    "    print(\"\\n\")\n",
    "    word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to close a file automatically when you use\n",
      "1 of 1 articles cleaned.\r\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "tokenize_test3(\"how to close a file automatically when you use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "1.0\n",
      "Testing Accuracy\n",
      "0.45454545454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50         7\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.37      0.31      0.33        11\n",
      "weighted avg       0.56      0.45      0.50        11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-f1c332f26812>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-a6e7c358b73a>\u001b[0m in \u001b[0;36mtokenize_test\u001b[1;34m(model, train, validation, num_comp, class1)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#and then creates the confusion_matrix from scikit learn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#How did we do?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Code'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_test(SVC(kernel = 'linear'),X_train,X_test,500,class1=list(range(0,y.nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "1.0\n",
      "Testing Accuracy\n",
      "0.45454545454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.25      0.33      0.29         3\n",
      "           2       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.45        11\n",
      "   macro avg       0.45      0.44      0.44        11\n",
      "weighted avg       0.49      0.45      0.47        11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-8dccdb5342e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adaptive'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-a6e7c358b73a>\u001b[0m in \u001b[0;36mtokenize_test\u001b[1;34m(model, train, validation, num_comp, class1)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#and then creates the confusion_matrix from scikit learn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#How did we do?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Code'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFMCAYAAAAA3S/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASgUlEQVR4nO3cX2iV9/3A8Y/JSeKfWKW09GaNYGp6kwtNelMkzHULxdoNNHQnumkvBOnVYIRtvTGItJrNXgx0HXSw4YTWiHhhBDtItQhhu8jBWMKwAesC242FKW1O1vzhPL+L4vkta5ujqcd8TV6vqzznOTnn44fg2+eoz4osy7IAAJJSs9gDAABfJdAAkCCBBoAECTQAJEigASBBAg0ACbqnQF+7di327t37lccvXboUXV1dkc/n48yZMw98OABYrnKVnvCHP/whzp8/H6tWrZrz+MzMTBw9ejTOnj0bq1atit27d8f3vve9ePLJJ6s2LAAsFxUD3dTUFMePH49f/vKXcx6/ceNGNDU1xbp16yIior29PYaHh2P79u3f+FqlUimKxWLU1dXFihUrvuXoAJC+LMtiZmYm1qxZEzU19/43yxUD/eKLL8Y///nPrzw+MTERa9euLR+vWbMmJiYm5n2tYrEYY2Nj9zwcACwVLS0tc7pZScVAf5PGxsYoFovl42KxWPGN6+rqIuLLIevr6xf61lQwOjoara2tiz3GkmfP1WfH1WfH1Tc9PR1jY2PlBt6rBQe6ubk5xsfH486dO7F69eoYHh6O/fv3z/s9dz/Wrq+vj4aGhoW+NffAfh8Oe64+O64+O3447vevdu870AMDAzE5ORn5fD5ef/312L9/f2RZFl1dXfHUU0/d78sBAF/jngL9ne98p/zfqH74wx+WH3/hhRfihRdeqM5kALCMuVEJACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIUMVAl0ql6O3tjXw+H3v37o3x8fE558+fPx87d+6Mrq6uePfdd6s2KAAsJ7lKTxgcHIzp6eno7++PkZGR6Ovri9///vfl87/5zW/iwoULsXr16tixY0fs2LEj1q1bV9WhAWCpqxjoQqEQHR0dERGxefPmGB0dnXP+2Wefjc8//zxyuVxkWRYrVqyozqQAsIxUDPTExEQ0NjaWj2tra2N2djZyuS+/ddOmTdHV1RWrVq2Kzs7OeOyxxyq+6f9GngevUCgs9gjLgj1Xnx1Xnx2nqWKgGxsbo1gslo9LpVI5ztevX48PP/wwPvjgg1i9enX84he/iIsXL8b27dvnfc3W1tZoaGj4lqPzTQqFQrS3ty/2GEuePVefHVefHVff1NTUgi5MK/4jsba2trhy5UpERIyMjERLS0v53Nq1a2PlypXR0NAQtbW18fjjj8dnn31230MAAHNVvILu7OyMoaGh6O7ujizL4siRIzEwMBCTk5ORz+cjn8/Hnj17oq6uLpqammLnzp0PY24AWNIqBrqmpiYOHz4857Hm5uby17t3747du3c/+MkAYBlzoxIASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJCgXKUnlEqlOHToUHz88cdRX18fb7zxRmzYsKF8/qOPPoq+vr7IsiyefPLJOHbsWDQ0NFR1aABY6ipeQQ8ODsb09HT09/dHT09P9PX1lc9lWRYHDx6Mo0ePxnvvvRcdHR3xr3/9q6oDA8ByUPEKulAoREdHR0REbN68OUZHR8vnbt68GevXr4+TJ0/G2NhYfPe7342NGzdWfNP/fg2qo1AoLPYIy4I9V58dV58dp6lioCcmJqKxsbF8XFtbG7Ozs5HL5eL27dtx9erVOHjwYGzYsCFee+21aG1tjeeff37e12xtbfUxeBUVCoVob29f7DGWPHuuPjuuPjuuvqmpqQVdmFb8iLuxsTGKxWL5uFQqRS73ZdfXr18fGzZsiGeeeSbq6uqio6PD1TEAPAAVA93W1hZXrlyJiIiRkZFoaWkpn3v66aejWCzG+Ph4REQMDw/Hpk2bqjQqACwfFT/i7uzsjKGhoeju7o4sy+LIkSMxMDAQk5OTkc/n480334yenp7Isiy2bNkS27ZtewhjA8DSVjHQNTU1cfjw4TmPNTc3l79+/vnn4+zZsw9+MgBYxtyoBAASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJKhioEulUvT29kY+n4+9e/fG+Pj41z7v4MGD8dZbbz3wAQFgOaoY6MHBwZieno7+/v7o6emJvr6+rzzn9OnTMTY2VpUBAWA5ylV6QqFQiI6OjoiI2Lx5c4yOjs45f/Xq1bh27Vrk8/n45JNP7ulN//c1ePAKhcJij7As2HP12XH12XGaKgZ6YmIiGhsby8e1tbUxOzsbuVwubt26FSdOnIgTJ07ExYsX7/lNW1tbo6GhYWETU1GhUIj29vbFHmPJs+fqs+Pqs+Pqm5qaWtCFacVANzY2RrFYLB+XSqXI5b78tvfffz9u374dBw4ciE8//TS++OKL2LhxY+zateu+BwEA/l/FQLe1tcXly5fjpZdeipGRkWhpaSmf27dvX+zbty8iIs6dOxeffPKJOAPAA1Ax0J2dnTE0NBTd3d2RZVkcOXIkBgYGYnJyMvL5/MOYEQCWnYqBrqmpicOHD895rLm5+SvPc+UMAA+OG5UAQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkKBcpSeUSqU4dOhQfPzxx1FfXx9vvPFGbNiwoXz+woULcfLkyaitrY2WlpY4dOhQ1NToPgB8GxVLOjg4GNPT09Hf3x89PT3R19dXPvfFF1/Eb3/72/jzn/8cp0+fjomJibh8+XJVBwaA5aBioAuFQnR0dERExObNm2N0dLR8rr6+Pk6fPh2rVq2KiIjZ2dloaGio0qgAsHxU/Ih7YmIiGhsby8e1tbUxOzsbuVwuampq4oknnoiIiFOnTsXk5GRs3bq14pv+d+SpjkKhsNgjLAv2XH12XH12nKaKgW5sbIxisVg+LpVKkcvl5hwfO3Ysbt68GcePH48VK1ZUfNPW1lZX2lVUKBSivb19scdY8uy5+uy4+uy4+qamphZ0YVrxI+62tra4cuVKRESMjIxES0vLnPO9vb0xNTUVb7/9dvmjbgDg26l4Bd3Z2RlDQ0PR3d0dWZbFkSNHYmBgICYnJ6O1tTXOnj0bzz33XLz66qsREbFv377o7Oys+uAAsJRVDHRNTU0cPnx4zmPNzc3lr69fv/7gpwKAZc5/WAaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAmqGOhSqRS9vb2Rz+dj7969MT4+Puf8pUuXoqurK/L5fJw5c6ZqgwLAclIx0IODgzE9PR39/f3R09MTfX195XMzMzNx9OjR+OMf/xinTp2K/v7++PTTT6s6MAAsB7lKTygUCtHR0REREZs3b47R0dHyuRs3bkRTU1OsW7cuIiLa29tjeHg4tm/f/rWvlWVZRERMT09/68GZ39TU1GKPsCzYc/XZcfXZcXXdbd7dBt6rioGemJiIxsbG8nFtbW3Mzs5GLpeLiYmJWLt2bfncmjVrYmJi4htfa2ZmJiIixsbG7mtI7t9//0GK6rHn6rPj6rPjh2NmZiZWrlx5z8+vGOjGxsYoFovl41KpFLlc7mvPFYvFOcH+X2vWrImWlpaoq6uLFStW3POQAPCoyrIsZmZmYs2aNff1fRUD3dbWFpcvX46XXnopRkZGoqWlpXyuubk5xsfH486dO7F69eoYHh6O/fv3f+Nr1dTUzBtwAFiK7ufK+a4VWYUPxUulUhw6dCjGxsYiy7I4cuRI/P3vf4/JycnI5/Nx6dKl+N3vfhdZlkVXV1f85Cc/WfAvAAD4UsVAAwAPnxuVAECCBBoAEiTQAJCgqgXaLUKrr9KOL1y4EK+88kp0d3dHb29vlEqlRZr00VVpx3cdPHgw3nrrrYc83dJQaccfffRR7NmzJ3bv3h0/+9nP3FRjgSrt+fz587Fz587o6uqKd999d5GmXBquXbsWe/fu/crj9929rEr+8pe/ZL/61a+yLMuyq1evZq+99lr53PT0dPaDH/wgu3PnTjY1NZXt2rUru3XrVrVGWbLm2/F//vOf7Pvf/342OTmZZVmW/fznP88GBwcXZc5H2Xw7vuu9997LfvzjH2fHjh172OMtCfPtuFQqZT/60Y+yf/zjH1mWZdmZM2eyGzduLMqcj7pKP8tbt27Nbt++nU1NTZV/f+b+vfPOO9nLL7+cvfLKK3MeX0j3qnYFfa+3CK2vry/fIpT7M9+O6+vr4/Tp07Fq1aqIiJidnY2GhoZFmfNRNt+OIyKuXr0a165di3w+vxjjLQnz7fjmzZuxfv36OHnyZPz0pz+NO3fuxMaNGxdr1EdapZ/lZ599Nj7//POYnp6OLMvcTGqBmpqa4vjx4195fCHdq1qgv+kWoXfP3c8tQvl68+24pqYmnnjiiYiIOHXqVExOTsbWrVsXZc5H2Xw7vnXrVpw4cSJ6e3sXa7wlYb4d3759O65evRp79uyJP/3pT/G3v/0t/vrXvy7WqI+0+fYcEbFp06bo6uqKHTt2xLZt2+Kxxx5bjDEfeS+++GL5bpv/bSHdq1qgH+QtQvl68+347vGvf/3rGBoaiuPHj/sT8QLMt+P3338/bt++HQcOHIh33nknLly4EOfOnVusUR9Z8+14/fr1sWHDhnjmmWeirq4uOjo63Dd6gebb8/Xr1+PDDz+MDz74IC5duhT//ve/4+LFi4s16pK0kO5VLdBtbW1x5cqViIh5bxE6PT0dw8PDsWXLlmqNsmTNt+OIiN7e3piamoq33367/FE392e+He/bty/OnTsXp06digMHDsTLL78cu3btWqxRH1nz7fjpp5+OYrFY/gdNw8PDsWnTpkWZ81E3357Xrl0bK1eujIaGhqitrY3HH388Pvvss8UadUlaSPcq3ot7oTo7O2NoaCi6u7vLtwgdGBgo3yL09ddfj/3795dvEfrUU09Va5Qla74dt7a2xtmzZ+O5556LV199NSK+DEpnZ+ciT/1oqfRzzLdXacdvvvlm9PT0RJZlsWXLlti2bdtij/xIqrTnfD4fe/bsibq6umhqaoqdO3cu9shLwrfpnlt9AkCC3KgEABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASND/Ab9OtqpEdLZXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp=MLPClassifier(hidden_layer_sizes=(200, 200),activation='relu',solver = 'adam', alpha = 1e-5, learning_rate = 'adaptive', learning_rate_init = 0.005, max_iter = 500, random_state = 21)\n",
    "tokenize_test(mlp,X_train,X_test,800,class1=list(range(0,X_train.nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-765cbff7b609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcatboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MultiClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "SGD=SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
    "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
    "                               fit_intercept=True, l1_ratio=0.15,\n",
    "                               learning_rate='optimal', loss='modified_huber',\n",
    "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
    "                               penalty='none', power_t=0.5, random_state=None,\n",
    "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
    "                               verbose=0, warm_start=False)\n",
    "import catboost\n",
    "cls = catboost.CatBoostClassifier(loss_function='MultiClass',learning_rate=0.1, iterations=50, depth=8)\n",
    "tokenize_test(cls,X_train,X_test,1500,class1=list(range(0,train[\"Code\"].nunique())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "1.0\n",
      "Testing Accuracy\n",
      "0.9090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "           4       1.00      0.50      0.67         2\n",
      "           6       1.00      1.00      1.00         2\n",
      "           8       1.00      0.50      0.67         2\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       1.00      1.00      1.00         2\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.91        22\n",
      "   macro avg       0.90      0.87      0.87        22\n",
      "weighted avg       0.98      0.91      0.92        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8c7f389b416b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mEnsembleClassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVotingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'lda'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mlp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sgd1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hard'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnsembleClassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-a6e7c358b73a>\u001b[0m in \u001b[0;36mtokenize_test\u001b[1;34m(model, train, validation, num_comp, class1)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#and then creates the confusion_matrix from scikit learn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#How did we do?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Code'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFMCAYAAAAA3S/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASgUlEQVR4nO3cX2iV9/3A8Y/JSeKfWKW09GaNYGp6kwtNelMkzHULxdoNNHQnumkvBOnVYIRtvTGItJrNXgx0HXSw4YTWiHhhBDtItQhhu8jBWMKwAesC242FKW1O1vzhPL+L4vkta5ujqcd8TV6vqzznOTnn44fg2+eoz4osy7IAAJJSs9gDAABfJdAAkCCBBoAECTQAJEigASBBAg0ACbqnQF+7di327t37lccvXboUXV1dkc/n48yZMw98OABYrnKVnvCHP/whzp8/H6tWrZrz+MzMTBw9ejTOnj0bq1atit27d8f3vve9ePLJJ6s2LAAsFxUD3dTUFMePH49f/vKXcx6/ceNGNDU1xbp16yIior29PYaHh2P79u3f+FqlUimKxWLU1dXFihUrvuXoAJC+LMtiZmYm1qxZEzU19/43yxUD/eKLL8Y///nPrzw+MTERa9euLR+vWbMmJiYm5n2tYrEYY2Nj9zwcACwVLS0tc7pZScVAf5PGxsYoFovl42KxWPGN6+rqIuLLIevr6xf61lQwOjoara2tiz3GkmfP1WfH1WfH1Tc9PR1jY2PlBt6rBQe6ubk5xsfH486dO7F69eoYHh6O/fv3z/s9dz/Wrq+vj4aGhoW+NffAfh8Oe64+O64+O3447vevdu870AMDAzE5ORn5fD5ef/312L9/f2RZFl1dXfHUU0/d78sBAF/jngL9ne98p/zfqH74wx+WH3/hhRfihRdeqM5kALCMuVEJACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIUMVAl0ql6O3tjXw+H3v37o3x8fE558+fPx87d+6Mrq6uePfdd6s2KAAsJ7lKTxgcHIzp6eno7++PkZGR6Ovri9///vfl87/5zW/iwoULsXr16tixY0fs2LEj1q1bV9WhAWCpqxjoQqEQHR0dERGxefPmGB0dnXP+2Wefjc8//zxyuVxkWRYrVqyozqQAsIxUDPTExEQ0NjaWj2tra2N2djZyuS+/ddOmTdHV1RWrVq2Kzs7OeOyxxyq+6f9GngevUCgs9gjLgj1Xnx1Xnx2nqWKgGxsbo1gslo9LpVI5ztevX48PP/wwPvjgg1i9enX84he/iIsXL8b27dvnfc3W1tZoaGj4lqPzTQqFQrS3ty/2GEuePVefHVefHVff1NTUgi5MK/4jsba2trhy5UpERIyMjERLS0v53Nq1a2PlypXR0NAQtbW18fjjj8dnn31230MAAHNVvILu7OyMoaGh6O7ujizL4siRIzEwMBCTk5ORz+cjn8/Hnj17oq6uLpqammLnzp0PY24AWNIqBrqmpiYOHz4857Hm5uby17t3747du3c/+MkAYBlzoxIASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJCgXKUnlEqlOHToUHz88cdRX18fb7zxRmzYsKF8/qOPPoq+vr7IsiyefPLJOHbsWDQ0NFR1aABY6ipeQQ8ODsb09HT09/dHT09P9PX1lc9lWRYHDx6Mo0ePxnvvvRcdHR3xr3/9q6oDA8ByUPEKulAoREdHR0REbN68OUZHR8vnbt68GevXr4+TJ0/G2NhYfPe7342NGzdWfNP/fg2qo1AoLPYIy4I9V58dV58dp6lioCcmJqKxsbF8XFtbG7Ozs5HL5eL27dtx9erVOHjwYGzYsCFee+21aG1tjeeff37e12xtbfUxeBUVCoVob29f7DGWPHuuPjuuPjuuvqmpqQVdmFb8iLuxsTGKxWL5uFQqRS73ZdfXr18fGzZsiGeeeSbq6uqio6PD1TEAPAAVA93W1hZXrlyJiIiRkZFoaWkpn3v66aejWCzG+Ph4REQMDw/Hpk2bqjQqACwfFT/i7uzsjKGhoeju7o4sy+LIkSMxMDAQk5OTkc/n480334yenp7Isiy2bNkS27ZtewhjA8DSVjHQNTU1cfjw4TmPNTc3l79+/vnn4+zZsw9+MgBYxtyoBAASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJKhioEulUvT29kY+n4+9e/fG+Pj41z7v4MGD8dZbbz3wAQFgOaoY6MHBwZieno7+/v7o6emJvr6+rzzn9OnTMTY2VpUBAWA5ylV6QqFQiI6OjoiI2Lx5c4yOjs45f/Xq1bh27Vrk8/n45JNP7ulN//c1ePAKhcJij7As2HP12XH12XGaKgZ6YmIiGhsby8e1tbUxOzsbuVwubt26FSdOnIgTJ07ExYsX7/lNW1tbo6GhYWETU1GhUIj29vbFHmPJs+fqs+Pqs+Pqm5qaWtCFacVANzY2RrFYLB+XSqXI5b78tvfffz9u374dBw4ciE8//TS++OKL2LhxY+zateu+BwEA/l/FQLe1tcXly5fjpZdeipGRkWhpaSmf27dvX+zbty8iIs6dOxeffPKJOAPAA1Ax0J2dnTE0NBTd3d2RZVkcOXIkBgYGYnJyMvL5/MOYEQCWnYqBrqmpicOHD895rLm5+SvPc+UMAA+OG5UAQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkKBcpSeUSqU4dOhQfPzxx1FfXx9vvPFGbNiwoXz+woULcfLkyaitrY2WlpY4dOhQ1NToPgB8GxVLOjg4GNPT09Hf3x89PT3R19dXPvfFF1/Eb3/72/jzn/8cp0+fjomJibh8+XJVBwaA5aBioAuFQnR0dERExObNm2N0dLR8rr6+Pk6fPh2rVq2KiIjZ2dloaGio0qgAsHxU/Ih7YmIiGhsby8e1tbUxOzsbuVwuampq4oknnoiIiFOnTsXk5GRs3bq14pv+d+SpjkKhsNgjLAv2XH12XH12nKaKgW5sbIxisVg+LpVKkcvl5hwfO3Ysbt68GcePH48VK1ZUfNPW1lZX2lVUKBSivb19scdY8uy5+uy4+uy4+qamphZ0YVrxI+62tra4cuVKRESMjIxES0vLnPO9vb0xNTUVb7/9dvmjbgDg26l4Bd3Z2RlDQ0PR3d0dWZbFkSNHYmBgICYnJ6O1tTXOnj0bzz33XLz66qsREbFv377o7Oys+uAAsJRVDHRNTU0cPnx4zmPNzc3lr69fv/7gpwKAZc5/WAaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAkSaABIkEADQIIEGgASJNAAkCCBBoAECTQAJEigASBBAg0ACRJoAEiQQANAggQaABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASJBAA0CCBBoAEiTQAJAggQaABAk0ACRIoAEgQQINAAmqGOhSqRS9vb2Rz+dj7969MT4+Puf8pUuXoqurK/L5fJw5c6ZqgwLAclIx0IODgzE9PR39/f3R09MTfX195XMzMzNx9OjR+OMf/xinTp2K/v7++PTTT6s6MAAsB7lKTygUCtHR0REREZs3b47R0dHyuRs3bkRTU1OsW7cuIiLa29tjeHg4tm/f/rWvlWVZRERMT09/68GZ39TU1GKPsCzYc/XZcfXZcXXdbd7dBt6rioGemJiIxsbG8nFtbW3Mzs5GLpeLiYmJWLt2bfncmjVrYmJi4htfa2ZmJiIixsbG7mtI7t9//0GK6rHn6rPj6rPjh2NmZiZWrlx5z8+vGOjGxsYoFovl41KpFLlc7mvPFYvFOcH+X2vWrImWlpaoq6uLFStW3POQAPCoyrIsZmZmYs2aNff1fRUD3dbWFpcvX46XXnopRkZGoqWlpXyuubk5xsfH486dO7F69eoYHh6O/fv3f+Nr1dTUzBtwAFiK7ufK+a4VWYUPxUulUhw6dCjGxsYiy7I4cuRI/P3vf4/JycnI5/Nx6dKl+N3vfhdZlkVXV1f85Cc/WfAvAAD4UsVAAwAPnxuVAECCBBoAEiTQAJCgqgXaLUKrr9KOL1y4EK+88kp0d3dHb29vlEqlRZr00VVpx3cdPHgw3nrrrYc83dJQaccfffRR7NmzJ3bv3h0/+9nP3FRjgSrt+fz587Fz587o6uqKd999d5GmXBquXbsWe/fu/crj9929rEr+8pe/ZL/61a+yLMuyq1evZq+99lr53PT0dPaDH/wgu3PnTjY1NZXt2rUru3XrVrVGWbLm2/F//vOf7Pvf/342OTmZZVmW/fznP88GBwcXZc5H2Xw7vuu9997LfvzjH2fHjh172OMtCfPtuFQqZT/60Y+yf/zjH1mWZdmZM2eyGzduLMqcj7pKP8tbt27Nbt++nU1NTZV/f+b+vfPOO9nLL7+cvfLKK3MeX0j3qnYFfa+3CK2vry/fIpT7M9+O6+vr4/Tp07Fq1aqIiJidnY2GhoZFmfNRNt+OIyKuXr0a165di3w+vxjjLQnz7fjmzZuxfv36OHnyZPz0pz+NO3fuxMaNGxdr1EdapZ/lZ599Nj7//POYnp6OLMvcTGqBmpqa4vjx4195fCHdq1qgv+kWoXfP3c8tQvl68+24pqYmnnjiiYiIOHXqVExOTsbWrVsXZc5H2Xw7vnXrVpw4cSJ6e3sXa7wlYb4d3759O65evRp79uyJP/3pT/G3v/0t/vrXvy7WqI+0+fYcEbFp06bo6uqKHTt2xLZt2+Kxxx5bjDEfeS+++GL5bpv/bSHdq1qgH+QtQvl68+347vGvf/3rGBoaiuPHj/sT8QLMt+P3338/bt++HQcOHIh33nknLly4EOfOnVusUR9Z8+14/fr1sWHDhnjmmWeirq4uOjo63Dd6gebb8/Xr1+PDDz+MDz74IC5duhT//ve/4+LFi4s16pK0kO5VLdBtbW1x5cqViIh5bxE6PT0dw8PDsWXLlmqNsmTNt+OIiN7e3piamoq33367/FE392e+He/bty/OnTsXp06digMHDsTLL78cu3btWqxRH1nz7fjpp5+OYrFY/gdNw8PDsWnTpkWZ81E3357Xrl0bK1eujIaGhqitrY3HH388Pvvss8UadUlaSPcq3ot7oTo7O2NoaCi6u7vLtwgdGBgo3yL09ddfj/3795dvEfrUU09Va5Qla74dt7a2xtmzZ+O5556LV199NSK+DEpnZ+ciT/1oqfRzzLdXacdvvvlm9PT0RJZlsWXLlti2bdtij/xIqrTnfD4fe/bsibq6umhqaoqdO3cu9shLwrfpnlt9AkCC3KgEABIk0ACQIIEGgAQJNAAkSKABIEECDQAJEmgASND/Ab9OtqpEdLZXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    SVM = SVC(kernel = 'linear', probability = False)\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb=XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators=500,\n",
    "     max_depth=15,\n",
    "     min_child_weight=12,\n",
    "     gamma=0,\n",
    "     subsample=0.8,\n",
    "     colsample_bytree=0.8,\n",
    "     objective= 'multi:softmax',\n",
    "     nthread=16,\n",
    "     scale_pos_weight=1,\n",
    "     seed=9999,\n",
    "     num_class= 3,\n",
    "     eval_metric='merror',    # evaluation metric\n",
    "     tree_method='exact',\n",
    "     silent=0, \n",
    "     nthreads=-1)\n",
    "    ext=ExtraTreesClassifier(n_estimators=100, \n",
    "                                                       class_weight=\"balanced\", \n",
    "                                                       random_state=4621)\n",
    "\n",
    "    lda=LinearDiscriminantAnalysis()\n",
    "  \n",
    "    EnsembleClassifier = VotingClassifier(estimators = [('sgd', ext), ('lda',lda),('svc', SVC(kernel = 'linear')),('mlp',mlp),('sgd1',SGD)], voting = 'hard', weights = [1,1,1,1,1])\n",
    "\n",
    "    tokenize_test(EnsembleClassifier,X_train,X_test,1200,list(range(0,y.nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
