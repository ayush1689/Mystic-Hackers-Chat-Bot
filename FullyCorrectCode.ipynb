{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import hstack\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e08383e100d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def cleaning(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    #s = str.split(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\",\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    s = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.MULTILINE)\n",
    "    s = re.sub(r'\\<a href', ' ', s)\n",
    "    s = re.sub(r'&amp;', '', s) \n",
    "    s = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', s)\n",
    "    s = re.sub(r'[^\\x00-\\x7f]',r'',s) #removes arabic\n",
    "    s = re.sub(r'<br />', ' ', s)\n",
    "    s = re.sub(r'\\'', ' ', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "train['Text'] = [cleaning(s) for s in train['Text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in single_sentence]\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(model,train,validation,num_comp):\n",
    "\n",
    "    lsa = TruncatedSVD(n_components=200)\n",
    "    X_test1 = char_vectorizer.transform(validation)\n",
    "    X_test2 = word_vectorizer.transform(validation)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    #train_text = vect.fit_transform(train[\"Text\"])\n",
    "    #print ('Features: ', train_text.shape[1])\n",
    "    #test_text = vect.transform(validation[\"Text\"])\n",
    "   \n",
    "    tsvd=TruncatedSVD(num_comp)\n",
    "    train_features1 = tsvd.fit_transform(train_features)\n",
    "    test_stack = tsvd.transform(test_stack)\n",
    "\n",
    "    train_features1 = Normalizer(copy=False).fit_transform(train_features1)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    model.fit(train_features1, y_train)\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    \n",
    "    print(\"Training Accuracy\")\n",
    "    print(model.score(train_features1,y_train))\n",
    "    print(\"Testing Accuracy\")\n",
    "    print(model.score(test_stack,y_test))\n",
    "    #print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "    #print(classification_report(np.argmax(y_test,axis=1),y_pred_class,target_names=list(labelEncoder.classes_)))\n",
    "    \n",
    "    \n",
    "    cm = ConfusionMatrix(model, classes=list(labelEncoder.classes_))\n",
    "\n",
    "\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "    cm.score(test_stack, y_test)\n",
    "\n",
    "#How did we do?\n",
    "    cm.poof()\n",
    "    #print(confusion_matrix(y_pred_class,validation[\"Code\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################ACTUAL CODE STARTS FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=pd.read_csv('Statistics_corpus.csv',encoding =  \"ISO-8859-1\")\n",
    "#stats.columns=[\"a\",\"Question\",\"Sub_Topic_Code\"]\n",
    "labelEncoder = LabelEncoder()\n",
    "stats[\"Topic_Name\"] = labelEncoder.fit_transform(stats[\"Topic_Name\"])\n",
    "stats = stats.rename(columns={'Topic_Name': 'Category', 'Question': 'Text'})\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"cleantext\"]=data_df.Text.apply(cleanString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Sub_Topic_Code</th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0001</td>\n",
       "      <td>8</td>\n",
       "      <td>How to Summarizes data (Data and Histogram)</td>\n",
       "      <td>summarizes data  data  histogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0015</td>\n",
       "      <td>12</td>\n",
       "      <td>When one tailed Hypothesis test is done</td>\n",
       "      <td>one tailed hypothesis test  done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0019</td>\n",
       "      <td>17</td>\n",
       "      <td>When to do Normalization</td>\n",
       "      <td>normalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0021</td>\n",
       "      <td>6</td>\n",
       "      <td>How to do Encoding of Categorical Data</td>\n",
       "      <td>encoding  categorical data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0001</td>\n",
       "      <td>8</td>\n",
       "      <td>What is Univariate Analysis</td>\n",
       "      <td>univariate analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0014</td>\n",
       "      <td>9</td>\n",
       "      <td>Give some examnples of Binomial Distribution a...</td>\n",
       "      <td>give  examnples  binomial distribution applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0010</td>\n",
       "      <td>15</td>\n",
       "      <td>Where probality is used in Machine Learning</td>\n",
       "      <td>probality  used  machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0018</td>\n",
       "      <td>19</td>\n",
       "      <td>When to do Chi Square test</td>\n",
       "      <td>chi square test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0001</td>\n",
       "      <td>8</td>\n",
       "      <td>What is Multvariate Analysis</td>\n",
       "      <td>multvariate analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>S0007</td>\n",
       "      <td>11</td>\n",
       "      <td>How to determine outliers in Boxplot</td>\n",
       "      <td>determine outlier  boxplot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Code Sub_Topic_Code  Category  \\\n",
       "0      AASKK02          S0001         8   \n",
       "1      AASKK02          S0015        12   \n",
       "2      AASKK02          S0019        17   \n",
       "3      AASKK02          S0021         6   \n",
       "4      AASKK02          S0001         8   \n",
       "..         ...            ...       ...   \n",
       "102    AASKK02          S0014         9   \n",
       "103    AASKK02          S0010        15   \n",
       "104    AASKK02          S0018        19   \n",
       "105    AASKK02          S0001         8   \n",
       "106    AASKK02          S0007        11   \n",
       "\n",
       "                                                  Text  \\\n",
       "0          How to Summarizes data (Data and Histogram)   \n",
       "1              When one tailed Hypothesis test is done   \n",
       "2                             When to do Normalization   \n",
       "3               How to do Encoding of Categorical Data   \n",
       "4                          What is Univariate Analysis   \n",
       "..                                                 ...   \n",
       "102  Give some examnples of Binomial Distribution a...   \n",
       "103        Where probality is used in Machine Learning   \n",
       "104                         When to do Chi Square test   \n",
       "105                       What is Multvariate Analysis   \n",
       "106               How to determine outliers in Boxplot   \n",
       "\n",
       "                                             cleantext  \n",
       "0                   summarizes data  data  histogram    \n",
       "1                    one tailed hypothesis test  done   \n",
       "2                                       normalization   \n",
       "3                          encoding  categorical data   \n",
       "4                                 univariate analysis   \n",
       "..                                                 ...  \n",
       "102  give  examnples  binomial distribution applica...  \n",
       "103                 probality  used  machine learning   \n",
       "104                                   chi square test   \n",
       "105                              multvariate analysis   \n",
       "106                        determine outlier  boxplot   \n",
       "\n",
       "[107 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Code</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic_Name</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>17</td>\n",
       "      <td>Test of variance and Proportion</td>\n",
       "      <td>What is Chi Square test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>1</td>\n",
       "      <td>Central Tendency and 3 Ms</td>\n",
       "      <td>What is median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>19</td>\n",
       "      <td>Exploratory Data analysis</td>\n",
       "      <td>How to do Exploratory Data analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>1</td>\n",
       "      <td>Central Tendency and 3 Ms</td>\n",
       "      <td>What are Central Tendency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>17</td>\n",
       "      <td>Test of variance and Proportion</td>\n",
       "      <td>When to do Test of variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>7</td>\n",
       "      <td>Data Visualizations</td>\n",
       "      <td>Whta is Data Visualizations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>11</td>\n",
       "      <td>Marginal Probablity</td>\n",
       "      <td>What is Marginal Probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>20</td>\n",
       "      <td>Data Preprocessing</td>\n",
       "      <td>What is Data Preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>18</td>\n",
       "      <td>Scaling and Normalization</td>\n",
       "      <td>What are Normalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>AASKK02</td>\n",
       "      <td>14</td>\n",
       "      <td>Hypothesis Formulation and Tests</td>\n",
       "      <td>What is one tailed Hypothesis test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Code  Category                        Topic_Name  \\\n",
       "0      AASKK02        17   Test of variance and Proportion   \n",
       "1      AASKK02         1         Central Tendency and 3 Ms   \n",
       "2      AASKK02        19         Exploratory Data analysis   \n",
       "3      AASKK02         1         Central Tendency and 3 Ms   \n",
       "4      AASKK02        17   Test of variance and Proportion   \n",
       "..         ...       ...                               ...   \n",
       "102    AASKK02         7               Data Visualizations   \n",
       "103    AASKK02        11               Marginal Probablity   \n",
       "104    AASKK02        20                Data Preprocessing   \n",
       "105    AASKK02        18         Scaling and Normalization   \n",
       "106    AASKK02        14  Hypothesis Formulation and Tests   \n",
       "\n",
       "                                    Text  \n",
       "0                What is Chi Square test  \n",
       "1                         What is median  \n",
       "2    How to do Exploratory Data analysis  \n",
       "3              What are Central Tendency  \n",
       "4            When to do Test of variance  \n",
       "..                                   ...  \n",
       "102          Whta is Data Visualizations  \n",
       "103         What is Marginal Probability  \n",
       "104           What is Data Preprocessing  \n",
       "105               What are Normalization  \n",
       "106   What is one tailed Hypothesis test  \n",
       "\n",
       "[107 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 of 107 articles cleaned.\r"
     ]
    }
   ],
   "source": [
    "def clean_data(data_df):\n",
    "    articles = []\n",
    "    n = data_df['Text'].shape[0]\n",
    "    col_number = data_df.columns.get_loc('Text')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    data_cleaned = data_df.copy()\n",
    "    for i in range(n):\n",
    "        temp_string= cleanString(data_df.iloc[i,col_number])\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    data_cleaned.loc[:,'Text'] = pd.Series(articles,index=data_df.index)\n",
    "    data_cleaned.loc[:,'Category'] = pd.Categorical(data_cleaned.Category)\n",
    "    data_cleaned['Code'] = data_cleaned.Category.cat.codes\n",
    "    categoryToCode = dict( enumerate(data_cleaned['Category'].cat.categories))\n",
    "    return(data_cleaned)\n",
    "data_cleaned=clean_data(data_df)\n",
    "from sklearn.model_selection import train_test_split\n",
    "y=data_cleaned[\"Code\"]\n",
    "X=data_cleaned[\"Text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=23, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_name:2254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_tokenizer(doc):\n",
    "    token_pattern=r\"(?u)\\b\\w+\"\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    return token_pattern.findall(doc)\n",
    "\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))\n",
    "    \n",
    "posts_root1=[]\n",
    "\n",
    "for post in X_train:\n",
    "    a=\" \".join([english_stemmer.stem(word) for word in build_tokenizer(post)])\n",
    "    posts_root1.append(a)\n",
    "\n",
    "word_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words = 'english',\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1,3),\n",
    "    dtype=np.float32,\n",
    "    max_features=8000,\n",
    "    \n",
    ")\n",
    "# Character Stemmer\n",
    "char_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    dtype=np.float32,\n",
    "    max_features=8000,\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "word_vectorizer.fit(posts_root1)\n",
    "char_vectorizer.fit(posts_root1)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(posts_root1)\n",
    "train_char_features = char_vectorizer.transform(posts_root1)\n",
    "\n",
    "\n",
    "train_features = hstack([\n",
    "    train_char_features,\n",
    "    train_word_features])\n",
    "\n",
    "\n",
    "filename = 'char_preprocessing.sav'\n",
    "pickle.dump(word_vectorizer, open(filename, 'wb'))\n",
    "filename = 'word_preprocessing.sav'\n",
    "pickle.dump(char_vectorizer, open(filename, 'wb'))\n",
    "print(\"feature_name:%s\" % len(char_vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test2(model,train,text):\n",
    "    from sklearn.decomposition import NMF\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    print(text)\n",
    "    X_test1 = char_vectorizer.transform(text)\n",
    "    X_test2 = word_vectorizer.transform(text)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    train_features1 = Normalizer(copy=False).fit_transform(train_features)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    model.fit(train_features, y_train)\n",
    "    filename = 'model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    return(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test3(text):\n",
    "    print(text)\n",
    "    clntxt=cleanData(text)\n",
    "    word_model = pickle.load( open( \"word_preprocessing.sav\", \"rb\" ) )\n",
    "    char_model = pickle.load( open( \"char_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    test_stack = hstack([X_test1,X_test2])\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    y_pred_class = model.predict(test_stack)\n",
    "    print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to close a file automatically when you use\n",
      "1 of 1 articles cleaned.\r",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "tokenize_test3(\"how to close a file automatically when you use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' hypothesis  formulated ']\n",
      "https://en.wikipedia.org/wiki/Statistical_hypothesis_testing\n"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "value=tokenize_test2(SVC(kernel = 'linear'),X_train,cleanData(\"How Hypothesis are formulated\"))\n",
    "out=labelEncoder.classes_[value]\n",
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "wikiResults = wikipedia.search(out)\n",
    "result = wikiResults[0]\n",
    "page_py = wiki_wiki.page(result)\n",
    "print(page_py.fullurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chi-squared test (id: ??, ns: 0)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bayes theorem and Example', 'Central Tendency and 3 Ms',\n",
       "       'Coefficient of Variation',\n",
       "       'Concepts of sampling distribution and Central Limit Theorem',\n",
       "       'Confidence Intervals and tests', 'Correlation Analysis',\n",
       "       'Data Preprocessing', 'Data Visualizations', 'Data and Histogram',\n",
       "       'Distributions and Example', 'Exploratory Data analysis',\n",
       "       'Five Number Summary Boxplot and other plots',\n",
       "       'Hypothesis Formulation and Tests', 'Marginal Probablity',\n",
       "       'Measures of Dispersion Range and IQR',\n",
       "       'Probability - Meaning and concepts',\n",
       "       'Rules for Computing Probability', 'Scaling and Normalization',\n",
       "       'Standard Deviation', 'Test of variance and Proportion',\n",
       "       'The Empirical Rule and Chebyshev Rule'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  close  file automatically   use ']\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "mlp=MLPClassifier(hidden_layer_sizes=(200, 200),activation='relu',solver = 'adam', alpha = 1e-5, learning_rate = 'adaptive', learning_rate_init = 0.005, max_iter = 500, random_state = 21)\n",
    "target_strings = label_encoder.inverse_transform(np.arange(num_classes))\n",
    "tokenize_test2(mlp,X_train,cleanData(\"how to close a file automatically when you use\"),class1=list(range(0,y.nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-765cbff7b609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcatboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MultiClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "SGD=SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
    "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
    "                               fit_intercept=True, l1_ratio=0.15,\n",
    "                               learning_rate='optimal', loss='modified_huber',\n",
    "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
    "                               penalty='none', power_t=0.5, random_state=None,\n",
    "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
    "                               verbose=0, warm_start=False)\n",
    "import catboost\n",
    "cls = catboost.CatBoostClassifier(loss_function='MultiClass',learning_rate=0.1, iterations=50, depth=8)\n",
    "tokenize_test(cls,X_train,X_test,1500,class1=list(range(0,train[\"Code\"].nunique())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "1.0\n",
      "Testing Accuracy\n",
      "0.8636363636363636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           6       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       1.00      1.00      1.00         2\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      0.50      0.67         2\n",
      "          20       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.86        22\n",
      "   macro avg       0.83      0.81      0.81        22\n",
      "weighted avg       0.93      0.86      0.88        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KarthikeyanNatarajan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-8c7f389b416b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mEnsembleClassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVotingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'lda'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mlp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sgd1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hard'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnsembleClassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-bfc78dd11c62>\u001b[0m in \u001b[0;36mtokenize_test\u001b[1;34m(model, train, validation, num_comp, class1)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#and then creates the confusion_matrix from scikit learn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#How did we do?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Code'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    SVM = SVC(kernel = 'linear', probability = False)\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb=XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators=500,\n",
    "     max_depth=15,\n",
    "     min_child_weight=12,\n",
    "     gamma=0,\n",
    "     subsample=0.8,\n",
    "     colsample_bytree=0.8,\n",
    "     objective= 'multi:softmax',\n",
    "     nthread=16,\n",
    "     scale_pos_weight=1,\n",
    "     seed=9999,\n",
    "     num_class= 3,\n",
    "     eval_metric='merror',    # evaluation metric\n",
    "     tree_method='exact',\n",
    "     silent=0, \n",
    "     nthreads=-1)\n",
    "    ext=ExtraTreesClassifier(n_estimators=100, \n",
    "                                                       class_weight=\"balanced\", \n",
    "                                                       random_state=4621)\n",
    "\n",
    "    lda=LinearDiscriminantAnalysis()\n",
    "  \n",
    "    EnsembleClassifier = VotingClassifier(estimators = [('sgd', ext), ('lda',lda),('svc', SVC(kernel = 'linear')),('mlp',mlp),('sgd1',SGD)], voting = 'hard', weights = [1,1,1,1,1])\n",
    "\n",
    "    tokenize_test(EnsembleClassifier,X_train,X_test,1200,list(range(0,y.nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
