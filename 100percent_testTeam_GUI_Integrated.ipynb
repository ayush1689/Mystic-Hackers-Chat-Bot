{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is AI, Machine Learning, Big Data computing and big data?\n",
      "[86] 1 articles cleaned.\n",
      "[45] [45]\n",
      "['Introduction']\n",
      "['Introduction']\n",
      "Does python has features of data analytics\n",
      "[478]1 articles cleaned.\n",
      "[62] [62]\n",
      "['Python Introduction']\n",
      "['Python Introduction']\n",
      "Hello\n",
      "[147]1 articles cleaned.\n",
      "[41] [41]\n",
      "['Greetings']\n",
      "['Greetings']\n",
      "Linear Regression formula\n",
      "[256]1 articles cleaned.\n",
      "[78] [78]\n",
      "['Types of Algorithm']\n",
      "['Types of Algorithm']\n",
      "unable to understand knn\n",
      "[206]1 articles cleaned.\n",
      "[55] [55]\n",
      "['Nurel Network']\n",
      "['Nurel Network']\n",
      "Which library is used for machine learning models\n",
      "[216]1 articles cleaned.\n",
      "[61] [45]\n",
      "['Probability - Meaning and concepts']\n",
      "['Introduction']\n",
      "did not get neural networks\n",
      "[62] 1 articles cleaned.\n",
      "[55] [55]\n",
      "['Nurel Network']\n",
      "['Nurel Network']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "def cleanString(review):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        #single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in sentence_token]\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_class(sentence,classes):\n",
    "\n",
    "    res,prob = predict_the_message_prob(sentence)\n",
    "\n",
    "    return_list = []\n",
    "    list1, list2 = (list(t) for t in zip(*sorted(zip(prob, res))))\n",
    "    for l,t in zip(list1, list2):\n",
    "        return_list.append({\"intent\": classes[t[-1]], \"probability\": str(l[-1])})\n",
    "    print(return_list)\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    print(tag)\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = i['responses']\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response1(msg):\n",
    "    words=[]\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_words = ['?', '!']\n",
    "    data_file = open('out1.json').read()\n",
    "    intents = json.loads(data_file)\n",
    "    for intent in list(intents['intents']):\n",
    "        #print(intent['patterns'])\n",
    "        w = nltk.word_tokenize(intent['patterns'])\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "    #words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "    #words = sorted(list(set(words)))\n",
    "\n",
    "    #classes = sorted(list(set(classes)))\n",
    "\n",
    "    #print (len(documents), \"documents\")\n",
    "\n",
    "    #print (len(classes), \"classes\", classes)\n",
    "\n",
    "    #print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "    ints = predict_class(msg,classes)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n",
    "\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1  \n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))\n",
    "    \n",
    "    \n",
    "def cleanData(string1):\n",
    "    articles = []\n",
    "    n = 1\n",
    "    for i in range(n):\n",
    "        temp_string = cleanString(string1)\n",
    "        articles.append(temp_string)\n",
    "        print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "    return(articles)\n",
    "\n",
    "def predict_the_message_prob(clntxt):\n",
    "    print(text)\n",
    "    clntxt=cleanData(text)\n",
    "    word_model = pickle.load( open( \"Python_word_preprocessing.sav\", \"rb\" ) )\n",
    "    char_model = pickle.load( open( \"Pyhton_char_preprocessing.sav\", \"rb\" ) )\n",
    "    svd_model = pickle.load( open( \"Pyhton_SVD_preprocessing.sav\", \"rb\" ) )\n",
    "    all_features = pickle.load( open( \"train_features1_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    test_stack = hstack([X_test2,X_test1])\n",
    "    test_stack=svd_model.transform(test_stack)\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    best_thread = pairwise_distances_argmin(\n",
    "            X=test_stack,\n",
    "            Y=all_features,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    print(best_thread)\n",
    "    y_pred_class1 = model.predict_proba(all_features[best_thread])[0]\n",
    "    y_pred_class2 = model.predict_proba(test_stack)\n",
    "    #print(np.sort(y_pred_class2, axis=1)[:,-8:])\n",
    "    #y_pred_class2 = model.predict_proba(test_stack)\n",
    "    best_n = np.argsort(y_pred_class2, axis=1)[:,-8:]\n",
    "    #best_n = np.argsort(probs, axis=1)[:,-n:]\n",
    "    #print(y_pred_class1,y_pred_class2)\n",
    "    return(best_n,np.sort(y_pred_class2, axis=1)[:,-8:])\n",
    "\n",
    "def predict_the_message(text):\n",
    "    print(text)\n",
    "    clntxt=cleanData(text)\n",
    "    word_model = pickle.load( open( \"Python_word_preprocessing.sav\", \"rb\" ) )\n",
    "    char_model = pickle.load( open( \"Pyhton_char_preprocessing.sav\", \"rb\" ) )\n",
    "    svd_model = pickle.load( open( \"Pyhton_SVD_preprocessing.sav\", \"rb\" ) )\n",
    "    all_features = pickle.load( open( \"train_features1_preprocessing.sav\", \"rb\" ) )\n",
    "    X_test1 = word_model.transform(clntxt)\n",
    "    X_test2 = char_model.transform(clntxt)\n",
    "    test_stack = hstack([X_test2,X_test1])\n",
    "    test_stack=svd_model.transform(test_stack)\n",
    "    model = pickle.load( open( \"model.sav\", \"rb\" ) )\n",
    "    best_thread = pairwise_distances_argmin(\n",
    "            X=test_stack,\n",
    "            Y=all_features,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    print(best_thread)\n",
    "    y_pred_class1 = model.predict(all_features[best_thread])\n",
    "    y_pred_class2 = model.predict(test_stack)\n",
    "    print(y_pred_class1,y_pred_class2)\n",
    "    return(y_pred_class1,y_pred_class2)\n",
    "\n",
    "def clicked (value):\n",
    "    clearscreen()   ## to clear the screen\n",
    "    label1 = Label(root, text=value, font=(\"Verdana\", 12) )\n",
    "    label1.pack()\n",
    "\n",
    "    #Create Chat window\n",
    "    ChatLog = Text(root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\")\n",
    "    \n",
    "    ChatLog.insert(END, \"AASKK:  Hello!!, Please type your question \" '\\n\\n')\n",
    "    ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ),borderwidth = 5)\n",
    "\n",
    "    ChatLog.config(state=DISABLED)\n",
    "\n",
    "    #Bind scrollbar to Chat window\n",
    "    scrollbar = Scrollbar(root, command=ChatLog.yview, cursor=\"heart\")\n",
    "    ChatLog['yscrollcommand'] = scrollbar.set\n",
    "    \n",
    "    #Create the box to enter message\n",
    "    EntryBox = Text(root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\",borderwidth = 5)\n",
    "    #EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "    #Create Button to send message\n",
    "    SendButton = Button(root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"11\", height=4,\n",
    "                        bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',borderwidth = 5,\n",
    "                        command= lambda: send(EntryBox,ChatLog,value) )\n",
    "\n",
    "    #Place all components on the screen\n",
    "    scrollbar.place(x=480,y=25, height=375)\n",
    "    ChatLog.place(x=6,y=25, height=375, width=480)\n",
    "    EntryBox.place(x=140, y=403, height=65, width=360)\n",
    "    SendButton.place(x=6, y=403, height=65, width=135)\n",
    "\n",
    "\n",
    "\n",
    "def clearscreen():\n",
    "    list = root.pack_slaves()\n",
    "    for l in list:\n",
    "        l.destroy()    \n",
    "        \n",
    "def send(EntryBox,ChatLog,value):\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    \n",
    "    if msg == '':\n",
    "        messagebox.showinfo(\"Allert\", \"Please enter your question and then click send\")\n",
    "        \n",
    "    elif msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "\n",
    "        res = chatbot_response(msg,value)\n",
    "        ChatLog.insert(END, \"AASKK: \" + res + '\\n\\n')\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "        \n",
    "\n",
    "def chatbot_response(msg,value):\n",
    "    \n",
    "   # msg =cleanData(msg)  ##output is articals\n",
    "    \n",
    "    if value == \"Python\":\n",
    "        output,output2= predict_the_message(msg)\n",
    "        \n",
    "        label=  pickle.load( open( \"labelEncoder.pickle\", \"rb\" ) )\n",
    "        className=label.classes_[output]\n",
    "        className1=label.classes_[output2]\n",
    "        print(label.classes_[output])\n",
    "        print(label.classes_[output2])\n",
    "        str1 = ''.join(className1)\n",
    "        dict_label=  pickle.load( open( \"LabelDictionary.pickle\", \"rb\" ) )\n",
    "        #dict_label[listToString(['Data Analysis'])]        \n",
    "        HardCodedResult1=dict_label[listToString(label.classes_[output])]\n",
    "        HardCodedResult2=dict_label[listToString(label.classes_[output2])]\n",
    "        match_object = re.search(r'CATEGORY(.*)', str1, flags=re.IGNORECASE)\n",
    "        if match_object:\n",
    "  #      if re.search(r\"CAT(.*)\", str1, flags=re.IGNORECASE):\n",
    "            index=match_object.group(1)\n",
    "            answers=pickle.load( open( \"data_df.pickle\", \"rb\" ) )\n",
    "            #answers=  pickle.load( open( \"answers.sav\", \"rb\" ) )\n",
    "            className=answers.iloc[int(index)]\n",
    "           # className=tlist[float(match_object.group(1))-1]\n",
    "            print(className)\n",
    "        #if re.match(CAT\\d+, output) is not None:\n",
    " \n",
    "        return(str(output)+str(className1)+\"\\n\"+str(HardCodedResult1))  \n",
    "    elif value == \"Statistics\":\n",
    "        output = predict_the_message(msg)\n",
    "        \n",
    "        label=  pickle.load( open( \"labelEncoder.pickle\", \"rb\" ) )\n",
    "        className=label.classes_[output]\n",
    "        className1=label.classes_[output2]\n",
    "        print(label.classes_[output])\n",
    "        print(label.classes_[output2])\n",
    "        str1 = ''.join(className1)\n",
    "        dict_label=  pickle.load( open( \"LabelDictionary.pickle\", \"rb\" ) )\n",
    "        #dict_label[listToString(['Data Analysis'])]        \n",
    "        HardCodedResult1=dict_label[listToString(label.classes_[output])]\n",
    "        HardCodedResult2=dict_label[listToString(label.classes_[output2])]\n",
    "        match_object = re.search(r'CAT(.*)', str1, flags=re.IGNORECASE)\n",
    "        if match_object:\n",
    "  #      if re.search(r\"CAT(.*)\", str1, flags=re.IGNORECASE):\n",
    "            index=match_object.group(1)\n",
    "            answers=pickle.load( open( \"data_df.pickle\", \"rb\" ) )\n",
    "            #answers=  pickle.load( open( \"answers.sav\", \"rb\" ) )\n",
    "            className=answers.iloc[int(index)]\n",
    "           # className=tlist[float(match_object.group(1))-1]\n",
    "            print(className)\n",
    "        #if re.match(CAT\\d+, output) is not None:\n",
    "     \n",
    "        import wikipediaapi\n",
    "        import wikipedia\n",
    "        wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "        print(className1)\n",
    "        wikiResults = wikipedia.search(className)\n",
    "        result = wikiResults[0]\n",
    "        page_py = wiki_wiki.page(result)\n",
    "        print(className)\n",
    "        print(page_py.fullurl)\n",
    "        return(str(output)+str(className1)+\"\\n\"+page_py.fullurl+\"\\n\"+HardCodedResult1+\"\\n\"+HardCodedResult2)  \n",
    "    elif value == \"Supervised Learning\":\n",
    "        output = predict_the_message(msg)\n",
    "        \n",
    "        label=  pickle.load( open( \"labelEncoder.pickle\", \"rb\" ) )\n",
    "        className=label.classes_[output]\n",
    "        className1=label.classes_[output2]\n",
    "        print(label.classes_[output])\n",
    "        print(label.classes_[output2])\n",
    "        str1 = ''.join(className1)\n",
    "        dict_label=  pickle.load( open( \"LabelDictionary.pickle\", \"rb\" ) )\n",
    "        #dict_label[listToString(['Data Analysis'])]        \n",
    "        HardCodedResult1=dict_label[listToString(label.classes_[output])]\n",
    "        HardCodedResult2=dict_label[listToString(label.classes_[output2])]\n",
    "        match_object = re.search(r'CAT(.*)', str1, flags=re.IGNORECASE)\n",
    "        if match_object:\n",
    "  #      if re.search(r\"CAT(.*)\", str1, flags=re.IGNORECASE):\n",
    "            index=match_object.group(1)\n",
    "            answers=pickle.load( open( \"data_df.pickle\", \"rb\" ) )\n",
    "            #answers=  pickle.load( open( \"answers.sav\", \"rb\" ) )\n",
    "            className=answers.iloc[int(index)]\n",
    "           # className=tlist[float(match_object.group(1))-1]\n",
    "            print(className)\n",
    "        #if re.match(CAT\\d+, output) is not None:\n",
    "     \n",
    "        import wikipediaapi\n",
    "        import wikipedia\n",
    "        wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "        print(className1)\n",
    "        wikiResults = wikipedia.search(className)\n",
    "        result = wikiResults[0]\n",
    "        page_py = wiki_wiki.page(result)\n",
    "        print(className)\n",
    "        print(page_py.fullurl)\n",
    "        return(str(output)+str(className1)+\"\\n\"+page_py.fullurl+\"\\n\"+HardCodedResult1+\"\\n\"+HardCodedResult2)  \n",
    "    elif value == \"Ensemble Techniques\":\n",
    "        output = predict_the_message(msg)\n",
    "        \n",
    "        label=  pickle.load( open( \"labelEncoder.pickle\", \"rb\" ) )\n",
    "        className=label.classes_[output]\n",
    "        className1=label.classes_[output2]\n",
    "        print(label.classes_[output])\n",
    "        print(label.classes_[output2])\n",
    "        str1 = ''.join(className1)\n",
    "        dict_label=  pickle.load( open( \"LabelDictionary.pickle\", \"rb\" ) )\n",
    "        #dict_label[listToString(['Data Analysis'])]        \n",
    "        HardCodedResult1=dict_label[output]\n",
    "        HardCodedResult2=dict_label[output2]\n",
    "        match_object = re.search(r'CAT(.*)', str1, flags=re.IGNORECASE)\n",
    "        if match_object:\n",
    "  #      if re.search(r\"CAT(.*)\", str1, flags=re.IGNORECASE):\n",
    "            index=match_object.group(1)\n",
    "            answers=pickle.load( open( \"data_df.pickle\", \"rb\" ) )\n",
    "            #answers=  pickle.load( open( \"answers.sav\", \"rb\" ) )\n",
    "            className=answers.iloc[int(index)]\n",
    "           # className=tlist[float(match_object.group(1))-1]\n",
    "            print(className)\n",
    "        #if re.match(CAT\\d+, output) is not None:\n",
    "     \n",
    "        import wikipediaapi\n",
    "        import wikipedia\n",
    "        wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "        print(className1)\n",
    "        wikiResults = wikipedia.search(className)\n",
    "        result = wikiResults[0]\n",
    "        page_py = wiki_wiki.page(result)\n",
    "        print(className)\n",
    "        print(page_py.fullurl)\n",
    "        return(str(output)+str(className1)+\"\\n\"+page_py.fullurl+\"\\n\"+HardCodedResult1+\"\\n\"+HardCodedResult2)  \n",
    "    elif value == \"Unsupervised learning\":\n",
    "        output = predict_the_message(msg)\n",
    "        \n",
    "        label=  pickle.load( open( \"labelEncoder.pickle\", \"rb\" ) )\n",
    "        className=label.classes_[output]\n",
    "        className1=label.classes_[output2]\n",
    "        print(label.classes_[output])\n",
    "        print(label.classes_[output2])\n",
    "        str1 = ''.join(className1)\n",
    "        dict_label=  pickle.load( open( \"LabelDictionary.pickle\", \"rb\" ) )\n",
    "        #dict_label[listToString(['Data Analysis'])]        \n",
    "        HardCodedResult1=dict_label[listToString(label.classes_[output])]\n",
    "        HardCodedResult2=dict_label[listToString(label.classes_[output2])]\n",
    "        match_object = re.search(r'CAT(.*)', str1, flags=re.IGNORECASE)\n",
    "        if match_object:\n",
    "  #      if re.search(r\"CAT(.*)\", str1, flags=re.IGNORECASE):\n",
    "            index=match_object.group(1)\n",
    "            #answers=pickle.load( open( \"data_df.pickle\", \"rb\" ) )\n",
    "            answers=  pickle.load( open( \"answers.sav\", \"rb\" ) )\n",
    "            className=answers.iloc[int(index)]\n",
    "           # className=tlist[float(match_object.group(1))-1]\n",
    "            print(className)\n",
    "        #if re.match(CAT\\d+, output) is not None:\n",
    "     \n",
    "        import wikipediaapi\n",
    "        import wikipedia\n",
    "        wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "        print(className1)\n",
    "        wikiResults = wikipedia.search(className)\n",
    "        result = wikiResults[0]\n",
    "        page_py = wiki_wiki.page(result)\n",
    "        print(className)\n",
    "        print(page_py.fullurl)\n",
    "        return(str(output)+str(className1)+\"\\n\"+page_py.fullurl+\"\\n\"+HardCodedResult1+\"\\n\"+HardCodedResult2)    \n",
    "    else:\n",
    "        Result=chatbot_response1(msg) \n",
    "        return(Result)\n",
    "#Description: This is a chat bot GUI\n",
    "\n",
    "\n",
    "#Import the library\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import ImageTk,Image\n",
    "from tkinter import messagebox\n",
    "\n",
    "root = Tk()\n",
    "#Name of the chat Bot\n",
    "root.title(\"AASKK\") \n",
    "#Geometry of the chat Bot\n",
    "root.geometry(\"500x600\")\n",
    "root.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "\n",
    "# Creating Background Image\n",
    "canvas=Canvas(root,width=500,height=600)\n",
    "#give this image path. image should be in png format.\n",
    "#image=ImageTk.PhotoImage(Image.open(\"wall_e.png\"))\n",
    "#canvas.create_image(0,0,anchor=NW,image=image)\n",
    "canvas.place(height=600, x=0, y=0)\n",
    "\n",
    "def donothing():\n",
    "   filewin = Toplevel(root)\n",
    "   button = Button(filewin, text=\"Do nothing button\")\n",
    "   button.pack()\n",
    "   \n",
    "menubar = Menu(root)\n",
    "filemenu = Menu(menubar, tearoff=0)\n",
    "filemenu.add_command(label=\"New\", command=donothing)\n",
    "filemenu.add_command(label=\"Open\", command=donothing)\n",
    "filemenu.add_command(label=\"Save\", command=donothing)\n",
    "filemenu.add_command(label=\"Save as...\", command=donothing)\n",
    "filemenu.add_command(label=\"Close\", command=donothing)\n",
    "\n",
    "filemenu.add_separator()\n",
    "\n",
    "filemenu.add_command(label=\"Exit\", command=root.quit)\n",
    "menubar.add_cascade(label=\"File\", menu=filemenu)\n",
    "editmenu = Menu(menubar, tearoff=0)\n",
    "editmenu.add_command(label=\"Undo\", command=donothing)\n",
    "\n",
    "editmenu.add_separator()\n",
    "\n",
    "editmenu.add_command(label=\"Cut\", command=donothing)\n",
    "editmenu.add_command(label=\"Copy\", command=donothing)\n",
    "editmenu.add_command(label=\"Paste\", command=donothing)\n",
    "editmenu.add_command(label=\"Delete\", command=donothing)\n",
    "editmenu.add_command(label=\"Select All\", command=donothing)\n",
    "\n",
    "menubar.add_cascade(label=\"Edit\", menu=editmenu)\n",
    "helpmenu = Menu(menubar, tearoff=0)\n",
    "helpmenu.add_command(label=\"Help Index\", command=donothing)\n",
    "helpmenu.add_command(label=\"About...\", command=donothing)\n",
    "menubar.add_cascade(label=\"Help\", menu=helpmenu)\n",
    "\n",
    "root.config(menu=menubar)\n",
    "\n",
    "label2 = Label(root, text=\"Welcome\", font=(\"Arial\", 18) )\n",
    "label2.pack(padx= 20, pady =10)\n",
    "\n",
    "label2 = Label(root, text=\"Please select the topic from which you want to learn\", font=(\"Arial\", 12) )\n",
    "label2.pack(padx= 20, pady =20)\n",
    "\n",
    "\n",
    "TOPICS = [(\"Python\",\"Python\"),\n",
    "          (\"Statistics\",\"Statistics\"),\n",
    "          (\"Supervised Learning\",\"Supervised Learning\"),\n",
    "          (\"Ensemble Techniques\",\"Ensemble Techniques\"),\n",
    "          (\"Unsupervised learning\",\"Unsupervised learning\"),\n",
    "          (\"Not Sure about the topic\",\"All Topic\")\n",
    "         ]\n",
    "option = StringVar()  \n",
    "option.set(\"Python\")\n",
    "  \n",
    "for text,topics in TOPICS:\n",
    "    r =Radiobutton(root, text = text, variable=option , value=topics, font=(\"Arial\", 12))\n",
    "    r.pack(fill=Y,anchor = W)\n",
    "Button2 = Button(root, text=\"Submit\",fg='#ffffff',\n",
    "                 bg=\"#32de97\",activebackground=\"#3c9d9b\",font=(\"Arial\", 12),\n",
    "                 command = lambda : clicked(option.get()))\n",
    "\n",
    "Button2.pack(padx= 20, pady =20)\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KarthikeyanNatarajan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label=  pickle.load( open( \"LabelDictionary.pickle\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Python Introduction': 'https://beginnersbook.com/2018/01/introduction-to-python-programming/ ',\n",
       " 'Lists': 'https://chrisalbon.com/#python',\n",
       " 'Dictionary': 'https://chrisalbon.com/#python',\n",
       " 'File': 'https://chrisalbon.com/#python',\n",
       " 'Numpy': 'https://en.wikipedia.org/wiki/NumPy',\n",
       " 'Ploting': 'https://en.wikipedia.org/wiki/Matplotlib',\n",
       " 'Pandas': 'https://nbviewer.jupyter.org/github/fonnesbeck/Bios8366/tree/master/notebooks/',\n",
       " 'PivotTable': 'https://nbviewer.jupyter.org/github/fonnesbeck/Bios8366/tree/master/notebooks/',\n",
       " 'GroupBy': 'https://nbviewer.jupyter.org/github/fonnesbeck/Bios8366/tree/master/notebooks/',\n",
       " 'Missing': 'https://www.kaggle.com/dark4user/python-programming-from-a-to-z ',\n",
       " 'Misc': 'https://www.kaggle.com/dark4user/python-programming-from-a-to-z ',\n",
       " 'String': 'https://chrisalbon.com/#python',\n",
       " 'Statistics': 'https://chrisalbon.com/#statistics',\n",
       " 'Sets': 'https://chrisalbon.com/#python',\n",
       " 'RegularExpression': 'https://chrisalbon.com/#regex',\n",
       " 'Data Analysis': 'https://www.geeksforgeeks.org/univariate-bivariate-and-multivariate-data-and-its-analysis/',\n",
       " 'Central Tendency and 3 Ms': 'https://en.wikipedia.org/wiki/Central_tendency',\n",
       " 'Covariance': 'https://en.wikipedia.org/wiki/Covariance',\n",
       " 'Measures of Dispersion, Range, IQR,  Standard Deviation, Variance': 'https://en.wikipedia.org/wiki/Statistical_dispersion',\n",
       " 'Coefficient of Variation': 'https://en.wikipedia.org/wiki/Coefficient_of_variation',\n",
       " 'The Empirical Rule and Chebyshev Rule': 'https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Shafer_and_Zhang)/02%3A_Descriptive_Statistics/2.05%3A_The_Empirical_Rule_and_Chebyshev%27s_Theorem',\n",
       " 'Five Number Summary': 'https://en.wikipedia.org/wiki/Five-number_summary',\n",
       " 'Boxplot and whisker plots': 'https://statisticsbyjim.com/basics/outliers/',\n",
       " 'Data Visualizations': 'https://en.wikipedia.org/wiki/Data_visualization',\n",
       " 'Skewness ': 'https://en.wikipedia.org/wiki/Skewness',\n",
       " 'Correlation Analysis': 'https://en.wikipedia.org/wiki/Correlation_coefficient',\n",
       " 'Probability - Meaning and concepts': 'https://en.wikipedia.org/wiki/Probability',\n",
       " 'Bayes theorem and Example': 'https://en.wikipedia.org/wiki/Bayes%27_theorem',\n",
       " 'Distributions and Example': 'https://en.wikipedia.org/wiki/Probability_distribution',\n",
       " 'Hypothesis Formulation': 'https://en.wikipedia.org/wiki/Statistical_hypothesis_testing',\n",
       " 'Hypothesis  Tests': 'https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTest-Means-Proportions/BS704_HypothesisTest-Means-Proportions_print.html',\n",
       " 'Test of variance': 'https://www.empirical-methods.hslu.ch/decisiontree/differences/variance/1-13chi-square-test-for-variance/',\n",
       " 'ANOVA': 'https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTesting-ANOVA/BS704_HypothesisTesting-Anova_print.html',\n",
       " 'Scaling and Normalization': 'https://kharshit.github.io/blog/2018/03/23/scaling-vs-normalization',\n",
       " 'Exploratory Data analysis': 'https://en.wikipedia.org/wiki/Exploratory_data_analysis',\n",
       " 'Data Preprocessing': 'https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c833',\n",
       " 'Concepts of sampling distribution and Central Limit Theorem': 'https://statisticsbyjim.com/basics/central-limit-theorem/',\n",
       " 'Confidence Intervals and tests': 'https://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests-confidence-intervals-and-confidence-levels',\n",
       " 'Feature Selection': 'https://en.wikipedia.org/wiki/Feature_selection',\n",
       " 'Greetings': 'Good to you too, how can I help you ?',\n",
       " 'Introduction': 'https://vinodsblog.com/2018/10/08/machine-learning-introduction-to-supervised-learning/',\n",
       " 'Preprocessing': 'https://medium.com/datadriveninvestor/data-preprocessing-for-machine-learning-188e9eef1d2c',\n",
       " 'Types of Algorithm': 'https://theappsolutions.com/blog/development/machine-learning-algorithm-types/',\n",
       " 'Linear Regression': 'https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e104',\n",
       " 'Logistic regression': 'https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc',\n",
       " 'Unsupervised Learnings Basic': 'https://en.wikipedia.org/wiki/Unsupervised_learning',\n",
       " 'Types of Unsupervised Learnings ': 'https://www.guru99.com/unsupervised-machine-learning.html#:~:text=Unsupervised%20machine%20learning%20helps%20you,3)%20Overlapping%204)%20Probabilistic.',\n",
       " 'Clustering Techniques basic': 'https://en.wikipedia.org/wiki/Cluster_analysis',\n",
       " 'Tyes of Clustering techiques': 'https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/',\n",
       " 'Non Hirerarchical Clustering': 'https://mb3is.megx.net/gustame/dissimilarity-based-methods/cluster-analysis/non-hierarchical-cluster-analysis',\n",
       " 'Tpyes of Hierarchical Clustering': 'https://mb3is.megx.net/gustame/dissimilarity-based-methods/cluster-analysis/non-hierarchical-cluster-analysis',\n",
       " 'K- Mean': 'https://en.wikipedia.org/wiki/K-means_clustering',\n",
       " 'DB scane': 'https://en.wikipedia.org/wiki/DBSCAN',\n",
       " 'Agglomerative Clustering ': 'http://mlwiki.org/index.php/Hierarchical_Clustering#:~:text=Agglomerative%20Clustering%3A%20This%20is%20a,one%20moves%20up%20the%20hierarchy.',\n",
       " 'PCA': 'https://en.wikipedia.org/wiki/Principal_component_analysis',\n",
       " 'SVM': 'https://en.wikipedia.org/wiki/Support_vector_machine',\n",
       " 'Ensemble Techniques': 'https://en.wikipedia.org/wiki/Ensemble_techniques',\n",
       " 'Ensemble Types': 'https://en.wikipedia.org/wiki/Ensemble_types',\n",
       " 'Stacking': 'https://en.wikipedia.org/wiki/Stacking',\n",
       " 'Randomization': 'https://en.wikipedia.org/wiki/Randomization',\n",
       " 'Decision Tree': 'https://en.wikipedia.org/wiki/Decision_tree_model',\n",
       " 'Random Forest': 'https://en.wikipedia.org/wiki/Random_forest',\n",
       " 'Boosting': 'https://en.wikipedia.org/wiki/Boost',\n",
       " 'Bagging': 'https://en.wikipedia.org/wiki/Bagging',\n",
       " 'Regression and Classification methods': 'https://en.wikipedia.org/wiki/Regression_analysis',\n",
       " 'Nurel Network': 'https://en.wikipedia.org/wiki/Deep_learning',\n",
       " 'CATEGORY6': 'Generative models , Discriminative models, Decision trees, mutual_info_classif, LDA, QDA, SVM,',\n",
       " 'CATEGORY7': ' Random Forest, Decision trees, Ensemble methods, Logsistic regression, Naive Bayes,KNeighborsClassifier, ',\n",
       " 'CATEGORY8': 'Mutual_info_regression,Ridge regression, Lasso Rigression,',\n",
       " 'CATEGORY9': 'Linear regression,  Polynomail regression, Decision trees regression,SVM Regression, Random Forest Regression, Decision trees regression, Ensemble methods,KNeighborsRegresspr ',\n",
       " 'CATEGORY10': 'Stochastic Gradient descent, Light Gradient Descent, XGBOOST, CATBOOST,One Vs All, One vs Rest Classifier.',\n",
       " 'CATEGORY11': 'Max Voting, Hard voting, weighted voting, Soft voting, Averaging, weight averaging, Sample selection, Row selection, Feature selection, Feture union',\n",
       " 'CATEGORY12': ' Bagging, Boosting, RandomForest, Stacking Classifier, ',\n",
       " 'CATEGORY13': 'df.sample,train_test_split, KFold, StratifiedKFold',\n",
       " 'CATEGORY14': 'Pipeline, PMML pipeline, column transformed, Missing value imputer, Variance threshold',\n",
       " 'CATEGORY15': 'matthews_corrcoef, Pearson Correlation coeff, AUC, ROC-AUC, False positive, False Negetive, True positive, True Negetive, False positive rate, True positive rate, specificity, sensitivity',\n",
       " 'CATEGORY16': 'StandardScaler, MinMaxScaler,OneHotEncoder, RobustScaler, OutputEncoding, Normalization, Frequency encoding, Mean Encoding, Median encoding.',\n",
       " 'CATEGORY17': 'PCA, SVD, Randomized SVD, Truncated SVD, ',\n",
       " 'CATEGORY18': 'Kmeans clustering, DBSCAN, Aggloromative clustering, Heirarchial clustering, Top down clustering, Bottom up clustering, Dendograms,Dynamic Clustering, Static clustering',\n",
       " 'CATEGORY19': 'Manhattan, Euclidean, Cosine,Jaccard distance,Edit distance,Mahalanobis distance,Elbow method,Entropy,Pruning,',\n",
       " 'CATEGORY20': 'GridSearch, RandomizedSearch, HyperoptEstimator, CrossValidation Framework'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> list1 = [3,2,4,1, 1]\n",
    ">>> list2 = ['three', 'two', 'four', 'one', 'one2']\n",
    ">>> list1, list2 = zip(*sorted(zip(list1, list2)))\n",
    ">>> list1\n",
    "(1, 1, 2, 3, 4)\n",
    ">>> list2 \n",
    "('one', 'one2', 'two', 'three', 'four')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
